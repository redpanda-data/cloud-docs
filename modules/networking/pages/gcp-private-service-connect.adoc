= Configure GCP Private Service Connect with the Cloud API
:description: Set up GCP Private Service Connect to securely access Redpanda Cloud.
:page-aliases: deploy:deployment-option/cloud/gcp-private-service-connect.adoc
:env-byoc: true

include::networking:partial$psc-api.adoc[]

== Create a new BYOVPC cluster with Private Service Connect

. In the https://cloud.redpanda.com/[Redpanda Cloud UI], go to **Resource groups** and select the resource group in which you want to create a cluster.
+
Copy and store the resource group ID (UUID) from the URL in the browser.
+
[,bash]
----
export RESOURCE_GROUP_ID=<uuid>
----

. Follow the BYOVPC steps to xref:get-started:cluster-types/byoc/gcp/vpc-byo-gcp.adoc#configure-the-service-project[configure the service project] to configure IAM role, permissions, and firewall rules.

. BYOVPC clusters need a NAT subnet with `purpose` set to `PRIVATE_SERVICE_CONNECT`. You can create the subnet using the `gcloud` CLI:
+
[,bash]
----
gcloud compute networks subnets create <psc-nat-subnet-name> \
    --project=<host-project-id> \
    --network=<shared-vpc-name> \
    --region=<region> \
    --range=<psc-nat-subnet-range> \
    --purpose=PRIVATE_SERVICE_CONNECT
----
+
Provide your values for the following placeholders:
+
- `<psc-nat-subnet-name>`: The name of the NAT subnet.
- `<host-project-id>`: The host GCP project ID.
- `<shared-vpc-name>`: The name of the VPC being used for your Redpanda Cloud cluster. The name is used to identify this network in the Cloud UI.
- `<region>`: The GCP region of the Redpanda Cloud cluster.
- `<psc-nat-subnet-range>`: The CIDR range of the subnet. The mask should be at least `/29`. Each Private Service Connect connection takes up one IP address from the NAT subnet, so the CIDR must be able to accommodate all projects from which connections to the service attachment will be issued.
+
See the GCP documentation for https://cloud.google.com/vpc/docs/configure-private-service-connect-producer#add-subnet-psc[creating a subnet for Private Service Connect^].

. Create VPC firewall rules to allow Private Service Connect traffic. Use the `gcloud` CLI to create the firewall rules:  
+
NOTE: The firewall rules support up to 20 Redpanda brokers. If you have more than 20 brokers, or for help enabling Private Service Connect, contact https://support.redpanda.com/hc/en-us/requests/new[Redpanda support^].
+
```
gcloud compute firewall-rules create redpanda-psc \
  --description="Allow access to Redpanda PSC endpoints" \
  --network="<shared-vpc-name>" \
  --project="<host-project-id>" \
  --direction="INGRESS" \
  --target-tags="redpanda-node" \
  --source-ranges="10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,100.64.0.0/10" \
  --allow="tcp:30181,tcp:30282,tcp:30292,tcp:31004,tcp:31082-31101,tcp:31182-31201,tcp:31282-31301,tcp:32092-32111,tcp:32192-32211,tcp:32292-32311"
```

. Make a request to the xref:api:ROOT:cloud-controlplane-api.adoc#post-/v1/networks[`POST /v1/networks`] endpoint to create a network.
+
[,bash]
----
NETWORK_POST_BODY=`cat << EOF
{
    "network": {
        "cloud_provider": "CLOUD_PROVIDER_GCP",
        "cluster_type": "TYPE_BYOC",
        "name": "<shared-vpc-name>",
        "resource_group_id": "$RESOURCE_GROUP_ID",
        "region": "<region>",
        "customer_managed_resources": {
            "gcp": {
                "network_name": "<byovpc-network-name>",
                "network_project_id": "<byovpc-network-gcp-project-id>",
                "management_bucket": { "name" : "<byovpc-management-bucket>" }
            }
        }
    }
}
EOF`

curl -vv -X POST \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $AUTH_TOKEN" \
-d "$NETWORK_POST_BODY" $PUBLIC_API_ENDPOINT/v1/networks
----
+
Replace the following placeholder variables for the request body:
+
--
- `<shared-vpc-name>`: The name for the network. 
- `<region>`: The GCP region where the network will be created.
- `<byovpc-network-gcp-project-id>`: The ID of the GCP project where your VPC is created.
- `<byovpc-network-name>`: The name of your VPC.
- `<byovpc-management-bucket>`: The name of the Google Storage bucket you created for the cluster.
--


. Store the network ID (`metadata.network_id`) returned in the response to the Create Network request.
+
[,bash]
----
export NETWORK_ID=<metadata.network_id>
----

. Make a request to the xref:api:ROOT:cloud-controlplane-api.adoc#post-/v1/clusters[`POST /v1/clusters`] endpoint to create a Redpanda Cloud cluster with Private Service Connect enabled.
+
[,bash]
----
export CLUSTER_POST_BODY=`cat << EOF
{
    "cluster": {
        "cloud_provider": "CLOUD_PROVIDER_GCP",
        "connection_type": "CONNECTION_TYPE_PRIVATE",
        "type": "TYPE_BYOC",
        "name": "<cluster-name>",
        "resource_group_id": "$RESOURCE_GROUP_ID",
        "network_id": "$NETWORK_ID",
        "region": "<region>",
        "zones": <zones>,
        "throughput_tier": "<throughput-tier>",
        "redpanda_version": "<redpanda-version>",
        "gcp_private_service_connect": {
            "enabled": true,
            "consumer_accept_list": <consumer-accept-list>
        },
        "customer_managed_resources": {
            "gcp": {
                "subnet": {
                    "name":"<byovpc-subnet-name>",
                    "secondary_ipv4_range_pods": { "name": "<byovpc-subnet-pods-range-name>" },
                    "secondary_ipv4_range_services": { "name": "<byovpc-subnet-services-range-name>" },
                    "k8s_master_ipv4_range": "<byovpc-subnet-master-range>"
                },
                "psc_nat_subnet_name": "<psc-nat-subnet-name>",
                "agent_service_account": { "email": "<byovpc-agent-service-acc-email>" },
                "connector_service_account": { "email": "<byovpc-connectors-service-acc-email>" },
                "console_service_account": { "email": "<byovpc-console-service-acc-email>" },
                "redpanda_cluster_service_account": { "email": "<byovpc-redpanda-service-acc-email>" },
                "gke_service_account": { "email": "<byovpc-gke-service-acc-email>" },
                "tiered_storage_bucket": { "name" : "<byovpc-tiered-storage-bucket>" }
            }
        }
    }
}
EOF`

curl -vv -X POST \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $AUTH_TOKEN" \
-d "$CLUSTER_POST_BODY" $PUBLIC_API_ENDPOINT/v1/clusters
----
+
Replace the following placeholders for the request body. Variables with a `byovpc_` prefix represent xref:get-started:cluster-types/byoc/gcp/vpc-byo-gcp.adoc[customer-managed resources] that should have been created previously:
+
--
- `<cluster-name>`: Provide a name for the new cluster.
- `<region>`: Choose a GCP region where the network will be created.
- `<zones>`: Provide the list of GCP zones where the brokers will be deployed. Format: `["<zone 1>", "<zone 2>", "<zone N>"]`
- `<throughput-tier>`: Choose a Redpanda Cloud cluster tier. For example, `tier-1-gcp-v2-x86`.
- `<redpanda-version>`: Choose the Redpanda Cloud version.
- `<consumer-accept-list>`: The list of IDs of GCP projects from which Private Service Connect connection requests are accepted. Format: `[{"source": "<GCP-project-ID-1>"}, {"source": "<GCP-project-ID-2>"}, {"source": "<GCP-project-ID-N>"}]`
- `<byovpc-subnet-name>`: The name of the GCP subnet that was created for the cluster.
- `<byovpc-subnet-pods-range-name>`: The name of the IPv4 range designated for K8s pods.
- `<byovpc-subnet-services-range-name>`: The name of the IPv4 range designated for services.
- `<byovpc-subnet-master-range>`: The master IPv4 range.
- `<psc-nat-subnet-name>`: The name of the GCP subnet that was created for Private Service Connect NAT.
- `<byovpc-agent-service-acc-email>`: The email for the agent service account.
- `<byovpc-connectors-service-acc-email>`: The email for the connectors service account.
- `<byovpc-console-service-acc-email>`: The email for the console service account.
- `<byovpc-redpanda-service-acc-email>`: The email for the Redpanda service account.
- `<byovpc-gke-service-acc-email>`: The email for the GKE service account.
- `<byovpc-tiered-storage-bucket>`: The name of the Google Storage bucket to use for Tiered Storage.
--

== Enable Private Service Connect on an existing BYOC or BYOVPC cluster

CAUTION: As soon as Private Service Connect is available on your VPC, all communication on existing Redpanda bootstrap server and  broker ports is interrupted due to the change on the private DNS resolution. Make sure all applications running in your VPC are ready to start using the corresponding Private Service Connect ports.

. In the Redpanda Cloud UI, go to the cluster overview and copy the cluster ID from the **Details** section.
+
[,bash]
----
export CLUSTER_ID=<cluster-id>
----

. For a *BYOC cluster*:
+ 
--
- Run `rpk cloud byoc gcp apply` to ensure that the PSC NAT subnets are created in your BYOC cluster. 
```bash
rpk cloud byoc gcp apply --redpanda-id="${CLUSTER_ID}" --project-id='<service-project-id>'
``` 
- Run `gcloud compute networks subnets list` to find the newly-created Private Service Connect NAT subnet name. 
```bash
gcloud compute networks subnets list --filter psc2-nat --format="value(name)"
```
--
+
For a *BYOVPC cluster*:
+
--
- xref:get-started:cluster-types/byoc/gcp/vpc-byo-gcp.adoc#configure-the-service-project[Configure the service project] to configure the IAM role, permissions, and firewall rules.
- Create a NAT subnet and firewall rules to allow Private Service Connect traffic. To do this, follow steps 3 and 4 in <<Create a new BYOVPC cluster with Private Service Connect>>.
- Run `rpk cloud byoc gcp apply`:
+
```bash
rpk cloud byoc gcp apply --redpanda-id="${CLUSTER_ID}" --project-id='<service-project-id>'
``` 
- Make a request to the xref:api:ROOT:cloud-controlplane-api.adoc#patch-/v1/clusters/-cluster.id-[`PATCH /v1/clusters/{cluster.id}`] endpoint to update the cluster to include the newly-created Private Service Connect NAT subnet.
+
[,bash]
----
export PSC_NAT_SUBNET_NAME='<psc-nat-subnet-name>'
export CLUSTER_PATCH_BODY=`cat << EOF
{
    "customer_managed_resources": {
        "gcp": {
            "psc_nat_subnet_name": "${PSC_NAT_SUBNET_NAME}"
        }
    }
}
EOF`
curl -v -X PATCH \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $AUTH_TOKEN" \
-d "$CLUSTER_PATCH_BODY" $PUBLIC_API_ENDPOINT/v1/clusters/$CLUSTER_ID
----
+
Replace the following placeholder:
+
`<psc-nat-subnet-name>`: The name of the Private Service Connect NAT subnet. Use the fully-qualified name, for example `"projects/<host-project-id>/regions/<region>/subnetworks/<psc-nat-subnet-name>"`.
--

. Make a xref:api:ROOT:cloud-controlplane-api.adoc#patch-/v1/clusters/-cluster.id-[`PATCH /v1/clusters/{cluster.id}`] request to update the cluster to enable Private Service Connect.
+
[,bash]
----
CLUSTER_PATCH_BODY=`cat << EOF
{
    "gcp_private_service_connect": {
        "enabled": true,
         "consumer_accept_list": <consumer-accept-list>
    }
}
EOF`
curl -v -X PATCH \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $AUTH_TOKEN" \
-d "$CLUSTER_PATCH_BODY" $PUBLIC_API_ENDPOINT/v1/clusters/$CLUSTER_ID
----
+
Replace the following placeholder:
+
`<consumer-accept-list>`: A JSON list specifying the projects from which incoming connections will be accepted. All other sources are rejected. For example, `[{"source": "consumer-project-ID-1"},{"source": "consumer-project-ID-2"}]`.
+
Wait for the cluster to apply the new configuration (around 15 minutes). The Private Service Connect attachment is available when the cluster update is complete. To monitor the service attachment creation, run the following `gcloud` command with the project ID:
+
[,bash]
----
gcloud compute service-attachments list --project '<service-project-id>'
----

include::networking:partial$psc-ui.adoc[]

== Disable Private Service Connect

Make a xref:api:ROOT:cloud-controlplane-api.adoc#patch-/v1/clusters/-cluster.id-[`PATCH /v1/clusters/{cluster.id}`] request to update the cluster to disable Private Service Connect.

[,bash]
----
CLUSTER_PATCH_BODY=`cat << EOF
{
    "gcp_private_service_connect": {
        "enabled": false
    }
}
EOF`
curl -v -X PATCH \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $AUTH_TOKEN" \
-d "$CLUSTER_PATCH_BODY" $PUBLIC_API_ENDPOINT/v1/clusters/$CLUSTER_ID
----

