= Migrate to AI Gateway
:description: Step-by-step migration guide to transition existing applications from direct LLM provider integrations to Redpanda AI Gateway with minimal disruption.
:page-topic-type: how-to
:personas: app_developer, platform_admin
:learning-objective-1: Migrate LLM integrations to AI Gateway with zero downtime using feature flags
:learning-objective-2: Verify gateway connectivity and compare performance metrics
:learning-objective-3: Roll back to direct integration if issues arise during migration

include::ai-agents:partial$adp-la.adoc[]

This guide helps you migrate existing applications from direct LLM provider integrations (OpenAI, Anthropic, and others) to Redpanda AI Gateway. Design the migration to be incremental and reversible, allowing you to test thoroughly before fully committing.

**Downtime required:** None (supports parallel operation)

**Rollback difficulty:** Easy (feature flag or environment variable)

== Prerequisites

Before migrating, ensure you have:

* AI Gateway configured in your Redpanda Cloud account
* Enabled providers and models in AI Gateway
* Created gateway with appropriate policies
* Your gateway endpoint URL (with the gateway ID embedded in the path)

Verify your gateway is reachable:

[source,bash]
----
curl <your-gateway-endpoint>/models \
  -H "Authorization: Bearer {YOUR_TOKEN}"
----

Expected output: List of enabled models

== Migration strategy

=== Recommended approach: Parallel operation

Run both direct and gateway-routed requests simultaneously to validate behavior before full cutover.

[source,text]
----
┌─────────────────┐
│  Application    │
└────────┬────────┘
         │
    ┌────▼─────┐
    │ Feature  │
    │  Flag    │
    └────┬─────┘
         │
    ┌────▼──────────────┐
    │                   │
┌───▼─────┐      ┌─────▼─────┐
│ Direct  │      │  Gateway  │
│Provider │      │  Route    │
└─────────┘      └───────────┘
----


Benefits:

* No downtime
* Easy rollback
* Compare results side-by-side
* Gradual traffic shift

== Step-by-step migration

=== Add environment variables

Add gateway configuration to your environment without removing existing provider keys (yet).

*.env (or equivalent)*
[source,bash]
----
# Existing (keep these for now)
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...

# New gateway configuration
REDPANDA_AI_GATEWAY_URL=<your-gateway-endpoint>
REDPANDA_AI_GATEWAY_TOKEN={YOUR_TOKEN}

# Feature flag (start with gateway disabled)
USE_AI_GATEWAY=false
----


=== Update your code

==== Option A: OpenAI SDK (recommended for most use cases)

Before (Direct OpenAI)

[source,python]
----
from openai import OpenAI

client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY")
)

response = client.chat.completions.create(
    model="gpt-5.2",
    messages=[{"role": "user", "content": "Hello"}]
)
----


After (Gateway-routed with feature flag)

[source,python]
----
from openai import OpenAI
import os

# Feature flag determines which client to use
use_gateway = os.getenv("USE_AI_GATEWAY", "false").lower() == "true"

if use_gateway:
    client = OpenAI(
        base_url=os.getenv("REDPANDA_AI_GATEWAY_URL"),
        api_key=os.getenv("REDPANDA_AI_GATEWAY_TOKEN"),
    )
    model = "openai/gpt-5.2"  # Add vendor prefix
else:
    client = OpenAI(
        api_key=os.getenv("OPENAI_API_KEY")
    )
    model = "gpt-5.2"  # Original model name

response = client.chat.completions.create(
    model=model,
    messages=[{"role": "user", "content": "Hello"}]
)
----


Better: Abstraction function

[source,python]
----
from openai import OpenAI
import os

def get_llm_client():
    """Returns configured OpenAI client (direct or gateway-routed)"""
    use_gateway = os.getenv("USE_AI_GATEWAY", "false").lower() == "true"

    if use_gateway:
        return OpenAI(
            base_url=os.getenv("REDPANDA_AI_GATEWAY_URL"),
            api_key=os.getenv("REDPANDA_AI_GATEWAY_TOKEN"),
            )
    else:
        return OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def get_model_name(base_model: str) -> str:
    """Returns model name with vendor prefix if using gateway"""
    use_gateway = os.getenv("USE_AI_GATEWAY", "false").lower() == "true"
    return f"openai/{base_model}" if use_gateway else base_model

# Usage
client = get_llm_client()
response = client.chat.completions.create(
    model=get_model_name("gpt-5.2"),
    messages=[{"role": "user", "content": "Hello"}]
)
----


==== Option B: Anthropic SDK

Before (Direct Anthropic)

[source,python]
----
from anthropic import Anthropic

client = Anthropic(
    api_key=os.getenv("ANTHROPIC_API_KEY")
)

response = client.messages.create(
    model="claude-sonnet-4.5",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Hello"}]
)
----


After (Gateway via OpenAI-compatible wrapper)

Because AI Gateway provides an OpenAI-compatible endpoint, we recommend migrating Anthropic SDK usage to OpenAI SDK for consistency:

[source,python]
----
from openai import OpenAI
import os

use_gateway = os.getenv("USE_AI_GATEWAY", "false").lower() == "true"

if use_gateway:
    # Use OpenAI SDK with gateway
    client = OpenAI(
        base_url=os.getenv("REDPANDA_AI_GATEWAY_URL"),
        api_key=os.getenv("REDPANDA_AI_GATEWAY_TOKEN"),
    )

    response = client.chat.completions.create(
        model="anthropic/claude-sonnet-4.5",
        max_tokens=1024,
        messages=[{"role": "user", "content": "Hello"}]
    )
else:
    # Keep existing Anthropic SDK
    from anthropic import Anthropic
    client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

    response = client.messages.create(
        model="claude-sonnet-4.5",
        max_tokens=1024,
        messages=[{"role": "user", "content": "Hello"}]
    )
----


Alternative: Use OpenAI client for OpenAI-compatible gateway

[source,python]
----
from openai import OpenAI

use_gateway = os.getenv("USE_AI_GATEWAY", "false").lower() == "true"

if use_gateway:
    client = OpenAI(
        base_url=os.getenv("REDPANDA_AI_GATEWAY_URL"),
        api_key=os.getenv("REDPANDA_AI_GATEWAY_TOKEN"),
    )
else:
    from anthropic import Anthropic
    client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
----


==== Option C: Multiple providers

Before (Separate SDKs)

[source,python]
----
from openai import OpenAI
from anthropic import Anthropic

openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
anthropic_client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

# Different code paths
if use_openai:
    response = openai_client.chat.completions.create(...)
else:
    response = anthropic_client.messages.create(...)
----


After (Unified via Gateway)

[source,python]
----
from openai import OpenAI

# Single client for all providers
client = OpenAI(
    base_url=os.getenv("REDPANDA_AI_GATEWAY_URL"),
    api_key=os.getenv("REDPANDA_AI_GATEWAY_TOKEN"),
)

# Same code, different models
if use_openai:
    response = client.chat.completions.create(
        model="openai/gpt-5.2",
        messages=[...]
    )
else:
    response = client.chat.completions.create(
        model="anthropic/claude-sonnet-4.5",
        messages=[...]
    )
----


=== Test gateway connection

Before changing the feature flag, verify gateway connectivity:

Python Test Script

[source,python]
----
from openai import OpenAI
import os

def test_gateway_connection():
    client = OpenAI(
        base_url=os.getenv("REDPANDA_AI_GATEWAY_URL"),
        api_key=os.getenv("REDPANDA_AI_GATEWAY_TOKEN"),
    )

    try:
        response = client.chat.completions.create(
            model="openai/gpt-5.2-mini",  # Use cheap model for testing
            messages=[{"role": "user", "content": "Test"}],
            max_tokens=10
        )
        print("✅ Gateway connection successful")
        print(f"Response: {response.choices[0].message.content}")
        return True
    except Exception as e:
        print(f"❌ Gateway connection failed: {e}")
        return False

if __name__ == "__main__":
    test_gateway_connection()
----


Expected output:

[source,text]
----
Gateway connection successful
Response: Hello
----


Common issues:

* `401 Unauthorized` → Check `REDPANDA_AI_GATEWAY_TOKEN`
* `404 Not Found` → Check `REDPANDA_AI_GATEWAY_URL` (should end with `/v1/chat/completions` or base path)
* `Model not found` → Ensure model is enabled in gateway configuration
* `Model not found` → Ensure model is enabled in gateway configuration

=== Verify in observability dashboard

After successful test:

1. Open AI Gateway observability dashboard
2. In the sidebar, navigate to *Agentic > AI Gateway > Routers > {GATEWAY_NAME}*, then select the *Logs* tab.
3. Verify your test request appears
4. Check fields:
   * Model: `openai/gpt-5.2-mini`
   * Provider: OpenAI
   * Status: 200
   * Token count: ~10 prompt + ~10 completion
   * Cost: // PLACEHOLDER: expected cost

*If request doesn't appear*: Verify gateway ID and authentication token are correct.

=== Enable gateway for subset of traffic

Gradually roll out gateway usage:

Staged rollout strategy:

1. *Week 1*: Internal testing only (dev team accounts)
2. *Week 2*: 10% of production traffic
3. *Week 3*: 50% of production traffic
4. *Week 4*: 100% of production traffic

Implementation options:

Option A: Environment-based

[source,python]
----
# Enable gateway in staging first
use_gateway = os.getenv("ENVIRONMENT") in ["staging", "production"]
----


Option B: Percentage-based

[source,python]
----
import random

# Route 10% of traffic through gateway
use_gateway = random.random() < 0.10
----


Option C: User-based

[source,python]
----
# Enable for internal users first
use_gateway = user.email.endswith("@yourcompany.com")
----


Option D: Feature flag service (recommended)

[source,python]
----
# LaunchDarkly, Split.io, etc.
use_gateway = feature_flags.is_enabled("ai-gateway", user_context)
----


=== Monitor and compare

During parallel operation, compare metrics:

Metrics to monitor:

[cols="2,1,1,3"]
|===
| Metric | Direct | Gateway | Notes

| Success rate 
| // track 
| // track 
| Should be identical

| Latency p50 
| // track 
| // track 
| Gateway adds ~// PLACEHOLDER: Xms

| Latency p99 
| // track 
| // track 
| Watch for outliers

| Error rate 
| // track 
| // track 
| Should be identical

| Cost per 1K requests 
| // track 
| // track 
| Compare estimated costs
|===

Monitoring code example:

[source,python]
----
import time

def call_llm_with_metrics(use_gateway: bool, model: str, messages: list):
    start_time = time.time()

    try:
        client = get_llm_client(use_gateway)
        response = client.chat.completions.create(
            model=model,
            messages=messages
        )

        latency = time.time() - start_time

        # Log metrics
        metrics.record("llm.request.success", 1, tags={
            "routing": "gateway" if use_gateway else "direct",
            "model": model
        })
        metrics.record("llm.request.latency", latency, tags={
            "routing": "gateway" if use_gateway else "direct"
        })

        return response

    except Exception as e:
        metrics.record("llm.request.error", 1, tags={
            "routing": "gateway" if use_gateway else "direct",
            "error": str(e)
        })
        raise
----


=== Full cutover

Once metrics confirm gateway reliability:

1. Set feature flag to 100%:
+
[source,bash]
----
USE_AI_GATEWAY=true
----

2. Deploy updated configuration

3. Monitor for 24-48 hours

4. Remove direct provider credentials (optional, for security):
+
[source,bash]
----
# .env
# OPENAI_API_KEY=sk-...  # Remove after confirming gateway stability
# ANTHROPIC_API_KEY=sk-ant-...  # Remove after confirming gateway stability

REDPANDA_AI_GATEWAY_URL=<your-gateway-endpoint>
REDPANDA_AI_GATEWAY_TOKEN={YOUR_TOKEN}
----

5. Remove direct integration code (optional, for cleanup):
+
[source,python]
----
# Remove feature flag logic, keep only gateway path
client = OpenAI(
    base_url=os.getenv("REDPANDA_AI_GATEWAY_URL"),
    api_key=os.getenv("REDPANDA_AI_GATEWAY_TOKEN"),
)
----

== Rollback procedure

If issues arise, rollback is simple:

Emergency rollback (< 1 minute):

[source,bash]
----
# Set feature flag back to false
USE_AI_GATEWAY=false

# Restart application (if needed)
----


Gradual rollback:

[source,python]
----
# Reduce gateway traffic percentage
use_gateway = random.random() < 0.50  # Back to 50%
use_gateway = random.random() < 0.10  # Back to 10%
use_gateway = False  # Back to 0%
----


*Keep direct provider credentials until you're confident in gateway stability.*

== Framework-specific migration

[tabs]
======
LangChain::
+
--
Before

[source,python]
----
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-5.2",
    api_key=os.getenv("OPENAI_API_KEY")
)
----

After

[source,python]
----
from langchain_openai import ChatOpenAI

use_gateway = os.getenv("USE_AI_GATEWAY", "false").lower() == "true"

if use_gateway:
    llm = ChatOpenAI(
        model="openai/gpt-5.2",
        base_url=os.getenv("REDPANDA_AI_GATEWAY_URL"),
        api_key=os.getenv("REDPANDA_AI_GATEWAY_TOKEN"),
    )
else:
    llm = ChatOpenAI(
        model="gpt-5.2",
        api_key=os.getenv("OPENAI_API_KEY")
    )
----
--

LlamaIndex::
+
--
Before

[source,python]
----
from llama_index.llms.openai import OpenAI

llm = OpenAI(model="gpt-5.2")
----

After

[source,python]
----
from llama_index.llms.openai import OpenAI

use_gateway = os.getenv("USE_AI_GATEWAY", "false").lower() == "true"

if use_gateway:
    llm = OpenAI(
        model="openai/gpt-5.2",
        api_base=os.getenv("REDPANDA_AI_GATEWAY_URL"),
        api_key=os.getenv("REDPANDA_AI_GATEWAY_TOKEN"),
    )
else:
    llm = OpenAI(model="gpt-5.2")
----
--

Vercel AI SDK::
+
--
Before

[source,typescript]
----
import { openai } from '@ai-sdk/openai';

const model = openai('gpt-5.2');
----

After

[source,typescript]
----
import { createOpenAI } from '@ai-sdk/openai';
import { openai } from '@ai-sdk/openai';

const useGateway = process.env.USE_AI_GATEWAY === 'true';

const model = useGateway
  ? createOpenAI({
      baseURL: process.env.REDPANDA_AI_GATEWAY_URL,
      apiKey: process.env.REDPANDA_AI_GATEWAY_TOKEN,
    })('openai/gpt-5.2')
  : openai('gpt-5.2');
----
--
======

== Migration checklist

Use this checklist to track your migration:

*Prerequisites*

  * [ ] Gateway configured and tested
  * [ ] Providers enabled
  * [ ] Models enabled
  * [ ] Gateway ID and endpoint URL obtained

*Code Changes*

  * [ ] Environment variables added
  * [ ] Feature flag implemented
  * [ ] Client initialization updated
  * [ ] Model name prefix added (vendor/model_id)
  * [ ] Authentication configured (API token)

*Testing*

  * [ ] Gateway connection test passes
  * [ ] Test request visible in observability dashboard
  * [ ] Integration tests pass with gateway
  * [ ] End-to-end tests pass with gateway

*Staged rollout*

  * [ ] Week 1: Internal testing (dev team only)
  * [ ] Week 2: 10% production traffic
  * [ ] Week 3: 50% production traffic
  * [ ] Week 4: 100% production traffic

*Monitoring*

  * [ ] Success rate comparison (direct vs gateway)
  * [ ] Latency comparison (direct vs gateway)
  * [ ] Error rate comparison (direct vs gateway)
  * [ ] Cost comparison (direct vs gateway)

*Cleanup* (optional, after 30 days stable)

  * [ ] Remove direct provider credentials
  * [ ] Remove feature flag logic
  * [ ] Update documentation
  * [ ] Archive direct integration code

== Common migration issues

=== "Model not found" error

Symptom:
[source,text]
----
Error: Model 'openai/gpt-5.2' not found
----


Causes:

1. Model not enabled in gateway configuration
2. Wrong model name format (missing vendor prefix)
3. Typo in model name

Solution:

1. Verify model is enabled: In the sidebar, navigate to *Agentic > AI Gateway > Models* and confirm the model is enabled.
2. Confirm format: `vendor/model_id` (for example, `openai/gpt-5.2`, not `gpt-5.2` without prefix)
3. Check supported models: // PLACEHOLDER: link to model catalog

=== Higher latency than expected

Expected gateway overhead: // PLACEHOLDER: Xms p50, Yms p99

If latency is significantly higher:

1. Check geographic routing (gateway → provider region)
2. Verify provider pool configuration (no unnecessary fallbacks)
3. Review CEL routing complexity
4. Check for rate limiting (adds retry latency)

Solution: Review geographic routing and provider pool configuration.

=== Requests not appearing in dashboard

Causes:

1. Wrong gateway ID
2. Request failed before reaching gateway
3. UI delay (logs may take // PLACEHOLDER: Xs to appear)

Solution: Verify gateway ID and check for UI delay (logs may take a few seconds to appear).

=== Different response format

Symptom: Response structure differs between direct and gateway

AI Gateway returns OpenAI-compatible responses for all providers. Anthropic responses are automatically transformed to match the OpenAI response format. If you encounter differences, file a support ticket with the request ID from logs.

== Advanced migration scenarios

=== Custom request timeouts

Before

[source,python]
----
client = OpenAI(api_key=..., timeout=30.0)
----


After

[source,python]
----
client = OpenAI(
    base_url=os.getenv("REDPANDA_AI_GATEWAY_URL"),
    api_key=os.getenv("REDPANDA_AI_GATEWAY_TOKEN"),
    timeout=30.0  # Still supported
)
----


=== Streaming responses

Before

[source,python]
----
stream = client.chat.completions.create(
    model="gpt-5.2",
    messages=[...],
    stream=True
)

for chunk in stream:
    print(chunk.choices[0].delta.content, end="")
----


After

[source,python]
----
stream = client.chat.completions.create(
    model="openai/gpt-5.2",  # Add vendor prefix
    messages=[...],
    stream=True
)

for chunk in stream:
    print(chunk.choices[0].delta.content, end="")
----


=== Custom headers (for example, user tracking)

Before

[source,python]
----
response = client.chat.completions.create(
    model="gpt-5.2",
    messages=[...],
    extra_headers={"X-User-ID": user.id}
)
----


After

[source,python]
----
response = client.chat.completions.create(
    model="openai/gpt-5.2",
    messages=[...],
    extra_headers={
        "X-User-ID": user.id,  # Custom headers still supported
    }
)
----


NOTE: Gateway may use custom headers for routing (for example, CEL expressions can reference `request.headers["X-User-ID"]`)

== Post-migration benefits

After successful migration, you gain:

Simplified provider management

[source,python]
----
# Switch providers with one config change (no code changes)
model = "anthropic/claude-sonnet-4.5"  # Was openai/gpt-5.2
----

Unified observability

* All requests in one dashboard
* Cross-provider cost comparison
* Session reconstruction across models

Automatic failover

* Configure once, benefit everywhere
* No application-level retry logic needed

Cost controls

* Enforce budgets centrally
* Rate limit per team/customer
* No surprises in cloud bills

A/B testing

* Test new models without code changes
* Compare quality/cost/latency
* Gradual rollout via routing policies

== Next steps

* xref:ai-agents:ai-gateway/cel-routing-cookbook.adoc[]: Configure advanced routing policies.
* xref:ai-agents:ai-gateway/mcp-aggregation-guide.adoc[]: Explore MCP aggregation.
