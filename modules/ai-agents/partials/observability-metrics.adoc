= Observability: metrics & analytics

== Overview

AI Gateway provides aggregate metrics and analytics dashboards to help you understand usage patterns, costs, performance, and errors across all your LLM traffic.

*Use Metrics For*:
* Cost tracking and budget management
* Usage trends over time
* Performance monitoring (latency, error rates)
* Capacity planning
* Model/provider comparison

*Use Logs For*: Debugging specific requests, viewing full prompts/responses → See [Observability: Logs](// PLACEHOLDER: link)

== Where to find metrics

// PLACEHOLDER: Add exact UI navigation path

1. *Navigate to Analytics Dashboard*:
   * Console → AI Gateway → // PLACEHOLDER: exact path
   * Or: Gateway detail page → Analytics tab

2. *Select Gateway* (optional):
   * View all gateways (org-wide metrics)
   * Or filter to specific gateway

3. *Set Time Range*:
   * Default: Last 7 days
   * Options: Last 24 hours, 7 days, 30 days, 90 days, Custom
   * // PLACEHOLDER: screenshot of time range picker

== Key metrics

=== Request volume

*What it shows*: Total number of requests over time

// PLACEHOLDER: Screenshot of request volume graph

*Graph Type*: Time series line chart

*Filters*:
* By gateway
* By model
* By provider
* By status (success/error)

*Use Cases*:
* Identify usage patterns (peak hours, days of week)
* Detect traffic spikes or drops
* Capacity planning

*Example Insights*:
* "Traffic doubles every Monday morning at 9am" → Scale infrastructure
* "Staging gateway has more traffic than prod" → Investigate runaway testing

=== Token usage

*What it shows*: Prompt, completion, and total tokens consumed

// PLACEHOLDER: Screenshot of token usage graph

*Graph Type*: Stacked area chart (prompt vs completion tokens)

*Metrics*:
* Total tokens
* Prompt tokens (input)
* Completion tokens (output)
* Tokens per request (average)

*Breakdowns*:
* By gateway
* By model
* By provider

*Use Cases*:
* Understand cost drivers (prompt vs completion tokens)
* Identify verbose prompts or responses
* Optimize token usage

*Example Insights*:
* "90% of tokens are completion tokens" → Responses are verbose, optimize max_tokens
* "Staging uses 10x more tokens than prod" → Investigate test suite

=== Estimated spend

*What it shows*: Calculated cost based on token usage and public pricing

// PLACEHOLDER: Screenshot of cost tracking dashboard

*Graph Type*: Time series line chart with cost breakdown

*Metrics*:
* Total estimated spend
* Spend by model
* Spend by provider
* Spend by gateway
* Cost per 1K requests
* Cost per 1M tokens

*Breakdowns*:
* By gateway (for chargeback/showback)
* By model (for cost optimization)
* By provider (for negotiation leverage)
* By custom header (if configured, e.g., `x-customer-id`)

*Use Cases*:
* Budget tracking ("Are we staying under $50K/month?")
* Cost attribution ("Which team spent the most?")
* Model comparison ("Is Claude cheaper than GPT-4 for our use case?")
* Forecasting ("At this rate, we'll spend $X next month")

*Important Notes*:
* *Estimates based on public pricing* (may differ from your contract)
* *Not a substitute for provider invoices* (use for approximation only)
* *Update frequency*: // PLACEHOLDER: Real-time? Hourly? Daily?

*Example Insights*:
* "Customer A accounts for 60% of spend" → Consider rate limits or tiered pricing
* "GPT-4o is 3x more expensive than Claude Sonnet for similar quality" → Optimize routing

=== Latency

*What it shows*: Request duration from gateway to provider and back

// PLACEHOLDER: Screenshot of latency histogram

*Metrics*:
* p50 (median) latency
* p95 latency
* p99 latency
* Min/max latency
* Average latency

*Breakdowns*:
* By gateway
* By model
* By provider
* By token range (longer responses = higher latency)

*Use Cases*:
* Identify slow models or providers
* Set SLO targets (e.g., "p95 < 2 seconds")
* Detect performance regressions

*Example Insights*:
* "GPT-4o p99 latency spiked to 10 seconds yesterday" → Investigate provider issue
* "Claude Sonnet is 30% faster than GPT-4o for same prompts" → Optimize for latency

*Latency Components* (if available):
// PLACEHOLDER: Does gateway show latency breakdown?
* Gateway processing time
* Provider API time
* Network time

=== Error rate

*What it shows*: Percentage of failed requests over time

// PLACEHOLDER: Screenshot of error rate graph

*Metrics*:
* Total error rate (%)
* Errors by status code (400, 401, 429, 500, etc.)
* Errors by model
* Errors by provider

*Graph Type*: Time series line chart with error percentage

*Breakdowns*:
* By error type:
  * Client errors (4xx)
  * Rate limits (429)
  * Server errors (5xx)
  * Provider errors
  * Gateway errors

*Use Cases*:
* Detect provider outages
* Identify configuration issues (e.g., model not enabled)
* Monitor rate limit breaches

*Example Insights*:
* "Error rate spiked to 15% at 2pm" → OpenAI outage, fallback to Anthropic worked
* "10% of requests fail with 'model not found'" → Model not enabled in gateway

=== Success rate

*What it shows*: Percentage of successful (200) requests over time

*Metric*: `Success Rate = (Successful Requests / Total Requests) × 100`

*Target*: Typically 99%+ for production workloads

*Use Cases*:
* Monitor overall health
* Set up alerts (e.g., "Alert if success rate < 95%")

=== Fallback rate

*What it shows*: Percentage of requests that used fallback provider

// PLACEHOLDER: Screenshot of fallback rate graph

*Metric*: `Fallback Rate = (Fallback Requests / Total Requests) × 100`

*Breakdowns*:
* By fallback reason:
  * Rate limit exceeded
  * Timeout
  * 5xx error

*Use Cases*:
* Monitor primary provider reliability
* Verify fallback is working
* Identify when to renegotiate rate limits

*Example Insights*:
* "Fallback rate increased to 20% yesterday" → OpenAI hit rate limits, time to increase quota
* "Zero fallbacks in 30 days" → Fallback config may not be working, or primary provider is very reliable

== Dashboard views

=== Overview dashboard

*Shows*: High-level metrics across all gateways

// PLACEHOLDER: Screenshot of overview dashboard

*Widgets*:
* Total requests (last 24h, 7d, 30d)
* Total spend (last 24h, 7d, 30d)
* Success rate (current)
* Average latency (current)
* Top 5 models by request volume
* Top 5 gateways by spend

*Use Case*: Executive view, health at a glance

=== Gateway dashboard

*Shows*: Metrics for a specific gateway

// PLACEHOLDER: Screenshot of gateway dashboard

*Widgets*:
* Request volume (time series)
* Token usage (time series)
* Estimated spend (time series)
* Latency percentiles (histogram)
* Error rate (time series)
* Model breakdown (pie chart)
* Provider breakdown (pie chart)

*Use Case*: Team-specific monitoring, gateway optimization

=== Model comparison dashboard

*Shows*: Side-by-side comparison of models

// PLACEHOLDER: Screenshot of model comparison

*Metrics per Model*:
* Request count
* Total tokens
* Estimated cost
* Cost per 1K requests
* Average latency
* Error rate

*Use Case*: Evaluate whether to switch models (cost vs performance)

*Example*:
| Model | Requests | Avg Latency | Cost per 1K | Error Rate |
|-------|----------|-------------|-------------|------------|
| openai/gpt-4o | 10,000 | 1.2s | $5.00 | 0.5% |
| anthropic/claude-sonnet-3.5 | 5,000 | 0.9s | $3.50 | 0.3% |
| openai/gpt-4o-mini | 20,000 | 0.7s | $0.50 | 1.0% |

*Insight*: Claude Sonnet is 25% faster and 30% cheaper than GPT-4o with better reliability

=== Provider comparison dashboard

*Shows*: Side-by-side comparison of providers

*Metrics per Provider*:
* Request count
* Total spend
* Average latency
* Error rate
* Fallback trigger rate

*Use Case*: Evaluate provider reliability, negotiate contracts

=== Cost breakdown dashboard

*Shows*: Detailed cost analysis

// PLACEHOLDER: Screenshot of cost breakdown

*Widgets*:
* Spend by gateway (stacked bar chart)
* Spend by model (pie chart)
* Spend by provider (pie chart)
* Spend by custom dimension (if configured, e.g., customer ID)
* Spend trend (time series with forecast)
* Budget utilization (progress bar: $X / $Y monthly limit)

*Use Case*: FinOps, budget management, chargeback/showback

== Filter & group

=== Filter by gateway

[source,text]
----
Filter: Gateway = "production-gateway"
----


Shows metrics for specific gateway only.

*Use Case*: Isolate prod from staging metrics

=== Filter by model

[source,text]
----
Filter: Model = "openai/gpt-4o"
----


Shows metrics for specific model only.

*Use Case*: Evaluate model performance in isolation

=== Filter by provider

[source,text]
----
Filter: Provider = "OpenAI"
----


Shows metrics for specific provider only.

*Use Case*: Evaluate provider reliability

=== Filter by status

[source,text]
----
Filter: Status = "200"  // Only successful requests
Filter: Status >= "500"  // Only server errors
----


*Use Case*: Focus on errors, or calculate success rate

=== Filter by custom dimension

// PLACEHOLDER: Confirm if custom dimensions are supported for filtering

[source,text]
----
Filter: request.headers["x-customer-id"] = "customer_abc"
----


Shows metrics for specific customer.

*Use Case*: Customer-specific cost tracking for chargeback

=== Group by dimension

*Common Groupings*:
* Group by Gateway
* Group by Model
* Group by Provider
* Group by Status
* Group by Hour/Day/Week/Month (time aggregation)

*Example*: "Show me spend grouped by model, for production gateway, over last 30 days"

== Alerting

// PLACEHOLDER: Confirm if alerting is supported

*If Alerting is Supported*:

=== Alert types

*Budget Alerts*:
* Alert when spend exceeds X% of monthly budget
* Alert when spend grows Y% week-over-week

*Performance Alerts*:
* Alert when error rate > X%
* Alert when p99 latency > Xms
* Alert when success rate < X%

*Usage Alerts*:
* Alert when request volume drops (potential outage)
* Alert when fallback rate > X% (primary provider issue)

=== Alert channels

// PLACEHOLDER: Supported notification channels
* Email
* Slack
* PagerDuty
* Webhook
* // PLACEHOLDER: Others?

=== Example alert configuration

[source,yaml]
----
# PLACEHOLDER: Actual alert configuration format
alerts:
  - name: "High Error Rate"
    condition: error_rate > 5%
    duration: 5 minutes
    channels: [slack, email]

  - name: "Budget Threshold"
    condition: monthly_spend > 80% of budget
    channels: [email]

  - name: "Latency Spike"
    condition: p99_latency > 5000ms
    duration: 10 minutes
    channels: [pagerduty]
----


See [Alerting Guide](// PLACEHOLDER: link) for detailed setup.

== Export metrics

// PLACEHOLDER: Confirm export capabilities

=== Export to CSV

1. Apply filters for desired metrics
2. Click "Export to CSV"
3. Download includes time series data

*Use Case*: Import into spreadsheet for analysis, reporting

=== Export via API

// PLACEHOLDER: If API is available for metrics

[source,bash]
----
curl https://{CLUSTER_ID}.cloud.redpanda.com/api/ai-gateway/metrics \
  -H "Authorization: Bearer ${REDPANDA_CLOUD_TOKEN}" \
  -G \
  --data-urlencode "gateway_id=gw_abc123" \
  --data-urlencode "start_time=2025-01-01T00:00:00Z" \
  --data-urlencode "end_time=2025-01-31T23:59:59Z" \
  --data-urlencode "metric=requests,tokens,cost"
----


*Response*:
[source,json]
----
{
  "gateway_id": "gw_abc123",
  "start_time": "2025-01-01T00:00:00Z",
  "end_time": "2025-01-31T23:59:59Z",
  "metrics": {
    "requests": 1000000,
    "tokens": 500000000,
    "estimated_cost": 2500.00
  }
}
----


=== Integration with observability platforms

// PLACEHOLDER: OpenTelemetry support? Other integrations?

*Supported Integrations* (if any):
* *Prometheus*: Metrics endpoint for scraping
* *OpenTelemetry*: Export metrics to OTel collector
* *Datadog*: Direct integration
* *Grafana*: Pre-built dashboards
* // PLACEHOLDER: Others?

See [Observability Integrations](// PLACEHOLDER: link) for setup guides.

== Common analysis tasks

=== Task 1: "are we staying within budget?"

1. *View Cost Breakdown Dashboard*
2. *Check Budget Utilization Widget*:
   * Current spend: $X
   * Monthly budget: $Y
   * Utilization: X%
   * Days remaining in month: Z
3. *Forecast*:
   * At current rate: $X × (30 / days_elapsed)
   * On track to exceed budget? Yes/No

*Action*:
* If approaching limit: Adjust rate limits, optimize models, pause non-prod usage
* If well under budget: Opportunity to test more expensive models

=== Task 2: "which team is using the most resources?"

1. *Filter by Gateway* (assuming one gateway per team)
2. *Sort by Spend* (descending)
3. *View Table*:

| Gateway | Requests | Tokens | Spend | % of Total |
|---------|----------|--------|-------|------------|
| team-ml | 500K | 250M | $1,250 | 50% |
| team-product | 300K | 150M | $750 | 30% |
| team-eng | 200K | 100M | $500 | 20% |

*Action*: Chargeback costs to teams, or investigate high-usage teams

=== Task 3: "is this model worth the extra cost?"

1. *Open Model Comparison Dashboard*
2. *Select Models to Compare*:
   * Expensive model: `openai/gpt-4o`
   * Cheap model: `openai/gpt-4o-mini`
3. *Compare Metrics*:

| Metric | GPT-4o | GPT-4o-mini | Difference |
|--------|--------|-------------|------------|
| Cost per 1K requests | $5.00 | $0.50 | *10x* |
| Avg Latency | 1.2s | 0.7s | 58% *faster* (mini) |
| Error Rate | 0.5% | 1.0% | 2x errors (mini) |

*Decision*: If mini's error rate is acceptable, save 10x on costs

=== Task 4: "why did costs spike yesterday?"

1. *View Cost Trend Graph*
2. *Identify Spike* (e.g., Jan 10th: $500 vs usual $100)
3. *Drill Down*:
   * *By Gateway*: Which gateway caused the spike?
   * *By Model*: Did someone switch to expensive model?
   * *By Hour*: What time did spike occur?
4. *Cross-Reference with Logs*:
   * Filter logs to spike timeframe
   * Check for unusual request patterns
   * Identify custom header (user ID, customer ID) if present

*Common Causes*:
* Test suite running against prod gateway
* A/B test routing all traffic to expensive model
* User error (wrong model in config)
* Runaway loop in application code

=== Task 5: "is provider x more reliable than provider y?"

1. *Open Provider Comparison Dashboard*
2. *Compare Error Rates*:

| Provider | Requests | Error Rate | Fallback Triggers |
|----------|----------|------------|-------------------|
| OpenAI | 500K | 0.8% | 50 (rate limits) |
| Anthropic | 300K | 0.3% | 5 (timeouts) |

*Insight*: Anthropic has 62% lower error rate

3. *Compare Latencies*:

| Provider | p50 Latency | p99 Latency |
|----------|-------------|-------------|
| OpenAI | 1.0s | 3.5s |
| Anthropic | 0.8s | 2.5s |

*Insight*: Anthropic is 20% faster at p50, 28% faster at p99

*Decision*: Prioritize Anthropic in routing pools

== Metrics retention

// PLACEHOLDER: Confirm metrics retention policy

*Retention Period*:
* *High-resolution* (1-minute granularity): // PLACEHOLDER: e.g., 7 days
* *Medium-resolution* (1-hour granularity): // PLACEHOLDER: e.g., 30 days
* *Low-resolution* (1-day granularity): // PLACEHOLDER: e.g., 1 year

*Note*: Aggregate metrics retained longer than individual request logs

== API access to metrics

// PLACEHOLDER: Document metrics API if available

=== List available metrics

[source,bash]
----
curl https://{CLUSTER_ID}.cloud.redpanda.com/api/ai-gateway/metrics/list \
  -H "Authorization: Bearer ${REDPANDA_CLOUD_TOKEN}"
----


*Response*:
[source,json]
----
{
  "metrics": [
    "requests",
    "tokens.prompt",
    "tokens.completion",
    "tokens.total",
    "cost.estimated",
    "latency.p50",
    "latency.p95",
    "latency.p99",
    "errors.rate",
    "success.rate",
    "fallback.rate"
  ]
}
----


=== Query specific metric

[source,bash]
----
curl https://{CLUSTER_ID}.cloud.redpanda.com/api/ai-gateway/metrics/query \
  -H "Authorization: Bearer ${REDPANDA_CLOUD_TOKEN}" \
  -H "Content-Type: application/json" \
  -d '{
    "metric": "requests",
    "gateway_id": "gw_abc123",
    "start_time": "2025-01-01T00:00:00Z",
    "end_time": "2025-01-31T23:59:59Z",
    "granularity": "1d",
    "group_by": ["model"]
  }'
----


*Response*:
[source,json]
----
{
  "metric": "requests",
  "granularity": "1d",
  "data": [
    {
      "timestamp": "2025-01-01T00:00:00Z",
      "model": "openai/gpt-4o",
      "value": 10000
    },
    {
      "timestamp": "2025-01-01T00:00:00Z",
      "model": "anthropic/claude-sonnet-3.5",
      "value": 5000
    },
    ...
  ]
}
----


== Best practices

=== 1. set up budget alerts early
* Don't wait for surprise bills
* Alert at 50%, 80%, 90% of budget
* Include multiple stakeholders (eng, finance)

=== 2. create team dashboards
* One dashboard per team showing their gateway(s)
* Empowers teams to self-optimize
* Reduces central ops burden

=== 3. monitor fallback rate
* Low fallback rate (0-5%): Normal, failover working
* High fallback rate (>20%): Investigate primary provider issues
* Zero fallback rate: Verify fallback config is correct

=== 4. compare models regularly
* Run A/B tests with metrics
* Reassess as pricing and models change
* Don't assume expensive = better quality for your use case

=== 5. track trends, not point-in-time
* Day-to-day variance is normal
* Look for week-over-week and month-over-month trends
* Seasonal patterns (e.g., more usage on weekdays)

== Troubleshoot metrics issues

=== Issue: "metrics don't match my provider invoice"

*Possible Causes*:
1. Metrics are estimates based on public pricing
2. Your contract has custom pricing
3. Provider changed pricing mid-month

*Solution*:
* Use metrics for trends and optimization decisions
* Use provider invoices for actual billing
* // PLACEHOLDER: Can users configure custom pricing in gateway?

=== Issue: "metrics are delayed or missing"

*Possible Causes*:
1. Metrics aggregation has delay (// PLACEHOLDER: typical delay?)
2. Time range outside retention period
3. No requests in selected time range (empty data)

*Solution*:
1. Wait and refresh (// PLACEHOLDER: Xminutes typical delay)
2. Check retention policy
3. Verify requests were sent (check logs)

=== Issue: "dashboard shows 'no data'"

*Possible Causes*:
1. Filters too restrictive (no matching requests)
2. Gateway has no traffic yet
3. Permissions issue (can't access this gateway's metrics)

*Solution*:
1. Remove filters, widen time range
2. Send test request (see [Quickstart](// PLACEHOLDER: link))
3. Check permissions with admin

== Next steps

* *View Individual Requests* → [Observability: Logs](// PLACEHOLDER: link)
* *Set Up Alerts* → [Alerting Guide](// PLACEHOLDER: link)
* *Optimize Costs* → [Cost Optimization Guide](// PLACEHOLDER: link)
* *Export to BI Tools* → [Data Export Guide](// PLACEHOLDER: link)
* *Compare Models* → [Model Selection Guide](// PLACEHOLDER: link)

== Related pages

* [Observability: Logs](// PLACEHOLDER: link)
* [Budget & Rate Limits](// PLACEHOLDER: link)
* [Cost Optimization](// PLACEHOLDER: link)
* [Performance Benchmarks](// PLACEHOLDER: link)
