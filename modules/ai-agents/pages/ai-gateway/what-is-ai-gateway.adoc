= What is an AI Gateway?
:description: Understand what an AI Gateway is, the problems it solves, and how it benefits your AI infrastructure.
:page-topic-type: concept
:personas: app_developer, platform_admin
:learning-objective-1: Describe how AI Gateway centralizes LLM provider management and reduces operational complexity
:learning-objective-2: Identify key features that address common LLM integration challenges
:learning-objective-3: Determine whether AI Gateway fits your use case based on traffic volume and provider diversity

include::ai-agents:partial$ai-gateway-byoc-note.adoc[]

Redpanda AI Gateway is a unified access layer for LLM providers and AI tools that sits between your applications and the AI services they use. It provides centralized routing, policy enforcement, cost management, and observability for all your AI traffic.

== The problem

Modern AI applications face four critical challenges that increase costs, reduce reliability, and slow down development.

First, applications typically hardcode provider-specific SDKs. An application using OpenAI's SDK cannot easily switch to Anthropic or Google without code changes and redeployment. This tight coupling makes testing across providers time-consuming and error-prone, and means provider outages directly impact your application availability.

Second, costs can spiral without visibility into usage patterns. Without a centralized view of token consumption across teams and applications, it's difficult to attribute costs to specific customers, features, or environments. Testing and debugging can generate unexpected bills, and there's no way to enforce budgets or rate limits per team or customer.

Third, AI agents that use MCP (Model Context Protocol) servers face tool coordination challenges. Managing tool discovery and execution is repetitive across projects, and agents typically load all available tools upfront, which creates high token costs. There's also no centralized governance over which tools agents can access.

Finally, observability is fragmented across provider dashboards. You cannot reconstruct user sessions that span multiple models, compare latency and costs across providers in a unified view, or efficiently debug issues. Troubleshooting "the AI gave the wrong answer" requires manual log diving across different systems.

== What AI Gateway solves

Redpanda AI Gateway addresses these challenges through four core capabilities:

=== 1. Unified LLM access (single endpoint for all providers)

AI Gateway provides a single OpenAI-compatible endpoint that routes requests to multiple LLM providers. Instead of integrating with each provider's SDK separately, you configure your application once and switch providers by changing only the model parameter.

Without AI Gateway, you need different SDKs and patterns for each provider:

[source,python]
----
# OpenAI
from openai import OpenAI
client = OpenAI(api_key="sk-...")
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Hello"}]
)

# Anthropic (different SDK, different patterns)
from anthropic import Anthropic
client = Anthropic(api_key="sk-ant-...")
response = client.messages.create(
    model="claude-sonnet-3.5",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Hello"}]
)
----

With AI Gateway, you use the OpenAI SDK for all providers:

[source,python]
----
from openai import OpenAI

# Single configuration, multiple providers
client = OpenAI(
    base_url="https://{GATEWAY_ENDPOINT}",
    api_key="your-redpanda-token",
    default_headers={"rp-aigw-id": "{GATEWAY_ID}"}
)

# Route to OpenAI
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Hello"}]
)

# Route to Anthropic (same code, different model string)
response = client.chat.completions.create(
    model="anthropic/claude-sonnet-3.5",
    messages=[{"role": "user", "content": "Hello"}]
)
----

To switch providers, you change only the `model` parameter from `openai/gpt-4o` to `anthropic/claude-sonnet-3.5`. No code changes or redeployment needed.

=== 2. Policy-based routing and cost control

AI Gateway lets you define routing rules, rate limits, and budgets once, then enforces them automatically for all requests.

You can route requests to different models based on user attributes. For example, to direct premium users to a more capable model while routing free tier users to a cost-effective option, use a CEL expression:

[source,cel]
----
// Route premium users to best model, free users to cost-effective model
request.headers["x-user-tier"] == "premium"
  ? "anthropic/claude-opus-4"
  : "anthropic/claude-sonnet-3.5"
----

You can also set different rate limits and spend limits per environment to prevent staging or development traffic from consuming production budgets.

For reliability, you can configure provider pools with automatic failover. If you configure OpenAI GPT-4 as your primary model and Anthropic Claude Opus as the fallback, the gateway automatically routes requests to the fallback when it detects rate limits or timeouts from the primary provider. This configuration can achieve 99.9% uptime even during provider outages.

=== 3. MCP aggregation and orchestration

AI Gateway aggregates multiple MCP (Model Context Protocol) servers and provides deferred tool loading, which dramatically reduces token costs for AI agents.

Without AI Gateway, agents typically load all available tools from multiple MCP servers at startup. This approach sends 50+ tool definitions with every request, creating high token costs (thousands of tokens per request), slow agent startup times, and no centralized governance over which tools agents can access.

With AI Gateway, you configure approved MCP servers once, and the gateway loads only search and orchestrator tools initially. Agents query for specific tools only when needed, which reduces token usage by 80-90% depending on your configuration. You also gain centralized approval and governance over which MCP servers your agents can access.

For complex workflows, AI Gateway provides a JavaScript-based orchestrator tool that reduces multi-step workflows from multiple round trips to a single call. For example, you can create a workflow that searches a vector database and, if the results are insufficient, falls back to web searchâ€”all in one orchestration step.

=== 4. Unified observability and cost tracking

AI Gateway provides a single dashboard that tracks all LLM traffic across providers, eliminating the need to switch between multiple provider dashboards.

The dashboard tracks request volume per gateway, model, and provider, along with token usage for both prompt and completion tokens. You can view estimated spend per model with cross-provider comparisons, latency metrics (p50, p95, p99), and errors broken down by type, provider, and model.

This unified view helps you answer critical questions such as which model is the most cost-effective for your use case, why a specific user request failed, how much your staging environment costs per week, and what the latency difference is between providers for your workload.

== Common gateway patterns

=== Team isolation

When multiple teams share infrastructure but need separate budgets and policies, create one gateway per team. For example, you might configure Team A's gateway with a $5K/month budget for both staging and production environments, while Team B's gateway has a $10K/month budget with different rate limits. Each team sees only their own traffic in the observability dashboards, providing clear cost attribution and isolation.

=== Environment separation

To prevent staging traffic from affecting production metrics, create separate gateways for each environment. Configure the staging gateway with lower rate limits, restricted model access, and aggressive cost controls to prevent runaway expenses. The production gateway can have higher rate limits, access to all models, and alerting configured to detect anomalies.

=== Primary and fallback for reliability

To ensure uptime during provider outages, configure provider pools with automatic failover. For example, you can set OpenAI as your primary provider (preferred for quality) and configure Anthropic as the fallback that activates when the gateway detects rate limits or timeouts from OpenAI. Monitor the fallback rate to detect primary provider issues early, before they impact your users.

=== A/B testing models

To compare model quality and cost without dual integration, route a percentage of traffic to different models. For example, you can send 80% of traffic to `claude-sonnet-3.5` and 20% to `claude-opus-4`, then compare quality metrics and costs in the observability dashboard before adjusting the split.

=== Customer-based routing

For SaaS products with tiered pricing (free, pro, enterprise), use CEL routing based on request headers to match users with appropriate models:

[source,cel]
----
request.headers["x-customer-tier"] == "enterprise" ? "anthropic/claude-opus-4" :
request.headers["x-customer-tier"] == "pro" ? "anthropic/claude-sonnet-3.5" :
"anthropic/claude-haiku"
----

== When to use AI Gateway

AI Gateway is ideal for organizations that:

* Use or plan to use multiple LLM providers
* Need centralized cost tracking and budgeting
* Want to experiment with different models without code changes
* Require high availability during provider outages
* Have multiple teams or customers using AI services
* Build AI agents that need MCP tool aggregation
* Need unified observability across all AI traffic

AI Gateway may not be necessary if:

* You only use a single provider with simple requirements
* You have minimal AI traffic (< 1000 requests/day)
* You don't need cost tracking or policy enforcement
* Your application doesn't require provider switching

== Next steps

Now that you understand what AI Gateway is and how it can benefit your organization:

*For Administrators:*

* xref:ai-gateway/admin/setup-guide.adoc[Setup Guide] - Enable providers, models, and create gateways
* xref:ai-gateway/gateway-architecture.adoc[Architecture Deep Dive] - Technical architecture details

*For Builders:*

* xref:ai-gateway/builders/discover-gateways.adoc[Discover Available Gateways] - Find which gateways you can access
* xref:ai-gateway/builders/connect-your-agent.adoc[Connect Your Agent] - Integrate your application
