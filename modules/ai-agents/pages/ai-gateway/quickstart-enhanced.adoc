= DRAFT Quickstart enhanced
:description: Get started with AI Gateway by routing your first request, viewing observability data, testing failover and CEL routing.
:page-personas: app_developer

Get your first request routed through Redpanda AI Gateway.

After completing this quickstart, you will be able to:

* Route your first LLM request through AI Gateway using the Cloud UI and verify it in the observability dashboard.
* Configure a provider and gateway with correct authentication and routing policies.
* Test failover behavior and CEL routing rules in a development environment.

== Prerequisites

Before starting, ensure you have:

* Redpanda Cloud account with BYOC 
* Admin access to configure providers and gateways
* API keys for at least one LLM provider (OpenAI, Anthropic, etc.)
* Python 3.8+ or Node.js 18+ (for examples)

== Step 1: Configure a provider

Providers must be configured before they can be used in gateways.

// PLACEHOLDER: Add UI navigation path, e.g., "Console → AI Gateway → Providers → Add Provider"

1. Navigate to *Providers*:
   * Open Redpanda Cloud Console
   * Go to // PLACEHOLDER: exact menu path

2. Add provider:
+
----
Provider: OpenAI
API Key: sk-...
Enabled Models: gpt-4o, gpt-4o-mini
----

   // PLACEHOLDER: Add screenshot of provider configuration form

3. Verify:

   * Provider status shows "Active"
   * Models appear in model catalog

Alternative: CLI (if available)

[source,bash]
----
# PLACEHOLDER: CLI command for adding provider
rpk cloud ai-gateway provider create \
  --provider openai \
  --api-key sk-... \
  --models gpt-4o,gpt-4o-mini
----


AI Gateway supports the following LLM providers:

* OpenAI
* Anthropic
// PLACEHOLDER: Add other supported providers

== Step 2: Create a gateway

Gateways define routing policies, rate limits, and observability scope.

// PLACEHOLDER: Add UI navigation path

1. Navigate to *Gateways*:
   * Go to // PLACEHOLDER: exact menu path

2. Create gateway:
+
----
Name: my-first-gateway
Workspace: default
Description: Quickstart gateway for testing
----

   // PLACEHOLDER: Add screenshot of gateway creation form

3. Save gateway ID:
+
After creation, copy your gateway ID (required for requests):
+
----
Gateway ID: gw_abc123...
Gateway Endpoint: https://{CLUSTER_ID}.cloud.redpanda.com/ai-gateway/v1
----

   // PLACEHOLDER: Confirm exact endpoint format

When planning your gateway structure, consider these common patterns:

* One gateway per environment (staging, production)
* One gateway per team (for budget isolation)
* One gateway per customer (for multi-tenant SaaS)

== Step 3: Send your first request

Route a request through your gateway.

[tabs]
====
Python::
+
--
[source,python]
----
from openai import OpenAI
import os

# Configure client to use AI Gateway
client = OpenAI(
    base_url="https://{CLUSTER_ID}.cloud.redpanda.com/ai-gateway/v1",  # Gateway endpoint
    api_key=os.getenv("REDPANDA_CLOUD_TOKEN"),  # Your Redpanda Cloud token
    default_headers={
        "rp-aigw-id": "gw_abc123..."  # Your gateway ID from Step 2
    }
)

# Make a request (note the vendor/model_id format)
response = client.chat.completions.create(
    model="openai/gpt-4o-mini",  # Format: {provider}/{model}
    messages=[
        {"role": "user", "content": "Say 'Hello from AI Gateway!'"}
    ],
    max_tokens=20
)

print(response.choices[0].message.content)
# Output: Hello from AI Gateway!
----
--

TypeScript/JavaScript::
+
--
[source,typescript]
----
import OpenAI from 'openai';

const client = new OpenAI({
  baseURL: 'https://{CLUSTER_ID}.cloud.redpanda.com/ai-gateway/v1',
  apiKey: process.env.REDPANDA_CLOUD_TOKEN,
  defaultHeaders: {
    'rp-aigw-id': 'gw_abc123...'
  }
});

const response = await client.chat.completions.create({
  model: 'openai/gpt-4o-mini',
  messages: [
    { role: 'user', content: 'Say "Hello from AI Gateway!"' }
  ],
  max_tokens: 20
});

console.log(response.choices[0].message.content);
// Output: Hello from AI Gateway!
----
--

cURL::
+
--
[source,bash]
----
curl https://{CLUSTER_ID}.cloud.redpanda.com/ai-gateway/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer ${REDPANDA_CLOUD_TOKEN}" \
  -H "rp-aigw-id: gw_abc123..." \
  -d '{
    "model": "openai/gpt-4o-mini",
    "messages": [
      {"role": "user", "content": "Say \"Hello from AI Gateway!\""}
    ],
    "max_tokens": 20
  }'
----

Expected response:

[source,json]
----
{
  "id": "chatcmpl-...",
  "object": "chat.completion",
  "created": 1704844800,
  "model": "openai/gpt-4o-mini",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello from AI Gateway!"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 8,
    "completion_tokens": 5,
    "total_tokens": 13
  }
}
----
--
====


If your request fails, check these common issues:

* `401 Unauthorized` - Verify your `REDPANDA_CLOUD_TOKEN` is valid
* `404 Not Found` - Confirm the `base_url` matches your gateway endpoint
* `Model not found` - Ensure the model is enabled in Step 1
* `Missing rp-aigw-id` - Add the gateway ID header to your request

== Step 4: Verify in observability dashboard

Confirm your request appears in the AI Gateway dashboard.

// PLACEHOLDER: Add UI navigation path and screenshots

1. *Navigate to Logs*:
   * Go to // PLACEHOLDER: Console → AI Gateway → {Gateway Name} → Logs

2. *Find your request*:
   * Filter by Gateway: `my-first-gateway`
   * Filter by Model: `openai/gpt-4o-mini`
   * Time range: Last 5 minutes

3. *Verify fields*:
   * Model: `openai/gpt-4o-mini`
   * Provider: OpenAI
   * Status: 200
   * Prompt tokens: ~8
   * Completion tokens: ~5
   * Estimated cost: // PLACEHOLDER: $X.XXXX
   * Latency: // PLACEHOLDER: ~XXXms

4. *Click Request to Expand*:
   * View full prompt and response
   * See request headers
   * Check routing decision (which provider pool was used)

If your request doesn't appear in the logs, try these steps:

* Wait a few seconds for logs to populate (there may be a brief delay)
* Verify the gateway ID in your request matches the gateway you're viewing
* Check that your client received a successful response (no errors)

== Next steps: Add failover (optional)

Add automatic failover to a backup provider for reliability.

=== Step 5: Add second provider

Add Anthropic as a fallback option:

// PLACEHOLDER: Add UI path

1. *Navigate to Providers* → *Add Provider*:
+
----
Provider: Anthropic
API Key: sk-ant-...
Enabled Models: claude-sonnet-3.5
----

2. *Verify*:

   * Anthropic provider status: Active
   * Models appear in catalog

=== Step 6: Configure provider pool with fallback

Update your gateway to use OpenAI as primary, Anthropic as fallback.

// PLACEHOLDER: Add UI path and configuration format

1. *Navigate to Gateway Settings*:

   * Go to // PLACEHOLDER: AI Gateway → {Gateway Name} → Routing

2. *Configure provider pool*:
+
[source,yaml]
----
# PLACEHOLDER: Confirm actual configuration format
routing:
  primary_pool:
    * provider: openai
      models: [gpt-4o, gpt-4o-mini]
  fallback_pool:
    * provider: anthropic
      models: [claude-sonnet-3.5]

fallback_triggers:
  * rate_limit_exceeded
  * timeout
  * 5xx_errors
----

   // PLACEHOLDER: Add screenshot of routing configuration

3. *Save configuration*

=== Step 7: Test failover

Simulate a provider failure to see fallback in action.

// PLACEHOLDER: Add method to test failback, or skip if not easily testable

*Option A: Disable primary provider temporarily*

1. Disable OpenAI provider in settings
2. Send request with `openai/gpt-4o` model
3. Gateway should automatically route to Anthropic fallback
4. Check logs to confirm fallback was used

*Option B: Trigger rate limit*

1. Send many requests rapidly to hit rate limit
2. Gateway should fallback to Anthropic
3. Check logs for "fallback_triggered" indicator

Verify fallback:

[source,python]
----
response = client.chat.completions.create(
    model="openai/gpt-4o",  # Request OpenAI model
    messages=[{"role": "user", "content": "Test fallback"}]
)

# Check which provider actually handled it
# PLACEHOLDER: How to verify this - response header? Log metadata?
----


Verify the fallback in the observability dashboard. The request log should display:

* Requested model: `openai/gpt-4o`
* Actual provider: Anthropic (fallback)
* Fallback reason: rate_limit, timeout, or error


== Next steps: Add routing rule (optional)

Use CEL expressions to route requests based on headers or content.

=== Step 8: Create CEL routing rule

Route premium users to better models automatically.

// PLACEHOLDER: Add UI path for CEL configuration

1. Navigate to *Gateway Settings*:

   * Go to // PLACEHOLDER: AI Gateway → {Gateway Name} → Routing Rules

2. Add CEL rule:
+
[source,cel]
----
# Route based on user tier header
request.headers["x-user-tier"] == "premium"
  ? "openai/gpt-4o"
  : "openai/gpt-4o-mini"
----

   // PLACEHOLDER: Add screenshot of CEL editor with syntax highlighting

3. Test rule (if UI supports testing):

   * Input test headers: `x-user-tier: premium`
   * Verify output: `openai/gpt-4o`
   * Input test headers: `x-user-tier: free`
   * Verify output: `openai/gpt-4o-mini`

4. Save rule

=== Step 9: Test routing rule

Send requests with different headers and verify routing.

*Premium user request*:

[source,python]
----
response = client.chat.completions.create(
    model="auto",  # PLACEHOLDER: or how to trigger CEL routing
    messages=[{"role": "user", "content": "Hello"}],
    extra_headers={"x-user-tier": "premium"}
)

# Should route to gpt-4o (premium model)
----


*Free user request*:

[source,python]
----
response = client.chat.completions.create(
    model="auto",
    messages=[{"role": "user", "content": "Hello"}],
    extra_headers={"x-user-tier": "free"}
)

# Should route to gpt-4o-mini (cost-effective model)
----


To verify the routing rule worked, check the observability dashboard:

* Open the request logs for your gateway
* Confirm the correct model was selected based on the header value
* Review the routing decision explanation to see which CEL rule matched

== What's next?

After completing this quickstart, consider these configurations to optimize your gateway:

. *Set rate limits* to protect against runaway costs and prevent abuse.
. *Add spend limits* to set monthly budgets per gateway and receive alerts before limits are reached.
. *Configure MCP aggregation* to give agents access to tools and reduce token costs with deferred loading.

// Explore advanced features
// A/B testing models
// Multi-tenancy patterns 
// Cost optimization 
// Performance tuning 

Connect your favorite AI development tools to AI Gateway:

* xref:ai-agents:ai-gateway/integrations/index.adoc[]: Overview of all integrations
* xref:ai-agents:ai-gateway/integrations/claude-code-user.adoc[]: Claude Code
* xref:ai-agents:ai-gateway/integrations/cline-user.adoc[]: Cline
* xref:ai-agents:ai-gateway/integrations/continue-user.adoc[]: Continue.dev
* xref:ai-agents:ai-gateway/integrations/cursor-user.adoc[]: Cursor IDE
* xref:ai-agents:ai-gateway/integrations/github-copilot-user.adoc[]: GitHub Copilot


== Related pages

* xref:ai-agents:ai-gateway/ai-gateway-overview.adoc[]
* xref:ai-agents:ai-gateway/cel-routing-cookbook.adoc[]
* xref:ai-agents:ai-gateway/observability-logs.adoc[]
* xref:ai-agents:ai-gateway/migration-guide.adoc[]
