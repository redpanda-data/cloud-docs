= AI Gateway Quickstart
:description: Get started with AI Gateway by configuring providers, creating your first gateway, and routing requests through unified LLM endpoints.
:page-topic-type: quickstart
:personas: app_developer, platform_admin
:learning-objective-1: Enable an LLM provider and create your first gateway
:learning-objective-2: Route your first request through AI Gateway and verify it works
:learning-objective-3: View request logs and token usage in the observability dashboard

include::ai-agents:partial$ai-gateway-byoc-note.adoc[]

Redpanda AI Gateway provides unified access to multiple large language model (LLM) providers and glossterm:MCP[,Model Context Protocol (MCP)] servers through a single endpoint. This quickstart walks you through configuring your first gateway and routing requests through it.

== Prerequisites

Before starting, ensure you have:

* Access to the AI Gateway UI (provided by your administrator)
* Admin permissions to configure providers and gateways
* API key for at least one LLM provider (OpenAI, Anthropic, or Google AI)
* Python 3.8+, Node.js 18+, or cURL (for testing)

== Configure a provider

Providers represent upstream LLM services (OpenAI, Anthropic, Google Gemini) and their associated credentials. Providers are disabled by default and must be enabled explicitly.

. In AI Gateways, navigate to *Providers*.
. Select a provider (for example, OpenAI, Anthropic, Google AI).
. On the Configuration tab, click *Add configuration* and enter your API Key.
. Verify the provider status shows "Active".

== Enable models

After enabling a provider, enable the specific models you want to make available through your gateways.

. Navigate to *Models*.
. Enable the models you want to use (for example, `gpt-5.2`, `gpt-5.2-mini`, `claude-sonnet-4.5`, `claude-opus-4.6`).
. Verify the models appear as "Enabled" in the catalog.

TIP: Different providers have different reliability and cost characteristics. When choosing models, consider your use case requirements for quality, speed, and cost.

=== Model naming convention

Requests through AI Gateway must use the `vendor/model_id` format. For example:

* OpenAI models: `openai/gpt-5.2`, `openai/gpt-5.2-mini`
* Anthropic models: `anthropic/claude-sonnet-4.5`, `anthropic/claude-opus-4.6`
* Google Gemini models: `google/gemini-2.0-flash`, `google/gemini-2.0-pro`

This format allows the gateway to route requests to the correct provider.

== Create a gateway

A gateway is a logical configuration boundary that defines routing policies, rate limits, spend limits, and observability scope. You can create separate gateways per team, environment (staging/production), or customer.

[IMPORTANT]
====
When creating a gateway, you choose between two modes:

* *AI Hub Mode*: Zero-configuration with pre-configured routing and backend pools. Just add provider credentials and start routing requests. Ideal for quick starts and standard use cases.
* *Custom Mode*: Full control over all routing rules, backend pools, and policies. Requires manual configuration. Ideal for custom routing logic and specialized requirements.

See xref:ai-gateway/gateway-modes.adoc[] to understand which mode fits your needs. This quickstart focuses on Custom mode configuration.
====

. Navigate to *Gateways*.
. Click *Create Gateway*.
. Select the gateway mode:
+
* *AI Hub*: Choose this for pre-configured intelligent routing (see xref:ai-gateway/admin/configure-ai-hub.adoc[] for setup)
* *Custom*: Choose this for full configuration control (continue with steps below)

. Configure the gateway (for Custom mode):
+
* Name: Choose a descriptive name (for example, `my-first-gateway`)
* Workspace: Select a workspace (conceptually similar to a resource group)
* Description: Optional metadata for documentation

. After creation, copy the *Gateway Endpoint* and *Gateway ID* from the gateway detail page. You'll need these for sending requests.

Your gateway endpoint format:
----
Gateway Endpoint: https://gw.ai.panda.com
Gateway ID: gw_abc123...
----

Common gateway patterns include the following:

* Environment separation: Create separate gateways for staging and production
* Team isolation: One gateway per team for budget tracking
* Customer multi-tenancy: One gateway per customer for isolated policies

== Send your first request

Now that you've configured a provider and created a gateway, send a test request to verify everything works.

[tabs]
====
Python::
+
--
[source,python]
----
from openai import OpenAI

# Configure client to use AI Gateway
client = OpenAI(
    base_url="https://gw.ai.panda.com",  # Your gateway endpoint
    api_key="<your-redpanda-api-key>",   # Your Redpanda API key
    default_headers={
        "rp-aigw-id": "gw_abc123..."     # Your gateway ID
    }
)

# Send a request (note the vendor/model_id format)
response = client.chat.completions.create(
    model="openai/gpt-5.2-mini",  # Format: {provider}/{model}
    messages=[
        {"role": "user", "content": "Say 'Hello from AI Gateway!'"}
    ],
    max_tokens=20
)

print(response.choices[0].message.content)
# Expected output: Hello from AI Gateway!
----
--

TypeScript/JavaScript::
+
--
[source,typescript]
----
import OpenAI from 'openai';

const client = new OpenAI({
  baseURL: 'https://gw.ai.panda.com',
  apiKey: process.env.REDPANDA_API_KEY,
  defaultHeaders: {
    'rp-aigw-id': 'gw_abc123...'
  }
});

const response = await client.chat.completions.create({
  model: 'openai/gpt-5.2-mini',
  messages: [
    { role: 'user', content: 'Say "Hello from AI Gateway!"' }
  ],
  max_tokens: 20
});

console.log(response.choices[0].message.content);
// Expected output: Hello from AI Gateway!
----
--

cURL::
+
--
[source,bash]
----
curl https://gw.ai.panda.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer ${REDPANDA_API_KEY}" \
  -H "rp-aigw-id: gw_abc123..." \
  -d '{
    "model": "openai/gpt-5.2-mini",
    "messages": [
      {"role": "user", "content": "Say \"Hello from AI Gateway!\""}
    ],
    "max_tokens": 20
  }'
----

Expected response:

[source,json]
----
{
  "id": "chatcmpl-...",
  "object": "chat.completion",
  "created": 1704844800,
  "model": "openai/gpt-5.2-mini",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello from AI Gateway!"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 8,
    "completion_tokens": 5,
    "total_tokens": 13
  }
}
----
--
====

=== Troubleshooting

If your request fails, check these common issues:

* 401 Unauthorized: Verify your API key is valid
* 404 Not Found: Confirm the base URL matches your gateway endpoint
* Model not found: Ensure the model is enabled in Step 2
* Missing rp-aigw-id: Add the gateway ID header to your request

== Verify in observability dashboard

Confirm your request appears in the AI Gateway observability dashboard.

. In the sidebar, navigate to *Agentic AI > Gateways*, then select your gateway.
. Filter by:
+
* *Gateway*: `my-first-gateway`
* *Model*: `openai/gpt-5.2-mini`
* *Time range*: Last 5 minutes

. Verify the request log shows:
+
* *Model*: `openai/gpt-5.2-mini`
* *Provider*: OpenAI
* *Status*: 200 (success)
* *Prompt tokens*: ~8
* *Completion tokens*: ~5
* *Estimated cost*: Based on provider pricing
* *Latency*: Response time in milliseconds

. Click the request to expand and view:
+
* Full prompt and response content
* Request headers
* Routing decision details

If your request doesn't appear:

* Wait a few seconds for logs to populate (there may be a brief delay)
* Verify the gateway ID in your request matches the gateway you're viewing
* Check that your client received a successful response

== Configure LLM routing (optional)

Configure rate limits, spend limits, and provider pools with failover.

On the Gateways page, select the *LLM* tab to configure routing policies. The LLM routing pipeline represents the request lifecycle:

. *Rate Limit*: Control request throughput (for example, 100 requests/second)
. *Spend Limit*: Set monthly budget caps (for example, $15K/month with blocking enforcement)
. *Provider Pools*: Define primary and fallback providers

=== Configure provider pool with fallback

For high availability, configure a fallback provider that activates when the primary fails:

. Add a second provider (for example, Anthropic).
. In your gateway's *LLM* routing configuration:
+
* *Primary pool*: OpenAI (preferred for quality)
* *Fallback pool*: Anthropic (activates on rate limits, timeouts, or errors)

. Save the configuration.

The gateway automatically routes to the fallback when it detects:

* Rate limit exceeded
* Request timeout
* 5xx server errors from primary provider

Monitor the fallback rate in observability to detect primary provider issues early.

== Configure MCP tools (optional)

If you're using glossterm:AI agent[,AI agents], configure glossterm:MCP[,Model Context Protocol (MCP)] tool aggregation.

On the Gateways page, select the *MCP* tab to configure tool discovery and execution. The MCP proxy aggregates multiple glossterm:MCP server[,MCP servers] behind a single endpoint, allowing agents to discover and call glossterm:MCP tool[,tools] through the gateway.

Configure the MCP settings:

* *Display name*: Descriptive name for the provider pool
* *Model*: Choose which model handles tool execution
* *Load balancing*: If multiple providers are available, select a strategy (for example, round robin)

=== Available MCP tools

The gateway provides these built-in MCP tools:

* *Data catalog API*: Query your data catalog
* *Memory store*: Persistent storage for agent state
* *Vector search*: Semantic search over embeddings
* *MCP orchestrator*: Built-in tool for programmatic multi-tool workflows

The *MCP orchestrator* enables agents to generate JavaScript code that calls multiple tools in a single orchestrated step, reducing round trips. For example, a workflow requiring 47 file reads can be reduced from 49 round trips to just 1.

To add external tools (for example, Slack, GitHub), add their MCP server endpoints to your gateway configuration.

=== Deferred tool loading

When many tools are aggregated, listing all tools upfront can consume significant tokens. With deferred tool loading, the MCP gateway initially returns only:

* A tool search capability
* The MCP orchestrator

Agents then search for specific tools they need, retrieving only that subset. This can reduce token usage by 80-90% when you have many tools configured.

// REVIEWERS: When/how exactly do you use the orchestrator? Also what happens after they create a gateway? Please provide an example of how to validate end-to-end routing against the gateway endpoint!

// REVIEWERS: How do users connect to the ADP catalog + MCP servers exposed through RPCN?

== Configure CEL routing rule (optional)

Use CEL (Common Expression Language) expressions to route requests dynamically based on headers, content, or other request properties.

The AI Gateway uses CEL for flexible routing without code changes. Use CEL to:

* Route premium users to better models
* Apply different rate limits based on user tiers
* Enforce policies based on request content

=== Add a routing rule

In your gateway's routing configuration:

. Add a CEL expression to route based on user tier:
+
[source,cel]
----
# Route based on user tier header
request.headers["x-user-tier"] == "premium"
  ? "openai/gpt-5.2"
  : "openai/gpt-5.2-mini"
----

. Save the rule.

The gateway editor helps you discover available request fields (headers, path, body, and so on).

=== Test the routing rule

Send requests with different headers to verify routing:

*Premium user request*:

[source,python]
----
response = client.chat.completions.create(
    model="openai/gpt-5.2",  # Will be routed based on CEL rule
    messages=[{"role": "user", "content": "Hello"}],
    extra_headers={"x-user-tier": "premium"}
)
# Should route to gpt-5.2 (premium model)
----

*Free user request*:

[source,python]
----
response = client.chat.completions.create(
    model="openai/gpt-5.2-mini",
    messages=[{"role": "user", "content": "Hello"}],
    extra_headers={"x-user-tier": "free"}
)
# Should route to gpt-5.2-mini (cost-effective model)
----

Check the observability dashboard to verify:

* The correct model was selected based on the header value
* The routing decision explanation shows which CEL rule matched

=== Common CEL patterns

Route based on model family:

[source,cel]
----
request.body.model.startsWith("anthropic/")
----

Apply a rule to all requests:

[source,cel]
----
true
----

Guard for field existence:

[source,cel]
----
has(request.body.max_tokens) && request.body.max_tokens > 1000
----

For more CEL examples, see xref:ai-agents:ai-gateway/cel-routing-cookbook.adoc[].

== Connect AI tools to your gateway

The AI Gateway provides standardized endpoints that work with various AI development tools. This section shows how to configure popular tools.

=== MCP endpoint

If you've configured MCP tools in your gateway, AI agents can connect to the aggregated MCP endpoint:

* *MCP endpoint URL*: `https://gw.ai.panda.com/mcp`
* *Required headers*:
** `Authorization: Bearer <your-api-key>`
** `rp-aigw-id: <your-gateway-id>`

This endpoint aggregates all MCP servers configured in your gateway.

=== Environment variables

For consistent configuration, set these environment variables:

[source,bash]
----
export REDPANDA_GATEWAY_URL="https://gw.ai.panda.com"
export REDPANDA_GATEWAY_ID="<your-gateway-id>"
export REDPANDA_API_KEY="<your-api-key>"
----

=== Claude Code

Configure Claude Code using HTTP transport for the MCP connection:

[source,bash]
----
claude mcp add --transport http redpanda-aigateway https://gw.ai.panda.com/mcp \
  --header "Authorization: Bearer <your-api-key>" \
  --header "rp-aigw-id: <your-gateway-id>"
----

Alternatively, edit `~/.claude/config.json`:

[source,json]
----
{
  "mcpServers": {
    "redpanda-ai-gateway": {
      "transport": "http",
      "url": "https://gw.ai.panda.com/mcp",
      "headers": {
        "Authorization": "Bearer <your-api-key>",
        "rp-aigw-id": "<your-gateway-id>"
      }
    }
  },
  "apiProviders": {
    "redpanda": {
      "baseURL": "https://gw.ai.panda.com",
      "headers": {
        "rp-aigw-id": "<your-gateway-id>"
      }
    }
  }
}
----

ifdef::integrations-available[]
For detailed Claude Code setup, see xref:ai-agents:ai-gateway/integrations/claude-code-user.adoc[].
endif::[]

=== Continue.dev

Edit your Continue config file (`~/.continue/config.json`):

[source,json]
----
{
  "models": [
    {
      "title": "Redpanda AI Gateway - GPT-5.2",
      "provider": "openai",
      "model": "openai/gpt-5.2",
      "apiBase": "https://gw.ai.panda.com",
      "apiKey": "<your-api-key>",
      "requestOptions": {
        "headers": {
          "rp-aigw-id": "<your-gateway-id>"
        }
      }
    },
    {
      "title": "Redpanda AI Gateway - Claude",
      "provider": "anthropic",
      "model": "anthropic/claude-sonnet-4.5",
      "apiBase": "https://gw.ai.panda.com",
      "apiKey": "<your-api-key>",
      "requestOptions": {
        "headers": {
          "rp-aigw-id": "<your-gateway-id>"
        }
      }
    },
    {
      "title": "Redpanda AI Gateway - Gemini",
      "provider": "google",
      "model": "google/gemini-2.0-flash",
      "apiBase": "https://gw.ai.panda.com",
      "apiKey": "<your-api-key>",
      "requestOptions": {
        "headers": {
          "rp-aigw-id": "<your-gateway-id>"
        }
      }
    }
  ]
}
----

ifdef::integrations-available[]
For detailed Continue setup, see xref:ai-agents:ai-gateway/integrations/continue-user.adoc[].
endif::[]

=== Cursor IDE

Configure Cursor in Settings (*Cursor* â†’ *Settings* or `Cmd+,`):

[source,json]
----
{
  "cursor.ai.providers.openai.apiBase": "https://gw.ai.panda.com",
  "cursor.ai.providers.openai.defaultHeaders": {
    "rp-aigw-id": "<your-gateway-id>"
  }
}
----

ifdef::integrations-available[]
For detailed Cursor setup, see xref:ai-agents:ai-gateway/integrations/cursor-user.adoc[].
endif::[]

=== Custom applications

For custom applications using OpenAI, Anthropic, or Google Gemini SDKs:

*Python with OpenAI SDK*:

[source,python]
----
from openai import OpenAI

client = OpenAI(
    base_url="https://gw.ai.panda.com",
    api_key="<your-api-key>",
    default_headers={
        "rp-aigw-id": "<your-gateway-id>"
    }
)
----

*Python with Anthropic SDK*:

[source,python]
----
from anthropic import Anthropic

client = Anthropic(
    base_url="https://gw.ai.panda.com",
    api_key="<your-api-key>",
    default_headers={
        "rp-aigw-id": "<your-gateway-id>"
    }
)
----

*Node.js with OpenAI SDK*:

[source,javascript]
----
import OpenAI from 'openai';

const openai = new OpenAI({
  baseURL: 'https://gw.ai.panda.com',
  apiKey: process.env.REDPANDA_API_KEY,
  defaultHeaders: {
    'rp-aigw-id': '<your-gateway-id>'
  }
});
----

== Next steps

Explore advanced AI Gateway features:

* xref:ai-agents:ai-gateway/cel-routing-cookbook.adoc[]: Advanced CEL routing patterns for traffic distribution and cost optimization
* xref:ai-agents:ai-gateway/mcp-aggregation-guide.adoc[]: Configure MCP server aggregation and deferred tool loading
ifdef::integrations-available[]
* xref:ai-agents:ai-gateway/integrations/index.adoc[]: Connect more AI development tools
endif::[]

Learn about the architecture:

* xref:ai-agents:ai-gateway/gateway-architecture.adoc[]: Technical architecture, request lifecycle, and deployment models
* xref:ai-agents:ai-gateway/what-is-ai-gateway.adoc[]: Problems AI Gateway solves and common use cases
