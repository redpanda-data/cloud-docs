= AI Gateway Overview
:description: Overview of Redpanda AI Gateway, its features, benefits, architecture, supported providers, deployment models, and common usage patterns.
:page-personas: app_developer, platform_admin

NOTE: AI Gateway is supported on BYOC clusters running Redpanda version 25.3 and later.

Redpanda AI Gateway is a unified access layer for LLM providers and AI tools that sits between your applications and the AI services they use. It provides centralized routing, policy enforcement, cost management, and observability for all your AI traffic.

After reading this page, you will be able to:

* Explain how AI Gateway centralizes LLM provider management and reduces operational complexity.
* Identify key features (routing, observability, cost controls) that address common LLM integration challenges.
* Determine whether AI Gateway fits your use case based on traffic volume and provider diversity.

== The problem

Modern AI applications face four critical challenges that increase costs, reduce reliability, and slow down development.

First, applications typically hardcode provider-specific SDKs. An application using OpenAI's SDK cannot easily switch to Anthropic or Google without code changes and redeployment. This tight coupling makes testing across providers time-consuming and error-prone, and means provider outages directly impact your application availability.

Second, costs can spiral without visibility into usage patterns. Without a centralized view of token consumption across teams and applications, it's difficult to attribute costs to specific customers, features, or environments. Testing and debugging can generate unexpected bills, and there's no way to enforce budgets or rate limits per team or customer.

Third, AI agents that use MCP (Model Context Protocol) servers face tool coordination challenges. Managing tool discovery and execution is repetitive across projects, and agents typically load all available tools upfront, which creates high token costs. There's also no centralized governance over which tools agents can access.

Finally, observability is fragmented across provider dashboards. You cannot reconstruct user sessions that span multiple models, compare latency and costs across providers in a unified view, or efficiently debug issues. Troubleshooting "the AI gave the wrong answer" requires manual log diving across different systems.

== What AI Gateway solves

Redpanda AI Gateway addresses these challenges through four core capabilities:

=== 1. Unified LLM access (single endpoint for all providers)

AI Gateway provides a single OpenAI-compatible endpoint that routes requests to multiple LLM providers. Instead of integrating with each provider's SDK separately, you configure your application once and switch providers by changing only the model parameter.

// PLACEHOLDER: Add architecture diagram showing:
// - Application → AI Gateway → Multiple LLM Providers (OpenAI, Anthropic, etc.)
// - Single baseURL configuration
// - Model routing via vendor/model_id format

Without AI Gateway, you need different SDKs and patterns for each provider:

[source,python]
----
# OpenAI
from openai import OpenAI
client = OpenAI(api_key="sk-...")
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Hello"}]
)

# Anthropic (different SDK, different patterns)
from anthropic import Anthropic
client = Anthropic(api_key="sk-ant-...")
response = client.messages.create(
    model="claude-sonnet-3.5",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Hello"}]
)
----

With AI Gateway, you use the OpenAI SDK for all providers:

[source,python]
----
from openai import OpenAI

# Single configuration, multiple providers
client = OpenAI(
    base_url="https://{GATEWAY_ENDPOINT}",
    api_key="your-redpanda-token",
    default_headers={"rp-aigw-id": "{GATEWAY_ID}"}
)

# Route to OpenAI
response = client.chat.completions.create(
    model="openai/gpt-4o",
    messages=[{"role": "user", "content": "Hello"}]
)

# Route to Anthropic (same code, different model string)
response = client.chat.completions.create(
    model="anthropic/claude-sonnet-3.5",
    messages=[{"role": "user", "content": "Hello"}]
)
----

To switch providers, you change only the `model` parameter from `openai/gpt-4o` to `anthropic/claude-sonnet-3.5`. No code changes or redeployment needed.

=== 2. Policy-based routing and cost control

AI Gateway lets you define routing rules, rate limits, and budgets once, then enforces them automatically for all requests.

You can route requests to different models based on user attributes. For example, to direct premium users to a more capable model while routing free tier users to a cost-effective option, use a CEL expression:

[source,cel]
----
// Route premium users to best model, free users to cost-effective model
request.headers["x-user-tier"] == "premium"
  ? "anthropic/claude-opus-4"
  : "anthropic/claude-sonnet-3.5"
----

You can also set different rate limits and spend limits per environment to prevent staging or development traffic from consuming production budgets:

// PLACEHOLDER: Confirm exact policy configuration format

[source,yaml]
----
rate_limits:
  staging: 100 requests/minute
  production: 10000 requests/minute

spend_limits:
  staging: $500/month
  production: $50000/month
----

For reliability, you can configure provider pools with automatic failover. If you configure OpenAI GPT-4 as your primary model and Anthropic Claude Opus as the fallback, the gateway automatically routes requests to the fallback when it detects rate limits or timeouts from the primary provider. This configuration can achieve 99.9% uptime even during provider outages.

// PLACEHOLDER: Add details on pool configuration and failback behavior

=== 3. MCP aggregation and orchestration

AI Gateway aggregates multiple MCP (Model Context Protocol) servers and provides deferred tool loading, which dramatically reduces token costs for AI agents.

Without AI Gateway, agents typically load all available tools from multiple MCP servers at startup. This approach sends 50+ tool definitions with every request, creating high token costs (thousands of tokens per request), slow agent startup times, and no centralized governance over which tools agents can access.

With AI Gateway, you configure approved MCP servers once, and the gateway loads only search and orchestrator tools initially. Agents query for specific tools only when needed, which reduces token usage by 80-90% depending on your configuration. You also gain centralized approval and governance over which MCP servers your agents can access.

For complex workflows, AI Gateway provides a JavaScript-based orchestrator tool that reduces multi-step workflows from multiple round trips to a single call. For example, you can create a workflow that searches a vector database and, if the results are insufficient, falls back to web search—all in one orchestration step.

=== 4. Unified observability and cost tracking

AI Gateway provides a single dashboard that tracks all LLM traffic across providers, eliminating the need to switch between multiple provider dashboards.

// PLACEHOLDER: Add screenshots of:
// - Request logs view
// - Cost breakdown by model/provider
// - Latency histogram
// - Error rate tracking

The dashboard tracks request volume per gateway, model, and provider, along with token usage for both prompt and completion tokens. You can view estimated spend per model with cross-provider comparisons, latency metrics (p50, p95, p99), and errors broken down by type, provider, and model.

This unified view helps you answer critical questions such as which model is the most cost-effective for your use case, why a specific user request failed, how much your staging environment costs per week, and what the latency difference is between providers for your workload.

== Common gateway patterns

=== Team isolation

When multiple teams share infrastructure but need separate budgets and policies, create one gateway per team. For example, you might configure Team A's gateway with a $5K/month budget for both staging and production environments, while Team B's gateway has a $10K/month budget with different rate limits. Each team sees only their own traffic in the observability dashboards, providing clear cost attribution and isolation.

=== Environment separation

To prevent staging traffic from affecting production metrics, create separate gateways for each environment. Configure the staging gateway with lower rate limits, restricted model access, and aggressive cost controls to prevent runaway expenses. The production gateway can have higher rate limits, access to all models, and alerting configured to detect anomalies.

=== Primary and fallback for reliability

To ensure uptime during provider outages, configure provider pools with automatic failover. For example, you can set OpenAI as your primary provider (preferred for quality) and configure Anthropic as the fallback that activates when the gateway detects rate limits or timeouts from OpenAI. Monitor the fallback rate to detect primary provider issues early, before they impact your users.

=== A/B testing models
To compare model quality and cost without dual integration, route a percentage of traffic to different models. For example, you can send 80% of traffic to `claude-sonnet-3.5` and 20% to `claude-opus-4`, then compare quality metrics and costs in the observability dashboard before adjusting the split.

// PLACEHOLDER: Confirm if percentage-based routing is supported, or if it's header-based only

=== Customer-based routing

For SaaS products with tiered pricing (free, pro, enterprise), use CEL routing based on request headers to match users with appropriate models:

[source,cel]
----
request.headers["x-customer-tier"] == "enterprise" ? "anthropic/claude-opus-4" :
request.headers["x-customer-tier"] == "pro" ? "anthropic/claude-sonnet-3.5" :
"anthropic/claude-haiku"
----

== What's supported today

LLM providers

* OpenAI
* Anthropic
* // PLACEHOLDER: Google, AWS Bedrock, Azure OpenAI, others?

API compatibility

* OpenAI-compatible `/v1/chat/completions` endpoint
* // PLACEHOLDER: Streaming support?
* // PLACEHOLDER: Embeddings support?
* // PLACEHOLDER: Other endpoints?

Policy features

* CEL-based routing expressions
* Rate limiting (// PLACEHOLDER: per-gateway, per-header, per-tenant?)
* Monthly spend limits (// PLACEHOLDER: per-gateway, per-workspace?)
* Provider pools with automatic failover
* // PLACEHOLDER: Caching support?

MCP support

* MCP server aggregation
* Deferred tool loading (80-90% token reduction)
* JavaScript orchestrator for multi-step workflows
* // PLACEHOLDER: Tool execution sandboxing?

Observability

* Request logs with full prompt/response history
* Token usage tracking
* Estimated cost per request
* Latency metrics
* // PLACEHOLDER: Metrics export? OpenTelemetry support?

// What's not supported yet
// PLACEHOLDER: List current limitations, for example:
// - Custom model deployments (Azure OpenAI BYOK, AWS Bedrock custom models)
// - Response caching
// - Prompt templates/versioning
// - Guardrails (PII detection, content moderation)
// - Multi-region active-active deployment
// - Metrics export to external systems
// - Budget alerts/notifications

== Architecture

AI Gateway consists of three planes: a control plane for configuration and management, a data plane for request processing and routing, and an observability plane for monitoring and analytics.

// PLACEHOLDER: Add architecture diagram showing:
// 1. Control Plane:
//    - Workspace management
//    - Provider/model configuration
//    - Gateway creation and policy definition
//    - Admin console
//
// 2. Data Plane:
//    - Request ingestion
//    - Policy evaluation (rate limits → spend limits → routing → execution)
//    - Provider pool selection and failover
//    - MCP aggregation layer
//    - Response logging and metrics
//
// 3. Observability Plane:
//    - Request logs storage
//    - Metrics aggregation
//    - Dashboard UI

When a request flows through AI Gateway, it passes through several policy and routing stages before reaching the LLM provider. Understanding this lifecycle helps you configure policies effectively and troubleshoot issues:

. Application sends request to gateway endpoint with `rp-aigw-id` header
. Gateway authenticates request
. Rate limit policy evaluates (allow/deny)
. Spend limit policy evaluates (allow/deny)
. Routing policy evaluates (which model/provider to use)
. Provider pool selects backend (primary/fallback)
. Request forwarded to LLM provider
. Response returned to application
. Request logged with tokens, cost, latency, status

Each policy evaluation happens synchronously in the request path. If rate limits or spend limits reject the request, the gateway returns an error immediately without calling the LLM provider, which helps you control costs.

For MCP tool requests, the lifecycle differs slightly to support deferred tool loading:

. Application discovers tools via `/mcp` endpoint
. Gateway aggregates tools from approved MCP servers
. Application receives search + orchestrator tools (deferred loading)
. Application invokes specific tool
. Gateway routes to appropriate MCP server
. Tool execution result returned
. Request logged with execution time, status

The gateway only loads and exposes specific tools when requested, which dramatically reduces the token overhead compared to loading all tools upfront.

== Next steps

* xref:ai-agents:ai-gateway/ai-gateway.adoc[]: Route your first request through AI Gateway.
* xref:ai-agents:ai-gateway/mcp-aggregation-guide.adoc[]: Configure MCP server aggregation for AI agents.
* xref:ai-agents:ai-gateway/observability-logs.adoc[]: Monitor request logs, token usage, and costs.
