= Observability: Metrics and Analytics
:description: Guide to AI Gateway metrics and analytics, including where to find metrics, key metrics explained, dashboard views, filtering/grouping, alerting, exporting, common analysis tasks, retention, API access, best practices, and troubleshooting.
:page-topic-type: reference
:page-personas: platform_admin, app_developer
:learning-objective-1: Monitor aggregate metrics to track usage patterns and budget adherence
:learning-objective-2: Compare model and provider performance using latency and cost metrics
:learning-objective-3: Configure alerts for budget thresholds and performance degradation

include::ai-agents:partial$ai-gateway-byoc-note.adoc[]

AI Gateway provides aggregate metrics and analytics dashboards to help you understand usage patterns, costs, performance, and errors across all your LLM traffic.

== Before you begin

* You have an active AI Gateway with at least one request processed.
* You have access to the Redpanda Cloud Console.
* You have the appropriate permissions to view gateway metrics.

Use metrics for:

* Cost tracking and budget management
* Usage trends over time
* Performance monitoring (latency, error rates)
* Capacity planning
* Model/provider comparison

Use logs for: Debugging specific requests, viewing full prompts/responses. See xref:ai-agents:ai-gateway/observability-logs.adoc[].

== Where to find metrics

// PLACEHOLDER: Add exact UI navigation path

1. Navigate to analytics dashboard:
   * Console → AI Gateway → // PLACEHOLDER: exact path
   * Or: Gateway detail page → Analytics tab

2. Select gateway (optional):
   * View all gateways (org-wide metrics)
   * Or filter to specific gateway

3. Set time range:
   * Default: Last 7 days
   * Options: Last 24 hours, 7 days, 30 days, 90 days, Custom
   * // PLACEHOLDER: screenshot of time range picker

== Key metrics

=== Request volume

What it shows: Total number of requests over time

// PLACEHOLDER: Screenshot of request volume graph

Graph type: Time series line chart

Filters:

* By gateway
* By model
* By provider
* By status (success/error)

Use cases:

* Identify usage patterns (peak hours, days of week)
* Detect traffic spikes or drops
* Capacity planning

Example insights:

* "Traffic doubles every Monday morning at 9am" → Scale infrastructure
* "Staging gateway has more traffic than prod" → Investigate runaway testing

=== Token usage

What it shows: Prompt, completion, and total tokens consumed

// PLACEHOLDER: Screenshot of token usage graph

Graph type: Stacked area chart (prompt vs completion tokens)

Metrics:

* Total tokens
* Prompt tokens (input)
* Completion tokens (output)
* Tokens per request (average)

Breakdowns:

* By gateway
* By model
* By provider

Use cases:

* Understand cost drivers (prompt vs completion tokens)
* Identify verbose prompts or responses
* Optimize token usage

Example insights:

* "90% of tokens are completion tokens" → Responses are verbose, optimize max_tokens
* "Staging uses 10x more tokens than prod" → Investigate test suite

=== Estimated spend

What it shows: Calculated cost based on token usage and public pricing

// PLACEHOLDER: Screenshot of cost tracking dashboard

Graph type: Time series line chart with cost breakdown

Metrics:

* Total estimated spend
* Spend by model
* Spend by provider
* Spend by gateway
* Cost per 1K requests
* Cost per 1M tokens

Breakdowns:

* By gateway (for chargeback/showback)
* By model (for cost optimization)
* By provider (for negotiation leverage)
* By custom header (if configured, e.g., `x-customer-id`)

Use cases:

* Budget tracking ("Are we staying under $50K/month?")
* Cost attribution ("Which team spent the most?")
* Model comparison ("Is Claude cheaper than GPT-4 for our use case?")
* Forecasting ("At this rate, we'll spend $X next month")

Important notes:

* *Estimates based on public pricing* (may differ from your contract)
* *Not a substitute for provider invoices* (use for approximation only)
* Update frequency: // PLACEHOLDER: Real-time? Hourly? Daily?

Example insights:

* "Customer A accounts for 60% of spend" → Consider rate limits or tiered pricing
* "GPT-4o is 3x more expensive than Claude Sonnet for similar quality" → Optimize routing

=== Latency

What it shows: Request duration from gateway to provider and back

// PLACEHOLDER: Screenshot of latency histogram

Metrics:

* p50 (median) latency
* p95 latency
* p99 latency
* Min/max latency
* Average latency

Breakdowns:

* By gateway
* By model
* By provider
* By token range (longer responses = higher latency)

Use cases:

* Identify slow models or providers
* Set SLO targets (e.g., "p95 < 2 seconds")
* Detect performance regressions

Example insights:

* "GPT-4o p99 latency spiked to 10 seconds yesterday" → Investigate provider issue
* "Claude Sonnet is 30% faster than GPT-4o for same prompts" → Optimize for latency

Latency components (if available):

// PLACEHOLDER: Does gateway show latency breakdown?
* Gateway processing time
* Provider API time
* Network time

=== Error rate

What it shows: Percentage of failed requests over time

// PLACEHOLDER: Screenshot of error rate graph

Metrics:

* Total error rate (%)
* Errors by status code (400, 401, 429, 500, etc.)
* Errors by model
* Errors by provider

Graph type: Time series line chart with error percentage

Breakdowns:

* By error type:
  * Client errors (4xx)
  * Rate limits (429)
  * Server errors (5xx)
  * Provider errors
  * Gateway errors

Use cases:

* Detect provider outages
* Identify configuration issues (e.g., model not enabled)
* Monitor rate limit breaches

Example insights:

* "Error rate spiked to 15% at 2pm" → OpenAI outage, fallback to Anthropic worked
* "10% of requests fail with 'model not found'" → Model not enabled in gateway

=== Success rate

What it shows: Percentage of successful (200) requests over time

Metric: `Success Rate = (Successful Requests / Total Requests) × 100`

Target: Typically 99%+ for production workloads

Use cases:

* Monitor overall health
* Set up alerts (e.g., "Alert if success rate < 95%")

=== Fallback rate

What it shows: Percentage of requests that used fallback provider

// PLACEHOLDER: Screenshot of fallback rate graph

Metric: `Fallback Rate = (Fallback Requests / Total Requests) × 100`

Breakdowns:

* By fallback reason:
  * Rate limit exceeded
  * Timeout
  * 5xx error

Use cases:

* Monitor primary provider reliability
* Verify fallback is working
* Identify when to renegotiate rate limits

Example insights:

* "Fallback rate increased to 20% yesterday" → OpenAI hit rate limits, time to increase quota
* "Zero fallbacks in 30 days" → Fallback config may not be working, or primary provider is very reliable

== Dashboard views

=== Overview dashboard

Shows: High-level metrics across all gateways

// PLACEHOLDER: Screenshot of overview dashboard

Widgets:

* Total requests (last 24h, 7d, 30d)
* Total spend (last 24h, 7d, 30d)
* Success rate (current)
* Average latency (current)
* Top 5 models by request volume
* Top 5 gateways by spend

Use case: Executive view, health at a glance

=== Gateway dashboard

Shows: Metrics for a specific gateway

// PLACEHOLDER: Screenshot of gateway dashboard

Widgets:

* Request volume (time series)
* Token usage (time series)
* Estimated spend (time series)
* Latency percentiles (histogram)
* Error rate (time series)
* Model breakdown (pie chart)
* Provider breakdown (pie chart)

Use case: Team-specific monitoring, gateway optimization

=== Model comparison dashboard

Shows: Side-by-side comparison of models

// PLACEHOLDER: Screenshot of model comparison

Metrics per model:

* Request count
* Total tokens
* Estimated cost
* Cost per 1K requests
* Average latency
* Error rate

Use case: Evaluate whether to switch models (cost vs performance)

Example:

[cols="2,1,1,1,1"]
|===
| Model | Requests | Avg Latency | Cost per 1K | Error Rate

| openai/gpt-4o 
| 10,000 
| 1.2s 
| $5.00 
| 0.5%

| anthropic/claude-sonnet-3.5 
| 5,000 
| 0.9s 
| $3.50 
| 0.3%

| openai/gpt-4o-mini 
| 20,000 
| 0.7s 
| $0.50 
| 1.0%
|===

Insight: Claude Sonnet is 25% faster and 30% cheaper than GPT-4o with better reliability

=== Provider comparison dashboard

Shows: Side-by-side comparison of providers

Metrics per provider:

* Request count
* Total spend
* Average latency
* Error rate
* Fallback trigger rate

Use case: Evaluate provider reliability, negotiate contracts

=== Cost breakdown dashboard

Shows: Detailed cost analysis

// PLACEHOLDER: Screenshot of cost breakdown

Widgets:

* Spend by gateway (stacked bar chart)
* Spend by model (pie chart)
* Spend by provider (pie chart)
* Spend by custom dimension (if configured, e.g., customer ID)
* Spend trend (time series with forecast)
* Budget utilization (progress bar: $X / $Y monthly limit)

Use case: FinOps, budget management, chargeback/showback

== Filter and group

=== Filter by gateway

[source,text]
----
Filter: Gateway = "production-gateway"
----


Shows metrics for specific gateway only.

Use case: Isolate prod from staging metrics

=== Filter by model

[source,text]
----
Filter: Model = "openai/gpt-4o"
----


Shows metrics for specific model only.

Use case: Evaluate model performance in isolation

=== Filter by provider

[source,text]
----
Filter: Provider = "OpenAI"
----


Shows metrics for specific provider only.

Use case: Evaluate provider reliability

=== Filter by status

[source,text]
----
Filter: Status = "200"  // Only successful requests
Filter: Status >= "500"  // Only server errors
----


Use case: Focus on errors, or calculate success rate

=== Filter by custom dimension

// PLACEHOLDER: Confirm if custom dimensions are supported for filtering

[source,text]
----
Filter: request.headers["x-customer-id"] = "customer_abc"
----


Shows metrics for specific customer.

Use case: Customer-specific cost tracking for chargeback

=== Group by dimension

Common groupings:

* Group by Gateway
* Group by Model
* Group by Provider
* Group by Status
* Group by Hour/Day/Week/Month (time aggregation)

Example: "Show me spend grouped by model, for production gateway, over last 30 days"

== Alerting

// PLACEHOLDER: Confirm if alerting is supported

If alerting is supported:

=== Alert types

Budget alerts:

* Alert when spend exceeds X% of monthly budget
* Alert when spend grows Y% week-over-week

Performance alerts:

* Alert when error rate > X%
* Alert when p99 latency > Xms
* Alert when success rate < X%

Usage alerts:

* Alert when request volume drops (potential outage)
* Alert when fallback rate > X% (primary provider issue)

=== Alert channels

// PLACEHOLDER: Supported notification channels
* Email
* Slack
* PagerDuty
* Webhook
* // PLACEHOLDER: Others?

=== Example alert configuration

[source,yaml]
----
# PLACEHOLDER: Actual alert configuration format
alerts:
  - name: "High Error Rate"
    condition: error_rate > 5%
    duration: 5 minutes
    channels: [slack, email]

  - name: "Budget Threshold"
    condition: monthly_spend > 80% of budget
    channels: [email]

  - name: "Latency Spike"
    condition: p99_latency > 5000ms
    duration: 10 minutes
    channels: [pagerduty]
----

== Export metrics

// PLACEHOLDER: Confirm export capabilities

=== Export to CSV

1. Apply filters for desired metrics
2. Click "Export to CSV"
3. Download includes time series data

Use case: Import into spreadsheet for analysis, reporting

=== Export via API

// PLACEHOLDER: If API is available for metrics

[source,bash]
----
curl https://{CLUSTER_ID}.cloud.redpanda.com/api/ai-gateway/metrics \
  -H "Authorization: Bearer ${REDPANDA_CLOUD_TOKEN}" \
  -G \
  --data-urlencode "gateway_id=gw_abc123" \
  --data-urlencode "start_time=2025-01-01T00:00:00Z" \
  --data-urlencode "end_time=2025-01-31T23:59:59Z" \
  --data-urlencode "metric=requests,tokens,cost"
----


Response:

[source,json]
----
{
  "gateway_id": "gw_abc123",
  "start_time": "2025-01-01T00:00:00Z",
  "end_time": "2025-01-31T23:59:59Z",
  "metrics": {
    "requests": 1000000,
    "tokens": 500000000,
    "estimated_cost": 2500.00
  }
}
----


=== Integration with observability platforms

// PLACEHOLDER: OpenTelemetry support? Other integrations?

Supported integrations (if any):

* Prometheus: Metrics endpoint for scraping
* OpenTelemetry: Export metrics to OTel collector
* Datadog: Direct integration
* Grafana: Pre-built dashboards
* // PLACEHOLDER: Others?

== Common analysis tasks

=== "Are we staying within budget?"

1. View cost breakdown dashboard
2. Check budget utilization widget:
   * Current spend: $X
   * Monthly budget: $Y
   * Utilization: X%
   * Days remaining in month: Z
3. Forecast:
   * At current rate: $X × (30 / days_elapsed)
   * On track to exceed budget? Yes/No

Action:

* If approaching limit: Adjust rate limits, optimize models, pause non-prod usage
* If well under budget: Opportunity to test more expensive models

=== "Which team is using the most resources?"

1. Filter by gateway (assuming one gateway per team)
2. *Sort by Spend* (descending)
3. View table:

[cols="2,1,1,1,1"]
|===
| Gateway | Requests | Tokens | Spend | % of Total

| team-ml 
| 500K 
| 250M 
| $1,250 
| 50%

| team-product 
| 300K 
| 150M 
| $750 
| 30%

| team-eng 
| 200K 
| 100M 
| $500 
| 20%
|===

Action: Chargeback costs to teams, or investigate high-usage teams

=== "Is this model worth the extra cost?"

1. *Open Model Comparison Dashboard*
2. Select models to compare:
   * Expensive model: `openai/gpt-4o`
   * Cheap model: `openai/gpt-4o-mini`
3. Compare metrics:

[cols="2,1,1,2"]
|===
| Metric | GPT-4o | GPT-4o-mini | Difference

| Cost per 1K requests 
| $5.00 
| $0.50 
| *10x*

| Avg Latency 
| 1.2s 
| 0.7s 
| 58% *faster* (mini)

| Error Rate 
| 0.5% 
| 1.0% 
| 2x errors (mini)
|===

Decision: If mini's error rate is acceptable, save 10x on costs

=== "Why did costs spike yesterday?"

1. View cost trend graph
2. Identify spike (e.g., Jan 10th: $500 vs usual $100)
3. Drill down:
   * By gateway: Which gateway caused the spike?
   * By model: Did someone switch to expensive model?
   * By hour: What time did spike occur?
4. Cross-reference with logs:
   * Filter logs to spike timeframe
   * Check for unusual request patterns
   * Identify custom header (user ID, customer ID) if present

Common causes:

* Test suite running against prod gateway
* A/B test routing all traffic to expensive model
* User error (wrong model in config)
* Runaway loop in application code

=== "Is provider X more reliable than provider Y?"

1. Open provider comparison dashboard
2. Compare error rates:

[cols="2,1,1,2"]
|===
| Provider | Requests | Error Rate | Fallback Triggers

| OpenAI 
| 500K 
| 0.8% 
| 50 (rate limits)

| Anthropic 
| 300K 
| 0.3% 
| 5 (timeouts)
|===

Insight: Anthropic has 62% lower error rate

3. Compare latencies:

[cols="2,1,1"]
|===
| Provider | p50 Latency | p99 Latency

| OpenAI 
| 1.0s 
| 3.5s

| Anthropic 
| 0.8s 
| 2.5s
|===

Insight: Anthropic is 20% faster at p50, 28% faster at p99

Decision: Prioritize Anthropic in routing pools

== Metrics retention

// PLACEHOLDER: Confirm metrics retention policy

Retention period:

* *High-resolution* (1-minute granularity): // PLACEHOLDER: for example, 7 days
* *Medium-resolution* (1-hour granularity): // PLACEHOLDER: for example, 30 days
* *Low-resolution* (1-day granularity): // PLACEHOLDER: for example, 1 year

Note: Aggregate metrics retained longer than individual request logs

== API access to metrics

// PLACEHOLDER: Document metrics API if available

=== List available metrics

[source,bash]
----
curl https://{CLUSTER_ID}.cloud.redpanda.com/api/ai-gateway/metrics/list \
  -H "Authorization: Bearer ${REDPANDA_CLOUD_TOKEN}"
----


Response:

[source,json]
----
{
  "metrics": [
    "requests",
    "tokens.prompt",
    "tokens.completion",
    "tokens.total",
    "cost.estimated",
    "latency.p50",
    "latency.p95",
    "latency.p99",
    "errors.rate",
    "success.rate",
    "fallback.rate"
  ]
}
----


=== Query specific metric

[source,bash]
----
curl https://{CLUSTER_ID}.cloud.redpanda.com/api/ai-gateway/metrics/query \
  -H "Authorization: Bearer ${REDPANDA_CLOUD_TOKEN}" \
  -H "Content-Type: application/json" \
  -d '{
    "metric": "requests",
    "gateway_id": "gw_abc123",
    "start_time": "2025-01-01T00:00:00Z",
    "end_time": "2025-01-31T23:59:59Z",
    "granularity": "1d",
    "group_by": ["model"]
  }'
----


Response:

[source,json]
----
{
  "metric": "requests",
  "granularity": "1d",
  "data": [
    {
      "timestamp": "2025-01-01T00:00:00Z",
      "model": "openai/gpt-4o",
      "value": 10000
    },
    {
      "timestamp": "2025-01-01T00:00:00Z",
      "model": "anthropic/claude-sonnet-3.5",
      "value": 5000
    },
    ...
  ]
}
----


== Best practices

Set up budget alerts early

* Don't wait for surprise bills
* Alert at 50%, 80%, 90% of budget
* Include multiple stakeholders (eng, finance)

Create team dashboards

* One dashboard per team showing their gateway(s)
* Empowers teams to self-optimize
* Reduces central ops burden

Monitor fallback rate

* Low fallback rate (0-5%): Normal, failover working
* High fallback rate (>20%): Investigate primary provider issues
* Zero fallback rate: Verify fallback config is correct

Compare models regularly

* Run A/B tests with metrics
* Reassess as pricing and models change
* Don't assume expensive = better quality for your use case

Track trends, not point-in-time

* Day-to-day variance is normal
* Look for week-over-week and month-over-month trends
* Seasonal patterns (e.g., more usage on weekdays)

== Troubleshoot metrics issues

=== Issue: "Metrics don't match my provider invoice"

Possible causes:

1. Metrics are estimates based on public pricing
2. Your contract has custom pricing
3. Provider changed pricing mid-month

Solution:

* Use metrics for trends and optimization decisions
* Use provider invoices for actual billing
* // PLACEHOLDER: Can users configure custom pricing in gateway?

=== Issue: "Metrics are delayed or missing"

Possible causes:

1. Metrics aggregation has delay (// PLACEHOLDER: typical delay?)
2. Time range outside retention period
3. No requests in selected time range (empty data)

Solution:

1. Wait and refresh (// PLACEHOLDER: Xminutes typical delay)
2. Check retention policy
3. Verify requests were sent (check logs)

=== Issue: "Dashboard shows 'no data'"

Possible causes:

1. Filters too restrictive (no matching requests)
2. Gateway has no traffic yet
3. Permissions issue (can't access this gateway's metrics)

Solution:

1. Remove filters, widen time range
2. Send test request (see xref:ai-agents:ai-gateway/gateway-quickstart.adoc[])
3. Check permissions with admin

== Next steps

* xref:ai-agents:ai-gateway/observability-logs.adoc[]: View individual requests and debug issues.
