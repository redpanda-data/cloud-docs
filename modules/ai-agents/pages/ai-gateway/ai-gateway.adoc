= AI Gateway Quickstart
:description: Quickstart to configure the AI Gateway for unified access to multiple LLM providers and MCP servers through a single endpoint.


NOTE: AI Gateway is supported on BYOC clusters running Redpanda version 25.3 and later.

The Redpanda AI Gateway is a production-grade proxy that provides unified access to multiple Large Language Model (LLM) providers and Model Context Protocol (MCP) servers through a single endpoint. MCP servers expose tools that agents can discover and call. An AI Gateway maintains centralized control over routing, rate limiting, cost optimization, security, and observability.

After completing this quickstart, you will be able to:

* Route your first LLM request through AI Gateway using the Cloud Console and verify it in the observability dashboard.
* Configure a provider and gateway with correct authentication and routing policies.
* Test failover behavior and CEL routing rules in a development environment.

== Prerequisites

* Access to the AI Gateway UI (provided by your administrator)
* API key for at least one LLM provider: OpenAI or Anthropic
* Optional: MCP server endpoints if you plan to use tool aggregation

== Get started

Before you can create a gateway, an administrator must enable LLM providers and models.

=== Step 1: Enable a provider 

Providers represent upstream services (Anthropic, OpenAI) and associated credentials. Providers are disabled by default. An administrator must enable them explicitly by adding credentials.

. In AI Gateways, navigate to *Providers*.
. Select a provider (for example, Anthropic).
. On the *Configuration* tab for the provider, click *Add configuration* and enter your API Key.

=== Step 2: Enable models

The model catalog is the set of models made available through the gateway. Models are disabled by default. After enabling a provider, an administrator can enable its models.

The infrastructure that is serving the model is different based on the provider you select. For example, OpenAI has different reliability and availability metrics than Anthropic. When you consider all the metrics, you can design your gateway to use different providers for different use cases.

. Navigate to *Models*.
. Enable the models you want exposed through gateways.

==== Model naming convention

Model provider requests must use the `vendor/model_id` format in the model property of the request body, and include the `rp-aigw-id` header with the gateway ID the request is being sent to. The following example routes OpenAI API calls through Redpanda's AI Gateway for centralized control.

[source,python]
----
# Example: Using the OpenAI Python SDK with AI Gateway
from openai import OpenAI

client = OpenAI(
    base_url="https://gw.ai.panda.com", <1>
    api_key="your-api-key",
)

# Add header per request
response = client.chat.completions.create(
    model="openai/gpt-5", <2>
    messages=[{"role": "user", "content": "Hello!"}],
    extra_headers={
        "rp-aigw-id": "gateway-abc"  # Override for this request
    } <3>
)
----
<1> This redirects the OpenAI client to the AI Gateway endpoint.
<2> The `model` property uses the `vendor/model_id` format as required by the AI Gateway.
<3> Includes the `rp-aigw-id` header to specify which gateway configuration to use.

=== Step 3: Create a gateway 

A gateway is a logical configuration boundary (policies + routing + observability) on top of a single deployment. It's a "virtual gateway" that you can create per team, environment (staging/production), product, or customer.

. Navigate to *Gateways*.
. Click *Create Gateway*.
. Choose a name, workspace, and optional metadata. 
+
TIP: A _workspace_ is conceptually similar to a _resource group_ in Redpanda streaming. 

. After creation, copy the *Gateway Endpoint* from the gateway detail page.

=== Step 4: Configure LLM routing 

On the Gateways page, select the *LLM* tab to configure rate limits, spend limits, routing, and provider pools with fallback options. 

The LLM routing pipeline visually represents the request lifecycle:

. Rate Limit: For example, global rate limit of 100 requests/second.
. Spend Limit / Monthly Budget: For example, $15K/month with blocking enforcement, so it blocks requests after that budget is exceeded.
. Routing to a primary provider pool with optional fallback provider pools: For example, primary route to Anthropic backend pool, and if that fails, it will fallback to OpenAI pool.

*Load balancing / multi-provider distribution:*
If a provider pool contains multiple providers, you can distribute traffic (for example, balancing across Anthropic and OpenAI).

TIP: Provider pool (UI) = Backend pool (API)

=== Step 5: Configure MCP tools

On the Gateways page, select the *MCP* tab to configure your MCP tool discovery and tool execution. This MCP proxy is an aggregator of MCP servers, allowing multiple MCP servers behind a single endpoint. Agents can then find tools and call them through the gateway. To configure the MCP proxy, add the following:

* Display name: When you drag a provider pool, you give it a name.
* Model dropdown: Choose a model from the available models in the catalog.
* Load Balancing options: If you have multiple providers, you can load balance requests between them; for example, round robin.

MCP tools include a data catalog API, the memory store, a vector search service, and an MCP orchestrator. The *MCP orchestrator* is a built-in MCP server that enables programmatic tool calling. Agents can generate code to call multiple tools in a single orchestrated step, which reduces the number of round trips. For example, a workflow requiring 47 file reads can be reduced from 49 round trips to just 1. To add other tools, (for example, Slack), add the Slack MCP server endpoint.

When many tools are aggregated, listing all tools can consume significant tokens. With *deferred tool loading*, instead of returning all tools, the MCP gateway initially returns a tool search capability and the MCP orchestrator. The agent then searches for the specific tool it needs and retrieves only that subset. That way, the exchange of messages between the MCP gateway and the agent is small. This can reduce token usage significantly when you have many tools configured. 

*REVIEWERS: When/how exactly do you use the orchestrator? Also what happens after they create a gateway? Please provide an example of how to validate end-to-end routing against the gateway endpoint!*

*REVIEWERS: How do users connect to the ADP catalog + MCP servers exposed through RPCN?*

== Observability

After traffic flows through a gateway, you can inspect:

* Request volume
* Token usage
* Estimated spend
* Latency
* Per-model breakdown

This is central to governance: You can see and control usage by gateway boundary (for example, by team, environment, customer, or product).

*REVIEWERS: Where do those metrics appear in the UI, or how does a user validate observability after setup?*

== CEL routing

The AI Gateway uses Common Expression Language (CEL) for flexible routing and policy application. CEL expressions let you create sophisticated routing rules based on request properties without code changes. Use CEL to:

* Route requests to specific providers based on model family
* Apply different rate limits based on user tiers
* Enforce policies based on request content

The editor in the UI helps you discover available request fields (headers, path, body, and so on).

=== CEL examples

Route based on model family:

[,cel]
----
request.body.model.startsWith("anthropic/")
----

Apply a rule to all requests:

[,cel]
----
true
----

Route based on a header (for example, product tier):

[,cel]
----
request.headers['tier'][0] == "premium"
----

Guard for field existence:

[,cel]
----
has(request.body.max_tokens) && request.body.max_tokens > 1000
----

== Integrate with AI agents and tools

The AI Gateway provides standardized endpoints that work with various AI development tools and agents. This section shows how to configure popular tools to use your AI Gateway endpoints.

=== MCP server endpoint

If you've configured MCP tools in your gateway, AI agents can connect to the aggregated MCP endpoint:

* MCP endpoint URL: `https://gw.ai.panda.com/mcp`

* Headers required:
** `Authorization: Bearer your-api-key` 
** `rp-aigw-id: your-gateway-id`

This endpoint aggregates all MCP servers configured in your gateway, providing a unified interface for tool discovery and execution.

=== Environment variables

For consistent configuration across tools, set these environment variables:

[source,bash]
----
export REDPANDA_GATEWAY_URL="https://gw.ai.panda.com"
export REDPANDA_GATEWAY_ID="your-gateway-id" 
export REDPANDA_API_KEY="your-api-key"
----

Many tools and SDKs can automatically use these environment variables when configured appropriately.

=== Claude Code

Configure Claude Code to use AI Gateway endpoints using HTTP transport for the MCP connection.

*For Claude Code CLI:*

Use the `claude mcp add` command to configure the HTTP transport:

[source,bash]
----
claude mcp add --transport http redpanda-aigateway https://gw.ai.panda.com/mcp \
  --header "Authorization: Bearer YOUR_API_KEY" \
  --header "rp-aigw-id: GATEWAY_ID"
----

*Alternative configuration via config file:*

Create or edit `~/.claude/config.json`:

[source,json]
----
{
  "mcpServers": {
    "redpanda-ai-gateway": {
      "transport": "http",
      "url": "https://gw.ai.panda.com/mcp",
      "headers": {
        "Authorization": "Bearer YOUR_API_KEY",
        "rp-aigw-id": "GATEWAY_ID"
      }
    }
  },
  "apiProviders": {
    "redpanda": {
      "baseURL": "https://gw.ai.panda.com",
      "headers": {
        "rp-aigw-id": "your-gateway-id"
      }
    }
  }
}
----

=== VS Code extensions

Configure VS Code extensions that support OpenAI-compatible APIs:

*Continue extension:*

Edit your Continue config file (`~/.continue/config.json`):

[source,json]
----
{
  "models": [
    {
      "title": "Redpanda AI Gateway - GPT-4",
      "provider": "openai",
      "model": "openai/gpt-4",
      "apiBase": "https://gw.ai.panda.com",
      "apiKey": "your-api-key",
      "requestOptions": {
        "headers": {
          "rp-aigw-id": "your-gateway-id"
        }
      }
    },
    {
      "title": "Redpanda AI Gateway - Claude",
      "provider": "anthropic", 
      "model": "anthropic/claude-3-5-sonnet-20241022",
      "apiBase": "https://gw.ai.panda.com",
      "apiKey": "your-api-key",
      "requestOptions": {
        "headers": {
          "rp-aigw-id": "your-gateway-id"
        }
      }
    }
  ]
}
----

=== Cursor IDE

Configure Cursor to route requests through the AI Gateway:

. Open Cursor Settings (*Cursor* â†’ *Settings* or `Cmd+,`)
. Navigate to *AI* settings
. Add a custom OpenAI-compatible provider:

[source,json]
----
{
  "cursor.ai.providers.openai.apiBase": "https://gw.ai.panda.com",
  "cursor.ai.providers.openai.defaultHeaders": {
    "rp-aigw-id": "your-gateway-id"
  }
}
----

=== Custom applications

For custom applications using OpenAI or Anthropic SDKs:

*OpenAI SDK (Python):*

[source,python]
----
from openai import OpenAI

client = OpenAI(
    base_url="https://gw.ai.panda.com",
    api_key="your-api-key",
    default_headers={
        "rp-aigw-id": "your-gateway-id"
    }
)
----

*Anthropic SDK (Python):*

[source,python]
----
from anthropic import Anthropic

client = Anthropic(
    base_url="https://gw.ai.panda.com",
    api_key="your-api-key",
    default_headers={
        "rp-aigw-id": "your-gateway-id"
    }
)
----

*Node.js with OpenAI SDK:*

[source,javascript]
----
import OpenAI from 'openai';

const openai = new OpenAI({
  baseURL: 'https://gw.ai.panda.com',
  apiKey: process.env.OPENAI_API_KEY,
  defaultHeaders: {
    'rp-aigw-id': 'your-gateway-id'
  }
});
----

== Next steps

* xref:ai-agents:ai-gateway/ai-gateway-overview.adoc[]: Learn about AI Gateway architecture, deployment models, and common usage patterns.
* xref:ai-agents:ai-gateway/cel-routing-cookbook.adoc[]: Explore advanced CEL routing patterns for traffic distribution, cost optimization, and failover.
* xref:ai-agents:ai-gateway/mcp-aggregation-guide.adoc[]: Configure MCP server aggregation and deferred tool loading for AI agents.
* xref:ai-agents:ai-gateway/observability-logs.adoc[]: Monitor request logs, token usage, and costs through the observability dashboard.
* xref:ai-agents:ai-gateway/integrations/index.adoc[]: Connect AI development tools like Claude Code, Cursor, and Continue to your gateway.