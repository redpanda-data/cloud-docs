= AI Gateway Architecture
:description: Technical architecture of Redpanda AI Gateway, including how the control plane, data plane, and observability plane deliver high availability, cost governance, and multi-tenant isolation.
:page-topic-type: concept
:personas: app_developer, platform_admin
:learning-objective-1: Describe the three architectural planes of AI Gateway
:learning-objective-2: Explain the request lifecycle through policy evaluation stages
:learning-objective-3: Identify supported providers, features, and current limitations

include::ai-agents:partial$ai-gateway-byoc-note.adoc[]

This page provides technical details about AI Gateway's architecture, request processing, and capabilities. For an overview of AI Gateway, see xref:ai-agents:ai-gateway/what-is-ai-gateway.adoc[]

== Architecture overview

AI Gateway consists of a glossterm:control plane[] for configuration and management, a glossterm:data plane[] for request processing and routing, and an observability plane for monitoring and analytics.

// PLACEHOLDER: Add architecture diagram showing:
// 1. Control Plane:
//    - Workspace management
//    - Provider/model configuration
//    - Gateway creation and policy definition
//    - Admin console
//
// 2. Data Plane:
//    - Request ingestion
//    - Policy evaluation (rate limits → spend limits → routing → execution)
//    - Provider pool selection and failover
//    - MCP aggregation layer
//    - Response logging and metrics
//
// 3. Observability Plane:
//    - Request logs storage
//    - Metrics aggregation
//    - Dashboard UI

=== Control plane

The control plane manages gateway configuration and policy definition:

* **Workspace management**: Multi-tenant isolation with separate namespaces for different teams or environments
* **Provider configuration**: Enable and configure LLM providers (such as OpenAI and Anthropic)
* **Gateway creation**: Define gateways with specific routing rules, budgets, and rate limits
* **Policy definition**: Create CEL-based routing policies, spend limits, and rate limits
* **MCP server registration**: Configure which MCP servers are available to agents

=== Data plane

The data plane handles all runtime request processing:

* **Request ingestion**: Accept requests via OpenAI-compatible API endpoints
* **Authentication**: Validate API keys and gateway access
* **Policy evaluation**: Apply rate limits, spend limits, and routing policies
* **Provider pool management**: Select primary or fallback providers based on availability
* **MCP proxy**: Aggregate tools from multiple MCP servers with deferred loading
* **Response transformation**: Normalize provider-specific responses to OpenAI format
* **Metrics collection**: Record token usage, latency, and cost for every request

=== Observability plane

The observability plane provides monitoring and analytics:

* **Request logs**: Store full request/response history with prompt and completion content
* **Metrics aggregation**: Calculate token usage, costs, latency percentiles, and error rates
* **Dashboard UI**: Display real-time and historical analytics per gateway, model, or provider
* **Cost tracking**: Estimate spend based on provider pricing and token consumption

== Request lifecycle

When a request flows through AI Gateway, it passes through several policy and routing stages before reaching the LLM provider. Understanding this lifecycle helps you configure policies effectively and troubleshoot issues:

. Application sends request to gateway endpoint
. Gateway authenticates request
. Rate limit policy evaluates (allow/deny)
. Spend limit policy evaluates (allow/deny)
. Routing policy evaluates (which model/provider to use)
. Provider pool selects backend (primary/fallback)
. Request forwarded to LLM provider
. Response returned to application
. Request logged with tokens, cost, latency, status

Each policy evaluation happens synchronously in the request path. If rate limits or spend limits reject the request, the gateway returns an error immediately without calling the LLM provider, which helps you control costs.

=== MCP tool request lifecycle

For MCP tool requests, the lifecycle differs slightly to support deferred tool loading:

. Application discovers tools via `/mcp` endpoint
. Gateway aggregates tools from approved MCP servers
. Application receives search + orchestrator tools (deferred loading)
. Application invokes specific tool
. Gateway routes to appropriate MCP server
. Tool execution result returned
. Request logged with execution time, status

The gateway only loads and exposes specific tools when requested, which dramatically reduces the token overhead compared to loading all tools upfront.

ifdef::ai-hub-available[]
== AI Hub mode architecture

AI Gateway supports two modes. In Custom mode, administrators configure all routing rules and backend pools manually. In AI Hub mode, the gateway provides pre-configured intelligent routing.

=== Intelligent router

AI Hub mode implements an intelligent router with immutable system rules and user-configurable preferences:

*6 Pre-configured Backend Pools:*

* OpenAI (standard requests)
* OpenAI Streaming
* Anthropic with OpenAI-compatible transform (standard requests)
* Anthropic with OpenAI-compatible transform (streaming)
* Anthropic Native (direct passthrough for `/v1/messages`)
* Anthropic Native Streaming

*17 System Routing Rules:*

Immutable rules that route requests based on:

* Model prefix: `openai/*`, `anthropic/*`
* Model name patterns: `gpt-*`, `claude-*`, `o1-*`
* Special routing: embeddings, images, audio, content moderation, legacy completions → OpenAI only
* Native SDK detection: `/v1/messages` → Anthropic passthrough
* Streaming detection → Extended timeout backends

*Automatic Failover:*

Built-in fallback behavior when primary providers are unavailable (configurable via preference toggles).

*6 User Preference Toggles:*

Configurable preferences that influence routing without modifying rules (see xref:ai-gateway/admin/configure-ai-hub.adoc[] for details).

Configurable preferences that influence routing without modifying rules.

=== System-managed vs user-configurable resources

In AI Hub mode, resources are divided into two categories:

*System-Managed Resources* (immutable):

* Backend pool definitions
* Core routing rules
* Failover logic
* Provider selection algorithms

*User-configurable resources:*

* Provider credentials (OpenAI, Anthropic, Google Gemini)
* 6 preference toggles
* Rate limits (within bounds)
* Spend limits

This separation ensures consistent, reliable behavior while allowing customization of common preferences.

=== Ejecting to Custom mode

Gateways can be ejected from AI Hub mode to Custom mode in a one-way transition. After ejection:

* `gateway.mode` changes from `ai_hub` to `custom`
* All previously system-managed resources become user-configurable
* No more automatic AI Hub version updates
* Full control over routing rules, backend pools, and policies

This allows organizations to start with zero-configuration simplicity and graduate to full control when needed.

See xref:ai-gateway/admin/eject-to-custom-mode.adoc[] for the ejection process.
endif::[]

// == Supported features

// === LLM providers

// * OpenAI
// * Anthropic
// * // PLACEHOLDER: Google, AWS Bedrock, Azure OpenAI, others?

// === API compatibility

// * OpenAI-compatible `/v1/chat/completions` endpoint
// * // PLACEHOLDER: Streaming support?
// * // PLACEHOLDER: Embeddings support?
// * // PLACEHOLDER: Other endpoints?

// === Policy features

// * CEL-based routing expressions
// * Rate limiting (// PLACEHOLDER: per-gateway, per-header, per-tenant?)
// * Monthly spend limits (// PLACEHOLDER: per-gateway, per-workspace?)
// * Provider pools with automatic failover
// * // PLACEHOLDER: Caching support?

// === MCP support

// * MCP server aggregation
// * Deferred tool loading (often 80-90% token reduction depending on configuration)
// * JavaScript orchestrator for multi-step workflows
// * PLACEHOLDER: Tool execution sandboxing?

// === Observability

// * Request logs with full prompt/response history
// * Token usage tracking
// * Estimated cost per request
// * Latency metrics
// * PLACEHOLDER: Metrics export? OpenTelemetry support?

// == Current limitations

// * // PLACEHOLDER: List current limitations, for example:
// ** // - Custom model deployments (Azure OpenAI BYOK, AWS Bedrock custom models)
// ** // - Response caching
// ** // - Prompt templates/versioning
// ** // - Guardrails (PII detection, content moderation)
// ** // - Multi-region active-active deployment
// ** // - Metrics export to external systems
// ** // - Budget alerts/notifications

== Next steps

* xref:ai-agents:ai-gateway/gateway-quickstart.adoc[]: Route your first request through AI Gateway
* xref:ai-agents:ai-gateway/mcp-aggregation-guide.adoc[]: Configure MCP server aggregation for AI agents
