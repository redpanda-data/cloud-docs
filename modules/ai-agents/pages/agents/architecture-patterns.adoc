= Agent Architecture Patterns
:description: Design maintainable agent systems with single-agent and multi-agent patterns based on domain complexity.
:page-topic-type: best-practices
:personas: ai_agent_developer, streaming_developer
:learning-objective-1: Evaluate single-agent versus multi-agent architectures for your use case
:learning-objective-2: Choose appropriate LLM models based on task requirements
:learning-objective-3: Apply agent boundary design principles for maintainability

Design agent systems that are maintainable, discoverable, and reliable by choosing the right architecture pattern and applying clear boundary principles.

After reading this page, you will be able to:

* [ ] {learning-objective-1}
* [ ] {learning-objective-2}
* [ ] {learning-objective-3}

== Why architecture matters

Agent architecture determines how you manage complexity as your system grows. The right pattern depends on your domain complexity, organizational structure, and how you expect requirements to evolve.

=== When simple agents become unmaintainable

Single agents work well initially but break down as complexity grows.

Warning signs include system prompts exceeding 2000 words, too many tools for the LLM to select correctly, multiple teams modifying the same agent, and changes in one domain breaking others. These symptoms indicate you need architectural boundaries, not just better prompts.

=== Domain complexity drives architecture

Match agent architecture to domain structure:

[cols="2,3,3"]
|===
| Domain Characteristics | Architecture Fit | Reasoning

| Single business area, stable requirements
| Single agent
| Simplicity outweighs flexibility needs

| Multiple business areas, shared infrastructure
| Root agent with internal subagents
| Domain separation without deployment complexity

| Cross-organization workflows, independent evolution
| External agent-to-agent
| Organizational boundaries require system boundaries
|===

=== Trade-offs

Every architecture pattern involves trade-offs.

- *Latency versus isolation:* Internal subagents have lower latency because they avoid network calls, but they share a failure domain. External agents have higher latency due to network overhead, but they provide independent failure isolation.

- *Shared state versus independence:* Single deployments share model, budget, and policies but offer less flexibility. Multiple deployments allow independent scaling and updates but add coordination complexity.

- *Complexity now versus complexity later:* Starting simple means faster initial development but may require refactoring. Starting structured requires more upfront work but makes the system easier to extend.

For foundational concepts on how agents execute and manage complexity, see xref:ai-agents:agents/concepts.adoc[].

== Single-agent pattern

A single-agent architecture uses one agent with one system prompt and one tool set to handle all requests.

This pattern works best for narrow domains with limited scope, single data sources, and tasks that don't require specialized subsystems.

=== When to use single agents

Use single agents for focused problems that won't expand significantly.

Examples include order lookup agents that retrieve history from a single topic, weather agents that query APIs and return formatted data, and inventory checkers that report stock levels.

=== Trade-offs

Single agents are simpler to build and maintain. You have one system prompt, one tool set, and one deployment.

However, all capabilities must coexist in one agent. Adding features increases complexity rapidly, making single agents difficult to scale to multi-domain problems.

== Root agent with subagents pattern

A multi-agent architecture uses a root agent that delegates to specialized internal subagents.

This pattern works for complex domains spanning multiple areas, multiple data sources with different access patterns, and tasks requiring specialized expertise within one deployment.

NOTE: Subagents in Redpanda Cloud are internal specialists within a single agent. They share the parent agent's model, budget, and policies, but each can have different names, descriptions, system prompts, and MCP tools.

=== How it works

The root agent interprets user requests and routes them to appropriate subagents.

Each subagent owns a specific business area with focused expertise. Subagents access only the MCP tools they need.

All subagents share the same LLM model and budget from the parent agent.

=== Example: E-commerce platform

A typical e-commerce agent includes a root agent that interprets requests and delegates to specialists, an order subagent for processing, history, and status updates, an inventory subagent for stock checks and warehouse operations, and a customer subagent for profiles, preferences, and history. All subagents share the same model but have different system prompts and tool access.

=== Why choose internal subagents

Internal subagents provide domain isolation, allowing you to update the order subagent without affecting inventory. Debugging is easier because each subagent has narrow scope and fewer potential failure points. All subagents share resources, reducing complexity and cost compared to separate deployments. Use internal subagents when you need domain separation within a single agent deployment.

== External agent-to-agent pattern

External A2A integration connects agents across organizational boundaries, platforms, or independent systems.

NOTE: Cross-agent calling between separate Redpanda Cloud agents is not supported. This pattern applies to connecting Redpanda Cloud agents with external agents you host elsewhere.

=== When to use external A2A

Use external A2A for multi-organization workflows that coordinate agents across company boundaries, for platform integration connecting Redpanda Cloud agents with agents hosted elsewhere, and when agents require different deployment environments such as GPU clusters, air-gapped networks, or regional constraints.

=== How it works

Agents communicate using the xref:ai-agents:agents/a2a-concepts.adoc[A2A protocol], a standard HTTP-based protocol for discovery and invocation. Each agent manages its own credentials and access control independently, and can deploy, scale, and update without coordinating with other agents. Agent cards define capabilities without exposing implementation details.

=== Example: Multi-platform customer service

A customer service workflow might span multiple platforms:

* Redpanda Cloud agent accesses real-time order and inventory data
* CRM agent hosted elsewhere manages customer profiles and support tickets
* Payment agent from a third party handles transactions in a secure environment

Each agent runs on its optimal infrastructure while coordinating through A2A.

=== Why choose external A2A

External A2A lets different teams own and deploy their agents independently, with each agent choosing its own LLM, tools, and infrastructure. Sensitive operations stay in controlled environments with security isolation, and you can add agents incrementally without rewriting existing systems.

=== Trade-offs

External A2A adds network latency on every cross-agent call, and authentication complexity multiplies with each agent requiring credential management. Removing capabilities or changing contracts requires coordination across consuming systems, and debugging requires tracing requests across organizational boundaries.

=== Choosing between patterns

[cols="1,2,2"]
|===
| Use Case | Internal Subagents | External A2A

| Domain separation within one team
| Recommended
| Unnecessary complexity

| Cross-organization workflows
| Not possible
| Required

| Shared infrastructure acceptable
| Simpler
| Use external if independence needed

| Different deployment requirements
| Limited (same cluster)
| Full flexibility

| Real-time performance critical
| Lower latency
| Higher latency
|===

For implementation details on external A2A integration, see xref:ai-agents:agents/integration-overview.adoc[].

== Common anti-patterns

Avoid these architecture mistakes that lead to unmaintainable agent systems.

=== The monolithic prompt

A monolithic prompt is a single 3000+ word system prompt covering multiple domains.

This pattern fails because LLM confusion increases with prompt length, multiple teams modify the same prompt creating conflicts and unclear ownership, and changes to one domain risk breaking others.

Split into domain-specific subagents instead. Each subagent gets a focused prompt under 500 words.

=== The tool explosion

A tool explosion occurs when a single agent has 30+ tools from every MCP server in the cluster.

This pattern fails because the LLM struggles to choose correctly from large tool sets, tool descriptions compete for limited prompt space, and the agent invokes wrong tools with similar names, wasting iteration budget on selection mistakes.

Limit tools per agent. Use subagents to partition tools by domain. For tool design patterns, see xref:ai-agents:mcp/remote/tool-patterns.adoc[].

=== Premature A2A splitting

Premature splitting creates three separate A2A agents when all logic could fit in one agent with internal subagents.

This pattern fails because network latency affects every cross-agent call, authentication complexity multiplies with three sets of credentials, debugging requires correlating logs across systems, and you manage three deployments instead of one.

Start with internal subagents for domain separation. Split to external A2A only when you need organizational boundaries or different infrastructure.

=== Unbounded tool chaining

Unbounded chaining sets max iterations to 100, returns hundreds of items from tools, and places no constraints on tool call frequency.

This pattern fails because the context window fills with tool results, requests time out before completion, costs spiral with many iterations multiplied by large context, and the agent loses track of the original goal.

Design workflows to complete in 20-30 iterations. Return paginated results from tools. Add prompt constraints like "Never call the same tool more than 3 times per request."

== Model selection guide

Choose models based on task complexity, latency requirements, and cost constraints. The Redpanda Cloud Console displays available models with descriptions when creating agents.

=== Match models to task complexity

For simple queries, choose cost-effective models such as Haiku.

For balanced workloads, choose mid-tier models. Look for standard names like base GPT versions, or Sonnet tiers.

For complex reasoning, choose premium models. Look for labels like Opus or the highest version numbers.

=== Balance latency and model size

For real-time responses, choose smaller models. Use models optimized for speed, such as Mini or base tiers.

For batch processing, optimize for accuracy over speed. Use larger models when users don't wait for results.

=== Optimize for cost and volume

For high volume, use cost-effective models. Smaller tiers reduce costs while maintaining acceptable quality.

For critical accuracy, use premium models. Higher costs are justified when errors are costly.

=== Model provider documentation

For complete model specifications, capabilities, and pricing:

* link:https://platform.openai.com/docs/models[OpenAI Models^]
* link:https://docs.anthropic.com/claude/docs/models-overview[Anthropic Claude Models^]
* link:https://ai.google.dev/gemini-api/docs/models[Google Gemini Models^]

== Design principles

Follow these principles to create maintainable agent systems.

=== Explicit agent boundaries

Each agent should have clear scope and responsibilities. Define scope explicitly in the system prompt, assign a specific tool set for the agent's domain, and specify well-defined inputs and outputs.

Do not create agents with overlapping responsibilities. Overlapping domains create confusion about which agent handles which requests.

=== Tool scoping per agent

Assign tools to the agent that needs them. Don't give all agents access to all tools. Limit tool access based on agent purpose.

Tool scoping reduces misuse risk and makes debugging easier.

=== Error handling and fallbacks

Design agents to handle failures gracefully.

Use retry logic for transient failures like network timeouts. Report permanent failures like invalid parameters immediately.

Provide clear error messages to users. Log errors for debugging.

== Summary

Choose architecture patterns based on domain complexity and organizational needs:

[cols="1,2,2"]
|===
| Pattern | Use When | Trade-off

| Single agent
| Narrow domain, single organization
| Limited scalability

| Internal subagents
| Complex domains, shared infrastructure
| Higher operational complexity

| External A2A
| Multi-organization, independent systems
| Network latency, coordination overhead
|===

== Next steps

* xref:ai-agents:agents/integration-overview.adoc[]: Choose between agents invoking tools and pipelines calling agents
* xref:ai-agents:agents/a2a-concepts.adoc[]: Learn how the A2A protocol enables agent communication
* xref:ai-agents:mcp/remote/tool-patterns.adoc[]: Explore tool design patterns
* xref:ai-agents:agents/overview.adoc[]: Review agent components
* xref:ai-agents:mcp/remote/best-practices.adoc[]: Learn MCP tool best practices
