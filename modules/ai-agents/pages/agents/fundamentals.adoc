= Agent Fundamentals
:description: Understand the four essential components of an AI agent and how they work together.
:page-topic-type: concepts
:learning-objective-1: Describe the four essential components of an AI agent
:learning-objective-2: Explain how system prompts constrain agent behavior
:learning-objective-3: Distinguish between focused and unfocused tool designs

Every AI agent consists of four essential components that work together to interpret user intent and accomplish tasks.

After reading this page, you will be able to:

* [ ] {learning-objective-1}
* [ ] {learning-objective-2}
* [ ] {learning-objective-3}

== Agent components

An AI agent consists of four components:

. System prompt: Defines scope, behavior, and constraints
. LLM: Performs reasoning and tool selection
. Tools: External capabilities exposed via MCP
. Context: Conversation state and relevant inputs

Everything else builds on these primitives.

== System prompts

The system prompt defines agent behavior. It should explicitly specify:

* The agent's role
* Its responsibilities
* Which tools are available
* Expected response structure
* Hard constraints and exclusions

Clear constraints prevent unintended behavior and reduce tool misuse.

=== Example: Order analytics agent

[,text]
----
You are an order analytics agent.

Responsibilities:
- Answer questions about customer orders
- Analyze recent order trends

Available tools:
- get_customer_orders
- analyze_recent_orders

Rules:
- Present structured data as tables
- Always state the analysis time window
- Never expose payment information
- Only analyze data from the last 90 days unless explicitly requested
----

This prompt defines scope (order analytics), lists available actions (two tools), and sets clear boundaries (no payment information, 90-day window).

== Tools

Tools are the actions your agent can take. With Redpanda Cloud:

* You define tools as MCP servers
* Each tool is a Redpanda Connect configuration with a component and meta block
* The LLM discovers and invokes tools automatically
* Tools access streaming data, databases, APIs, or services

Choose tools that are:

* Focused: One clear purpose per tool
* Reliable: Consistent results for the same inputs
* Observable: Can log actions for debugging

For detailed guidance on creating tools, see xref:ai-agents:mcp/remote/create-tool.adoc[].

=== Tool design principles

Keep each tool focused on a single action. Instead of one tool that "manages orders," create separate tools for:

* `get_customer_orders`: Retrieve order history
* `update_order_status`: Change order state
* `calculate_order_total`: Compute costs

Focused tools are easier to test, debug, and reason about.

== LLM selection

The LLM performs reasoning, interprets prompts, and decides which tools to use. Choose models based on:

* Task complexity: Simple queries vs. multi-step workflows
* Latency requirements: Real-time responses vs. batch processing
* Cost constraints: Balance capability with usage volume

Model families available through Redpanda Cloud integrations:

* OpenAI GPT models: Strong general-purpose reasoning
* Anthropic Claude models: Extended context windows
* Google Gemini models: Multimodal capabilities

For agent architectures that combine multiple models, see xref:ai-agents:agents/architecture-patterns.adoc[].

== Context

Context includes:

* Conversation history: Previous messages in the session
* Retrieved data: Results from tools or database queries
* Streaming context: Real-time events from Redpanda topics

Effective context management:

* Keep conversation history focused (prune irrelevant turns)
* Use streaming data for real-time decisions
* Store long-term state in databases, not conversation memory

=== Context window limits

Every LLM has a context window (the total amount of text it can process). When the conversation history plus tool results exceeds this limit, the agent fails or loses information.

Strategies to manage context:

* Summarize old conversations: Replace early messages with summaries
* Use semantic search: Retrieve only relevant conversation turns
* Store state externally: Put order details in a database, not in chat history

=== Streaming context

Agents can process real-time events from Redpanda topics as part of their context. This enables use cases like:

* Monitoring dashboards that answer questions about live metrics
* Alert systems that detect anomalies in streaming data
* Recommendation engines that adapt to current user behavior

== How components work together

Here's the flow when a user makes a request:

. User sends a message ("What were our sales yesterday?")
. LLM receives the system prompt, conversation history, and the user message
. LLM decides which tool(s) to invoke based on the system prompt
. Tools execute and return results
. LLM incorporates tool results into context
. LLM generates a response to the user

This cycle repeats until the agent completes the task or reaches a stopping condition.

== Next steps

* xref:ai-agents:agents/quickstart.adoc[]: Get your first agent running
* xref:ai-agents:agents/create-agent.adoc[]: Create an agent through the UI
* xref:ai-agents:agents/architecture-patterns.adoc[]: Explore agent design patterns
