= Monitor Agent Activity
:description: Monitor agent execution, analyze conversation history, track token usage, and debug issues using inspector, transcripts, and agent data topics.
:page-topic-type: how-to
:personas: agent_developer, platform_admin
:learning-objective-1: pass:q[Verify agent behavior using the *Inspector* tab]
:learning-objective-2: Track token usage and performance metrics
:learning-objective-3: Debug agent execution using transcripts

Use monitoring to track agent performance, analyze conversation patterns, debug execution issues, and optimize token costs.

After reading this page, you will be able to:

* [ ] {learning-objective-1}
* [ ] {learning-objective-2}
* [ ] {learning-objective-3}

For conceptual background on traces and observability, see xref:ai-agents:observability/concepts.adoc[].

== Prerequisites

You must have a running agent. If you do not have one, see xref:ai-agents:agents/quickstart.adoc[].

== Debug agent execution with transcripts

The transcripts view shows execution traces with detailed timing, errors, and performance metrics. Use this view to debug issues, verify agent behavior, and monitor performance in real-time.

:context: agent
include::ai-agents:partial$transcripts-ui-guide.adoc[]

=== Check agent health

Use the transcripts view to verify your agent is healthy:

Healthy agent indicators:

* Timeline shows consistent green bars (successful executions)
* Duration stays within expected range (check summary panel)
* Token usage is stable (not growing unexpectedly)
* LLM calls match expected patterns (1-3 calls for simple queries)
* No error bars in timeline

Warning signs:

* Red bars in timeline: Errors or failures, click to investigate
* Increasing duration: May indicate context window growth or slow tool calls
* High token usage: Check if conversation history is too long
* Many LLM calls: Agent may be stuck in loops or making unnecessary iterations
* Missing transcripts: Agent may be stopped or encountering deployment issues

Common patterns to investigate:

* All recent transcripts show errors: Check agent status, MCP server connectivity, or system prompt
* Duration increasing over session: Context window filling up, consider clearing conversation history
* Spiky timeline (alternating success/error): Intermittent tool failures or external API issues
* High token usage with few LLM calls: Large tool results or verbose system prompts

=== Debug with transcripts

Use transcripts to diagnose specific issues:

Agent not responding

. Check the timeline for recent transcripts. If none appear, the agent may be stopped.
. Verify agent status in the main *AI Agents* view.
. Look for error transcripts with deployment or initialization failures.

Tool execution errors

. Select the failed transcript (red bar in timeline).
. Expand the trace hierarchy to find the tool invocation span.
. Check the span details for error messages.
. Cross-reference with MCP server status.

Slow performance

. Compare duration across multiple transcripts in the summary panel.
. Look for specific spans with long durations (wide bars in trace list).
. Check if LLM calls are taking longer than expected.
. Verify tool execution time by examining nested spans.

Unexpected behavior

. Select the transcript for the problematic request.
. Expand the full trace hierarchy to see all operations.
. Look for missing tool calls (agent didn't invoke expected tools).
. Check LLM call count: excessive calls may indicate loops.

=== Track token usage and costs

View token consumption in the Summary panel when you select a transcript:

* Input tokens: Tokens sent to the LLM (system prompt + conversation history + tool results)
* Output tokens: Tokens generated by the LLM (agent responses)
* Total tokens: Sum of input and output

Calculate cost per request:

----
Cost = (input_tokens × input_price) + (output_tokens × output_price)
----

Example: GPT-5.2 with 4,302 input tokens and 1,340 output tokens at $0.00000175 per input token and $0.000014 per output token costs $0.026 per request.

For cost optimization strategies, see xref:ai-agents:agents/concepts.adoc#cost-calculation[Cost calculation].

== Test agent behavior with the inspector

The *Inspector* tab provides real-time conversation testing. Use it to test agent responses interactively and verify behavior before deploying changes.

=== Access the inspector

. Navigate to *Agentic AI* > *AI Agents* in the Redpanda Cloud Console.
. Click your agent name.
. Open the *Inspector* tab.
. Enter test queries and review responses.
. Check the conversation panel to see tool calls.
. Start a new session to test fresh conversations or click *Clear context* to reset history.

=== Testing best practices

Test your agents systematically with these scenarios:

* Boundary cases: Test requests at the edge of agent capabilities to verify scope enforcement.
* Error handling: Request unavailable data to verify graceful degradation.
* Iteration count: Monitor how many iterations complex requests require.
* Ambiguous input: Send vague queries to verify clarification behavior.
* Token usage: Track tokens per request to estimate costs.

== Next steps

* xref:ai-agents:observability/concepts.adoc[]
* xref:ai-agents:agents/troubleshooting.adoc[]
* xref:ai-agents:agents/concepts.adoc[]
