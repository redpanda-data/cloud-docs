= AI Gateway Quickstart
:description: Learn how to configure the AI Gateway for unified access to multiple LLM providers and MCP servers through a single endpoint.


NOTE: AI Gateway is supported on BYOC clusters running Redpanda version 25.3 and later.

The Redpanda AI Gateway is a production-grade proxy that provides unified access to multiple Large Language Model (LLM) providers and Model Context Protocol (MCP) servers through a single endpoint. It maintains centralized control over routing, rate limiting, cost optimization, security, and observability.

Common gateway patterns:

* *Team isolation*: Create separate gateways for each team to track usage and enforce budgets independently.
* *Environment separation*: Use different gateways for staging and production with appropriate rate limits.
* *Failover*: Configure a primary provider pool with a fallback pool for high availability.
* *A/B testing*: Distribute traffic across providers to compare performance and cost.

== Prerequisites

* Access to the AI Gateway UI (provided by your administrator)
* API key for at least one LLM provider: OpenAI or Anthropic
* Optional: MCP server endpoints if you plan to use tool aggregation

== Get started

Before a gateway owner can create a gateway, an administrator must enable LLM providers and models.

=== Step 1: Enable a provider 

Providers represent upstream services (Anthropic, OpenAI) and associated credentials. Providers are disabled by default. An administrator must enable them explicitly by adding credentials.

. In AI Gateways, navigate to *Providers*.
. Select a provider (for example, Anthropic).
. On the *Configuration* tab, enter your API Key.

=== Step 2: Enable models

The model catalog is the set of models made available through the gateway. Models are disabled by default. An administrator must enable them explicitly.

The infrastructure that is serving the model is different based on the provider you select. For example, OpenAI has different reliability and availability metrics than Anthropic. When you consider all the metrics, you can design your gateway to use different providers for different use cases.

. Navigate to *Models*.
. Enable the models you want exposed through gateways.

==== Model naming convention

Model provider requests must use the `vendor/model_id` format in the model property of the request body, and include the `rp-aigw-id` header with the gateway ID the request is being sent to. The following example routes OpenAI API calls through Redpanda's AI Gateway for centralized control.

[source,python]
----
# Example: Using the OpenAI Python SDK with AI Gateway
from openai import OpenAI

client = OpenAI(
    base_url="https://gw.ai.panda.com", <1>
    api_key="your-api-key",
)

# Add header per request
response = client.chat.completions.create(
    model="openai/gpt-5", <2>
    messages=[{"role": "user", "content": "Hello!"}],
    extra_headers={
        "rp-aigw-id": "gateway-abc"  # Override for this request
    } <3>
)
----
<1> This redirects the OpenAI client to the AI Gateway endpoint.
<2> The `model` property uses the `vendor/model_id` format as required by the AI Gateway.
<3> Includes the `rp-aigw-id` header to specify which gateway configuration to use.

=== Step 3: Create a gateway 

A gateway is a logical configuration boundary (policies + routing + observability) on top of a single deployment. It's a "virtual gateway" that you can create per team, environment (staging/production), product, or customer.

. Navigate to *Gateways*.
. Click *Create Gateway*.
. Choose a name, workspace, and optional metadata. 
+
TIP: A _workspace_ is conceptually similar to a _resource group_ in Redpanda streaming. 

. After creation, copy the *Gateway Endpoint* from the gateway detail page.

=== Step 4: Configure LLM routing 

On the Gateways page, select the *LLM* tab to configure rate limits, spend limits, and routing policies.

The LLM routing pipeline visually represents the request lifecycle:

. Rate Limit: For example, global rate limit of 100 requests/second
. Spend Limit / Monthly Budget: For example, $15K/month with blocking enforcement
. Routing to a primary provider pool with optional fallback provider pools: For example, primary route to Anthropic pool, fallback to OpenAI pool

*Load balancing / multi-provider distribution:*
If a provider pool contains multiple providers, you can distribute traffic (for example, balancing across Anthropic and OpenAI).

TIP: Provider pool (UI) = Backend pool (API)

=== Step 5: Configure MCP tools

NOTE: Model Context Protocol (MCP) is a standard for connecting AI agents to external tools and data sources. MCP servers expose tools that agents can discover and call.

On the Gateways page, select the *MCP* tab to configure tool discovery and tool execution. 

You can aggregate multiple MCP servers behind a single endpoint. For example:

* Data catalog API
* MCP orchestrator
* Research memory store
* Vector search service

*How MCP works:*

* You configure MCP server endpoints in the MCP gateway.
* The gateway presents a single aggregated MCP surface to the agent.
* Agents can list/search tools and call them through the gateway.

*MCP orchestrator:*

The orchestrator is a built-in MCP server that enables programmatic tool calling. The agent can generate JavaScript to call multiple tools in a single orchestrated step, which reduces the number of round trips. For example, a workflow requiring 47 file reads can be reduced from 49 round trips to just 1.

*REVIEWERS: When/how exactly do you use the orchestrator? Also what happens after they create a gateway? Please provide an example of how to validate end-to-end routing against the gateway endpoint!*

=== Step 6: Understand deferred tool loading (token savings)

When many tools are aggregated, listing all tools can consume significant tokens. Deferred tool loading effectively behaves as lazy tool discovery:

* Instead of returning all tools, the MCP gateway initially returns:
** a *tool search* capability, and
** the *MCP orchestrator*
* The agent then searches for the specific tool it needs and retrieves only that subset.

This can reduce token usage significantly (for example, 80-90% depending on how many servers/tools are configured).

== Observability

After traffic flows through a gateway, you can inspect:

* Request volume
* Token usage
* Estimated spend
* Latency
* Per-model breakdown

This is central to governance: You can see and control usage by gateway boundary (for example, by team, environment, customer, or product).

*REVIEWERS: Where do those metrics appear in the UI, or how does a user validate observability after setup?*

== CEL routing

The AI Gateway uses Common Expression Language (CEL) for flexible routing and policy application. CEL expressions let you create sophisticated routing rules based on request properties without code changes. Use CEL to:

* Route requests to specific providers based on model family
* Apply different rate limits based on user tiers
* Enforce policies based on request content

An inline editor in the UI helps you discover available request fields (headers, path, body, and so on).

=== CEL examples

Route based on model family:

[,cel]
----
request.body.model.startsWith("anthropic/")
----

Apply a rule to all requests:

[,cel]
----
true
----

Route based on a header (for example, product tier):

[,cel]
----
request.headers['tier'][0] == "premium"
----

Guard for field existence:

[,cel]
----
has(request.body.max_tokens) && request.body.max_tokens > 1000
----

== Integrate with AI agents and tools

The AI Gateway provides standardized endpoints that work with various AI development tools and agents. This section shows how to configure popular tools to use your AI Gateway endpoints.

=== MCP server endpoint

If you've configured MCP tools in your gateway, AI agents can connect to the aggregated MCP endpoint:

* MCP endpoint URL: `https://gw.ai.panda.com/mcp`

* Headers required:
** `Authorization: Bearer your-api-key` 
** `rp-aigw-id: your-gateway-id`

This endpoint aggregates all MCP servers configured in your gateway, providing a unified interface for tool discovery and execution.

=== Environment variables

For consistent configuration across tools, set these environment variables:

[source,bash]
----
export REDPANDA_GATEWAY_URL="https://gw.ai.panda.com"
export REDPANDA_GATEWAY_ID="your-gateway-id" 
export REDPANDA_API_KEY="your-api-key"
----

Many tools and SDKs can automatically use these environment variables when configured appropriately.

=== Claude Code

Configure Claude Code to use AI Gateway endpoints by creating or editing your MCP configuration file.

*For Claude Desktop (with VS Code extension):*

Create or edit `.vscode/settings.json`:

[source,json]
----
{
  "claude.mcpServers": {
    "redpanda-ai-gateway": {
      "command": "node",
      "args": ["/path/to/mcp-redpanda-gateway/index.js"],
      "env": {
        "GATEWAY_ENDPOINT": "https://gw.ai.panda.com",
        "GATEWAY_ID": "your-gateway-id",
        "API_KEY": "your-api-key"
      }
    }
  }
}
----

*For Claude Code CLI:*

Create or edit `~/.claude/config.json`:

[source,json]
----
{
  "mcpServers": {
    "redpanda-ai-gateway": {
      "command": "npx",
      "args": ["@redpanda/mcp-ai-gateway"],
      "env": {
        "REDPANDA_GATEWAY_URL": "https://gw.ai.panda.com",
        "REDPANDA_GATEWAY_ID": "your-gateway-id",
        "REDPANDA_API_KEY": "your-api-key"
      }
    }
  },
  "apiProviders": {
    "redpanda": {
      "baseURL": "https://gw.ai.panda.com",
      "headers": {
        "rp-aigw-id": "your-gateway-id"
      }
    }
  }
}
----

=== VS Code extensions

Configure VS Code extensions that support OpenAI-compatible APIs:

*Continue extension:*

Edit your Continue config file (`~/.continue/config.json`):

[source,json]
----
{
  "models": [
    {
      "title": "Redpanda AI Gateway - GPT-4",
      "provider": "openai",
      "model": "openai/gpt-4",
      "apiBase": "https://gw.ai.panda.com",
      "apiKey": "your-api-key",
      "requestOptions": {
        "headers": {
          "rp-aigw-id": "your-gateway-id"
        }
      }
    },
    {
      "title": "Redpanda AI Gateway - Claude",
      "provider": "anthropic", 
      "model": "anthropic/claude-3-5-sonnet-20241022",
      "apiBase": "https://gw.ai.panda.com",
      "apiKey": "your-api-key",
      "requestOptions": {
        "headers": {
          "rp-aigw-id": "your-gateway-id"
        }
      }
    }
  ]
}
----

=== Cursor IDE

Configure Cursor to route requests through the AI Gateway:

. Open Cursor Settings (*Cursor* â†’ *Settings* or `Cmd+,`)
. Navigate to *AI* settings
. Add a custom OpenAI-compatible provider:

[source,json]
----
{
  "cursor.ai.providers.openai.apiBase": "https://gw.ai.panda.com",
  "cursor.ai.providers.openai.defaultHeaders": {
    "rp-aigw-id": "your-gateway-id"
  }
}
----

=== Custom applications

For custom applications using OpenAI or Anthropic SDKs:

*OpenAI SDK (Python):*

[source,python]
----
from openai import OpenAI

client = OpenAI(
    base_url="https://gw.ai.panda.com",
    api_key="your-api-key",
    default_headers={
        "rp-aigw-id": "your-gateway-id"
    }
)
----

*Anthropic SDK (Python):*

[source,python]
----
from anthropic import Anthropic

client = Anthropic(
    base_url="https://gw.ai.panda.com",
    api_key="your-api-key",
    default_headers={
        "rp-aigw-id": "your-gateway-id"
    }
)
----

*Node.js with OpenAI SDK:*

[source,javascript]
----
import OpenAI from 'openai';

const openai = new OpenAI({
  baseURL: 'https://gw.ai.panda.com',
  apiKey: process.env.OPENAI_API_KEY,
  defaultHeaders: {
    'rp-aigw-id': 'your-gateway-id'
  }
});
----
