= Monitor Agent Activity
:description: Monitor agent execution, analyze conversation history, track token usage, and debug issues using inspector, transcripts, and agent data topics.
:page-topic-type: how-to
:personas: ai_agent_developer, platform_admin
:learning-objective-1: pass:q[Test agents interactively using the *Inspector* tab]
:learning-objective-2: Consume session and task topics for analysis
:learning-objective-3: Debug agent behavior using Transcripts

Monitor your agents to track performance, analyze conversations, debug issues, and optimize costs.

After reading this page, you will be able to:

* [ ] {learning-objective-1}
* [ ] {learning-objective-2}
* [ ] {learning-objective-3}

For conceptual background on traces and observability, see xref:ai-agents:observability/concepts.adoc[].

== Prerequisites

You must have a running agent. If you do not have one, see xref:ai-agents:agents/quickstart.adoc[].

== Test agents interactively

The *Inspector* tab provides real-time conversation testing and debugging.

You can use the Inspector to:

* Test agent responses interactively
* View full conversation history
* See tool invocations and results
* Monitor token usage per request
* Test error scenarios

=== Access the inspector

. Navigate to *Agentic AI* > *AI Agents* in the Redpanda Cloud UI.
. Click your agent name.
. Open the *Inspector* tab.
. Enter test queries and review responses.
. Check the conversation panel to see tool calls.
. Start a new session to test fresh conversations.

=== Testing best practices

Test your agents systematically:

* **Boundary cases**: Test requests at the edge of agent capabilities to verify scope enforcement
* **Error handling**: Request unavailable data to verify graceful degradation
* **Iteration count**: Monitor how many iterations complex requests require
* **Ambiguous input**: Send vague queries to verify clarification behavior
* **Token usage**: Track tokens per request to estimate costs

== View execution traces

The *Transcripts* view shows agent execution traces with detailed timing and error information.

=== What Transcripts show

* Agent startup and initialization
* Request and response flow
* Tool invocations and results
* Error messages and failures
* Token usage per request
* OpenTelemetry trace data

=== Access Transcripts

. Navigate to *Agentic AI* > *AI Agents*.
. Click your agent name.
. Click *Transcripts* in the left navigation.
. Select a transcript to view execution details.

=== When to use Transcripts

* Agent won't start (deployment issues)
* Tool execution errors
* Unexpected agent behavior
* Debugging conversation flow
* Verifying tool selection logic

== Consume agent data topics

Agents emit structured data to two Redpanda topics for monitoring and analysis.

=== Sessions topic

The sessions topic (`redpanda.aiagent.<agent-id>.sessions`) contains all conversation messages.

Use the sessions topic to:

* Review past conversations
* Analyze conversation patterns
* Debug multi-turn interactions
* Understand context management

=== Tasks topic

The tasks topic (`redpanda.aiagent.<agent-id>.tasks`) contains task execution records with status and artifacts.

Use the tasks topic to:

* Monitor task completion rates
* Track token usage and costs
* Debug failed tasks
* Analyze agent performance

=== Access agent topics

[tabs]
=====
Cloud UI::
+
--
. Navigate to *Topics* in the Redpanda Cloud UI.
. Find `redpanda.aiagent.<agent-id>.sessions` or `redpanda.aiagent.<agent-id>.tasks`.
. Click *Messages* to view recent data.
. Use filters to search for specific sessions or task states.
--

rpk::
+
--
Consume recent sessions:

[,bash]
----
rpk topic consume redpanda.aiagent.<agent-id>.sessions --offset end -n 10
----

Consume recent tasks:

[,bash]
----
rpk topic consume redpanda.aiagent.<agent-id>.tasks --offset end -n 10
----
--

Data Plane API::
+
--
Use the link:/api/doc/cloud-dataplane/[Data Plane API] to programmatically consume agent data and integrate with your monitoring pipeline.
--
=====

For schema details, see xref:ai-agents:agents/concepts.adoc#agent-data-topics[].

== Analyze conversation history

Use conversation history to identify behavior patterns and diagnose issues.

=== Common patterns

Look for these indicators when analyzing conversations:

* Agent calling the same tool repeatedly indicates loop detection is needed
* Large gaps between messages suggest tool timeout or slow execution
* Agent responses without tool calls indicate a tool selection issue
* Fabricated information suggests a missing "never make up data" constraint
* Truncated early messages indicate the context window was exceeded

=== Analysis workflow

. Use the inspector to reproduce the issue.
. Review full conversation including tool invocations.
. Identify where agent behavior diverged from expected.
. Check system prompt for missing guidance.
. Verify tool responses are formatted correctly.

== Track token usage

Monitor token consumption to optimize costs and set appropriate iteration limits.

=== View token usage

Token usage appears in:

* **Inspector tab**: Shows tokens per request during interactive testing
* **Tasks topic**: Contains `usage` metadata with input/output/total tokens
* **Transcripts**: Displays input and output token counts for each execution

=== Calculate costs

Use token data from the tasks topic to estimate costs:

[,bash]
----
Cost per request = (total_tokens Ã— model_price_per_token)
----

Track costs over time by:

. Consuming the tasks topic.
. Extracting `metadata.usage.total_tokens` from each task.
. Multiplying by your model's token price.
. Aggregating by time period.

For cost optimization strategies, see xref:ai-agents:agents/concepts.adoc#cost-calculation[].

== Monitor performance

Track agent performance metrics to identify bottlenecks.

=== Key metrics

Monitor these metrics from agent data topics:

* **Task completion rate**: Percentage of tasks with `TASK_STATE_COMPLETED`
* **Average iterations**: Mean iterations per task from conversation history
* **Token efficiency**: Tokens consumed per successful task completion
* **Tool invocation frequency**: Which tools are called most often
* **Error rate**: Percentage of tasks with `TASK_STATE_FAILED`

=== Performance analysis

. Consume the tasks topic over a time window.
. Calculate completion rate: `completed_tasks / total_tasks`.
. Identify high iteration tasks for prompt optimization.
. Track token usage trends over time.
. Correlate errors with specific tool invocations.

== Next steps

* xref:ai-agents:observability/concepts.adoc[]: Understand trace structure and OpenTelemetry spans
* xref:ai-agents:agents/troubleshooting.adoc[]: Diagnose and fix common agent issues
* xref:ai-agents:agents/concepts.adoc[]: Learn about agent execution and cost calculation
