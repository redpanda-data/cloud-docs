= Monitor Agent Activity
:description: Monitor agent execution, analyze conversation history, track token usage, and debug issues using Inspector, Transcripts, and agent data topics.
:page-topic-type: how-to
:personas: ai_agent_developer, platform_admin
:learning-objective-1: Test agents interactively using the Inspector tab
:learning-objective-2: Consume session and task topics for analysis
:learning-objective-3: Debug agent behavior using Transcripts

Monitor your agents to track performance, analyze conversations, debug issues, and optimize costs.

After reading this page, you will be able to:

* [ ] {learning-objective-1}
* [ ] {learning-objective-2}
* [ ] {learning-objective-3}

For conceptual background on traces and observability, see xref:ai-agents:observability/concepts.adoc[].

== Prerequisites

You must have a running agent. If you do not have one, see xref:ai-agents:agents/quickstart.adoc[].

== Test agents interactively

The *Inspector* tab provides real-time conversation testing and debugging. Use it to test agent responses interactively, view full conversation history, see tool invocations and results, monitor token usage per request, and test error scenarios.

=== Access the Inspector

. Navigate to *Agentic AI* > *AI Agents* in the Redpanda Cloud Console.
. Click your agent name.
. Open the *Inspector* tab.
. Enter test queries and review responses.
. Check the conversation panel to see tool calls.
. Start a new session to test fresh conversations.

=== Testing best practices

Test your agents systematically with these scenarios:

* **Boundary cases**: Test requests at the edge of agent capabilities to verify scope enforcement.
* **Error handling**: Request unavailable data to verify graceful degradation.
* **Iteration count**: Monitor how many iterations complex requests require.
* **Ambiguous input**: Send vague queries to verify clarification behavior.
* **Token usage**: Track tokens per request to estimate costs.

== View execution traces

The *Transcripts* view shows agent execution traces with detailed timing and error information. Transcripts capture agent startup and initialization, request and response flow, tool invocations and results, error messages and failures, token usage per request, and OpenTelemetry trace data.

=== Access Transcripts

. Navigate to *Agentic AI* > *AI Agents*.
. Click your agent name.
. Click *Transcripts* in the left navigation.
. Select a transcript to view execution details.

Use Transcripts when diagnosing deployment issues (agent won't start), debugging tool execution errors, investigating unexpected agent behavior, analyzing conversation flow, or verifying tool selection logic.

== Consume agent data topics

Agents emit structured data to two Redpanda topics for monitoring and analysis.

=== Sessions topic

The sessions topic (`redpanda.aiagent.<agent-id>.sessions`) contains all conversation messages. This topic lets you review past conversations, analyze conversation patterns, debug multi-turn interactions, and understand context management.

=== Tasks topic

The tasks topic (`redpanda.aiagent.<agent-id>.tasks`) contains task execution records with status and artifacts. Use this topic to monitor task completion rates, track token usage and costs, debug failed tasks, and analyze agent performance.

=== Access agent topics

[tabs]
=====
Cloud Console::
+
--
. Navigate to *Topics* in the Redpanda Cloud Console.
. Find `redpanda.aiagent.<agent-id>.sessions` or `redpanda.aiagent.<agent-id>.tasks`.
. Click *Messages* to view recent data.
. Use filters to search for specific sessions or task states.
--

rpk::
+
--
Consume recent sessions:

[,bash]
----
rpk topic consume redpanda.aiagent.<agent-id>.sessions --offset end -n 10
----

Consume recent tasks:

[,bash]
----
rpk topic consume redpanda.aiagent.<agent-id>.tasks --offset end -n 10
----
--

Data Plane API::
+
--
Use the link:/api/doc/cloud-dataplane/[Data Plane API] to programmatically consume agent data and integrate with your monitoring pipeline.
--
=====

For schema details, see xref:ai-agents:agents/concepts.adoc#agent-data-topics[Agent data topics].

== Analyze conversation history

Use conversation history to identify behavior patterns and diagnose issues. When analyzing conversations, watch for agents calling the same tool repeatedly (indicates loop detection is needed), large gaps between messages (suggests tool timeout or slow execution), agent responses without tool calls (indicates a tool selection issue), fabricated information (suggests a missing "never make up data" constraint), and truncated early messages (indicates the context window was exceeded).

=== Analysis workflow

. Use Inspector to reproduce the issue.
. Review full conversation including tool invocations.
. Identify where agent behavior diverged from expected.
. Check system prompt for missing guidance.
. Verify tool responses are formatted correctly.

== Track token usage

Monitor token consumption to optimize costs and set appropriate iteration limits. Token usage appears in three places: the *Inspector* tab shows tokens per request during interactive testing, the *Tasks* topic contains `usage` metadata with input/output/total tokens, and *Transcripts* displays token counts for each execution.

=== Calculate costs

Use token data from the tasks topic to estimate costs:

[,bash]
----
Cost per request = (total_tokens Ã— model_price_per_token)
----

Track costs over time by consuming the tasks topic, extracting `metadata.usage.total_tokens` from each task, multiplying by your model's token price, and aggregating by time period.

For cost optimization strategies, see xref:ai-agents:agents/concepts.adoc#cost-calculation[Cost calculation].

== Monitor performance

Track agent performance metrics to identify bottlenecks. The table below shows key metrics you can extract from agent data topics:

[cols="2,3", options="header"]
|===
| Metric | Description

| Task completion rate
| Percentage of tasks with `TASK_STATE_COMPLETED`

| Average iterations
| Mean iterations per task from conversation history

| Token efficiency
| Tokens consumed per successful task completion

| Tool invocation frequency
| Which tools are called most often

| Error rate
| Percentage of tasks with `TASK_STATE_FAILED`
|===

=== Performance analysis

. Consume the tasks topic over a time window.
. Calculate completion rate: `completed_tasks / total_tasks`.
. Identify high iteration tasks for prompt optimization.
. Track token usage trends over time.
. Correlate errors with specific tool invocations.

== Next steps

* xref:ai-agents:observability/concepts.adoc[]: Understand trace structure and OpenTelemetry spans
* xref:ai-agents:agents/troubleshooting.adoc[]: Diagnose and fix common agent issues
* xref:ai-agents:agents/concepts.adoc[]: Learn about agent execution and cost calculation
