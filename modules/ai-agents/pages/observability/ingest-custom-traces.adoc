= Ingest OpenTelemetry Traces from Custom Agents
:description: Configure a Redpanda Connect pipeline to ingest OTEL traces from custom agents into Redpanda for unified observability.
:page-topic-type: how-to
:learning-objective-1: Configure a Redpanda Connect pipeline to receive OpenTelemetry traces from custom agents via HTTP and publish them to redpanda.otel_traces
:learning-objective-2: Validate trace data format and compatibility with existing MCP server traces
:learning-objective-3: Secure the ingestion endpoint using authentication mechanisms

When you build custom agents or instrument applications outside of Remote MCP servers and declarative agents, you can send OpenTelemetry (OTEL) traces to Redpanda for centralized observability. Deploy a Redpanda Connect pipeline as an HTTP ingestion endpoint to collect and publish traces to the `redpanda.otel_traces` topic.

After reading this page, you will be able to:

* [ ] {learning-objective-1}
* [ ] {learning-objective-2}
* [ ] {learning-objective-3}

== Prerequisites

* A BYOC cluster
* Ability to manage secrets in Redpanda Cloud
* The latest version of `rpk` installed
* Custom agent or application instrumented with OpenTelemetry SDK
* Basic understanding of the https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-agent-spans/[OpenTelemetry span format^] and https://opentelemetry.io/docs/specs/otlp/[OpenTelemetry Protocol (OTLP)^]

== Quickstart for LangChain users

If you're using LangChain with OpenTelemetry tracing, you can send traces to Redpanda's `redpanda.otel_traces` glossterm:topic[] to view them in the Transcripts view.

. Deploy a Redpanda Connect pipeline using the `otlp_http` input to receive OTLP traces over HTTP. Create the pipeline in the *Connect* page of your cluster. See the <<configure-the-ingestion-pipeline,Configure the ingestion pipeline>> section for the required pipeline configuration.

. Authenticate to Redpanda Cloud using the OAuth2 client credentials flow described in xref:redpanda-cloud:security:cloud-authentication.adoc#authenticate-to-the-cloud-api[Authenticate to the Cloud API].

. Configure your application to export traces to the Redpanda Connect OTLP pipeline. The following example shows a complete LangChain agent that authenticates with Redpanda Cloud, streams through the AI Gateway using the native Google GenAI SDK, and exports traces to a Redpanda Connect OTLP pipeline:
+
[,python]
----
#!/usr/bin/env python3
"""
LangChain hello-world through Redpanda AI Gateway (Gemini native SDK)
with OTEL tracing to Redpanda Connect.

Reads configuration from .env file or environment variables.
"""

import os
import sys

import requests
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.httpx import HTTPXClientInstrumentor
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

load_dotenv()

# --- Configuration (from .env or environment) ---

GATEWAY_URL = os.environ["REDPANDA_GATEWAY_URL"]
GATEWAY_ID = os.environ["REDPANDA_GATEWAY_ID"]
OTLP_ENDPOINT = os.environ["REDPANDA_OTLP_ENDPOINT"]
MODEL = os.environ.get("MODEL", "gemini-3-flash-preview")

CLIENT_ID = os.environ["REDPANDA_CLOUD_CLIENT_ID"]
CLIENT_SECRET = os.environ["REDPANDA_CLOUD_CLIENT_SECRET"]

AUTH_URL = "https://auth.prd.cloud.redpanda.com/oauth/token"
AUTH_AUDIENCE = "cloudv2-production.redpanda.cloud"


def get_access_token() -> str:
    """Get a bearer token via OAuth2 client credentials flow."""
    resp = requests.post(
        AUTH_URL,
        json={
            "client_id": CLIENT_ID,
            "client_secret": CLIENT_SECRET,
            "audience": AUTH_AUDIENCE,
            "grant_type": "client_credentials",
        },
        timeout=10,
    )
    resp.raise_for_status()
    return resp.json()["access_token"]


def setup_otel(auth_token: str) -> None:
    """Configure OTEL to send traces to Redpanda Connect pipeline."""
    os.environ["LANGSMITH_OTEL_ENABLED"] = "true"
    os.environ["LANGSMITH_TRACING"] = "true"
    os.environ["LANGSMITH_OTEL_ONLY"] = "true"  # skip LangSmith, only custom endpoint

    exporter = OTLPSpanExporter(
        endpoint=f"{OTLP_ENDPOINT}/v1/traces",
        headers={"Authorization": f"Bearer {auth_token}"},
    )
    provider = TracerProvider()
    provider.add_span_processor(BatchSpanProcessor(exporter))
    trace.set_tracer_provider(provider)

    # Instrument httpx so traceparent headers propagate to the AI gateway
    HTTPXClientInstrumentor().instrument()


DEFAULT_PROMPT = "Say hello and tell me one fun fact about streaming data."


def main() -> None:
    prompt = sys.argv[1] if len(sys.argv) > 1 else DEFAULT_PROMPT

    print("Authenticating with Redpanda Cloud...")
    token = get_access_token()

    print("Setting up OTEL tracing...")
    setup_otel(token)

    print(f"Creating LangChain model ({MODEL}) via gateway...")
    llm = ChatGoogleGenerativeAI(
        model=MODEL,
        base_url=GATEWAY_URL,
        google_api_key="unused",  # auth handled by gateway via Bearer token
        additional_headers={
            "Authorization": f"Bearer {token}",
            "rp-aigw-id": GATEWAY_ID,
        },
        streaming=True,
    )

    print("Invoking model...\n")
    tracer = trace.get_tracer("langchain-demo")
    with tracer.start_as_current_span("llm_call", attributes={
        "gen_ai.system": "gemini",
        "gen_ai.request.model": MODEL,
    }):
        for chunk in llm.stream(prompt):
            text = chunk.content
            if isinstance(text, list):
                text = "".join(
                    part.get("text", "") for part in text if isinstance(part, dict)
                )
            print(text, end="", flush=True)
    print()

    # Flush traces before exit
    trace.get_tracer_provider().force_flush()
    print("\nTraces flushed to Redpanda.")


if __name__ == "__main__":
    main()
----
+
The example requires the following environment variables (or a `.env` file):
+
[cols="1,2", options="header"]
|===
| Variable | Description

| `REDPANDA_CLOUD_CLIENT_ID`
| Redpanda Cloud service account client ID

| `REDPANDA_CLOUD_CLIENT_SECRET`
| Redpanda Cloud service account client secret

| `REDPANDA_GATEWAY_URL`
| AI Gateway base URL, for example `\https://ai-gateway.<cluster-id>.clusters.rdpa.co`

| `REDPANDA_GATEWAY_ID`
| Virtual gateway ID (the `rp-aigw-id` header value)

| `REDPANDA_OTLP_ENDPOINT`
| Redpanda Connect OTLP pipeline URL, for example `\https://<pipeline-id>.pipelines.<cluster-id>.clusters.rdpa.co`

| `MODEL`
| Model name (default: `gemini-3-flash-preview`)
|===
+
Key implementation details:
+
--
* *LangSmith OTEL integration*: Setting `LANGSMITH_OTEL_ENABLED=true` and `LANGSMITH_OTEL_ONLY=true` sends traces only to the configured OTLP endpoint (not to LangSmith).
* *Trace context propagation*: The `HTTPXClientInstrumentor` instruments the httpx client used internally by the Google GenAI SDK, so that the W3C `traceparent` header propagates to the AI Gateway. This is necessary for client spans to appear nested under gateway spans in the Transcripts view.
* *Explicit parent span*: The LLM call is wrapped in an explicit OTEL span (`tracer.start_as_current_span`) to ensure there is an active trace context for httpx to propagate. Without this wrapper, LangSmith creates spans after-the-fact and the httpx calls have no parent context.
* *AI Gateway auth*: The `google_api_key` is set to `"unused"` because the gateway handles real provider authentication. The bearer token and gateway ID are passed as additional headers.
--

For non-LangChain applications or custom instrumentation, continue with the sections below.

== About custom trace ingestion

Custom agents include applications you build with OpenTelemetry instrumentation that operate independently of Redpanda's Remote MCP servers or declarative agents. Examples include:

* Custom AI agents built with LangChain, CrewAI, or other frameworks
* Applications with manual OpenTelemetry instrumentation
* Services that integrate with third-party AI platforms

When these applications send traces to Redpanda's `redpanda.otel_traces` glossterm:topic[], you gain unified observability across all agentic components in your system. Custom agent transcripts appear alongside Remote MCP server and declarative agent transcripts in the Transcripts view, creating xref:ai-agents:observability/concepts.adoc#cross-service-transcripts[cross-service transcripts] that allow you to correlate operations and analyze end-to-end request flows.

=== Trace format requirements

Custom agents must emit traces in OTLP format. The `otlp_http` input accepts both OTLP Protobuf (`application/x-protobuf`) and JSON (`application/json`) payloads. For <<use-grpc,gRPC transport>>, use the `otlp_grpc` input.

Each trace must follow the OTLP specification with these required fields:

[cols="1,3", options="header"]
|===
| Field | Description

| `traceId`
| Hex-encoded unique identifier for the entire trace

| `spanId`
| Hex-encoded unique identifier for this span

| `name`
| Descriptive operation name

| `startTimeUnixNano` and `endTimeUnixNano`
| Timing information in nanoseconds

| `instrumentationScope`
| Identifies the library that created the span

| `status`
| Operation status with code (1 = OK, 2 = ERROR, 0 = UNSET)
|===

Optional but recommended fields:
- `parentSpanId` for hierarchical traces
- `attributes` for contextual information

For complete trace structure details, see xref:ai-agents:observability/concepts.adoc#understand-the-transcript-structure[Understand the transcript structure].

== Configure the ingestion pipeline

Create a Redpanda Connect pipeline that receives HTTP requests containing OTLP traces and publishes them to the `redpanda.otel_traces` topic. The pipeline uses the `otlp_http` input component, which is specifically designed to receive OpenTelemetry Protocol data.

=== Create the pipeline configuration

Create a pipeline configuration file that defines the OTLP HTTP ingestion endpoint.

The `otlp_http` input component:

* Exposes an OpenTelemetry Collector HTTP receiver
* Accepts traces at the standard `/v1/traces` endpoint
* Converts incoming OTLP data into individual Redpanda OTEL v1 Protobuf messages and publishes them to the `redpanda.otel_traces` topic

The Transcripts UI expects messages in `redpanda.otel_traces` to have:

* **Protobuf encoding** with Schema Registry wire format headers (not JSON)
* **Kafka key** in `<hex-trace-id>/<hex-span-id>` format (for example, `15eb972e47e3611f3281e57703aea3a5/349306a1fea8f67a`)

The `otlp_http` input must be configured with `encoding: protobuf` and a `schema_registry` block to produce the correct wire format. A branch processor extracts the trace and span IDs from each message to construct the required Kafka key.

Redpanda Cloud automatically injects authentication handling for incoming requests, so you don't need to configure `auth_token` in the input.

NOTE: The `schema_registry` block currently requires SASL basic auth credentials. Service account authentication for Schema Registry is not yet supported in the `otlp_http` input.

[,yaml]
----
input:
  otlp_http:
    encoding: protobuf
    schema_registry:
      url: <schema-registry-url> # <1>
      basic_auth:
        enabled: true
        username: <sr-username>
        password: <sr-password>

pipeline:
  processors:
    - branch:
        request_map: root = content()
        processors:
          - schema_registry_decode:
              url: <schema-registry-url> # <1>
              basic_auth:
                enabled: true
                username: <sr-username>
                password: <sr-password>
          - mapping: |
              let trace_id = this.traceId | ""
              let span_id = this.spanId | ""
              root = if $trace_id != "" && $span_id != "" {
                $trace_id.decode("base64").encode("hex") + "/" + $span_id.decode("base64").encode("hex")
              } else { deleted() }
        result_map: meta kafka_key = content().string()

output:
  redpanda:
    seed_brokers:
      - "${PRIVATE_REDPANDA_BROKERS}"
    tls:
      enabled: ${PRIVATE_REDPANDA_TLS_ENABLED}
    sasl:
      - mechanism: "REDPANDA_CLOUD_SERVICE_ACCOUNT"
    topic: "redpanda.otel_traces"
    key: ${! @kafka_key }
    metadata:
      include_patterns: ["otel_.*"]
----
<1> Your cluster's Schema Registry URL. Find it in the cluster overview page.

The branch processor decodes each protobuf message to extract `traceId` and `spanId` fields (lowerCamelCase, as the `otlp_http` input flattens spans to root level), converts them from base64 to hex, and sets the result as the Kafka key. The original protobuf payload is preserved unmodified.

[[use-grpc]]
==== Use gRPC transport

If your agents use gRPC instead of HTTP, replace `otlp_http` with `otlp_grpc` in the input section. The rest of the pipeline configuration (processors, output) remains identical. See the <<grpc-exporter-examples,gRPC exporter examples>> below for client-side configuration.

=== Deploy the pipeline in Redpanda Cloud

. In the *Connect* page of your Redpanda Cloud cluster, click *Create Pipeline*.
. Paste the pipeline configuration from above, replacing the Schema Registry URL and credentials with your cluster's values.
. In the *Add permissions* step, create a service account with write access to the `redpanda.otel_traces` topic.
. Deploy the pipeline. Redpanda Cloud automatically handles authentication for incoming HTTP requests.

== Send traces from your custom agent

Configure your custom agent to send OpenTelemetry traces to the pipeline endpoint. After deploying the pipeline, you can find its URL in the Redpanda Cloud UI on the pipeline details page.

The endpoint URL format is:

* **HTTP**: `https://<pipeline-id>.pipelines.<cluster-id>.clusters.rdpa.co/v1/traces`
* **gRPC**: `<pipeline-id>.pipelines.<cluster-id>.clusters.rdpa.co:443`

=== Authenticate to the pipeline

The OTLP pipeline uses the same authentication mechanism as the Redpanda Cloud API. Obtain an access token using your service account credentials as described in xref:redpanda-cloud:security:cloud-authentication.adoc#authenticate-to-the-cloud-api[Authenticate to the Cloud API].

Include the token in your requests:

* **HTTP**: Set the `Authorization` header to `Bearer <token>`
* **gRPC**: Set the `authorization` metadata field to `Bearer <token>`

=== Configure your OTEL exporter

Install the OpenTelemetry SDK for your language and configure the OTLP exporter to target your Redpanda Connect pipeline endpoint.

The exporter configuration requires:

* **Endpoint**: Your pipeline's URL (some SDKs auto-append `/v1/traces` when using the `OTEL_EXPORTER_OTLP_ENDPOINT` environment variable, but when setting the endpoint programmatically you may need to include the full path)
* **Headers**: Authorization header with your bearer token
* **Protocol**: HTTP to match the `otlp_http` input (or gRPC for `otlp_grpc`)

.Python example for OTLP HTTP exporter
[,python]
----
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.sdk.resources import Resource

# Configure resource attributes to identify your agent
resource = Resource(attributes={
    "service.name": "my-custom-agent",
    "service.version": "1.0.0"
})

# Configure the OTLP HTTP exporter
exporter = OTLPSpanExporter(
    endpoint="https://<pipeline-id>.pipelines.<cluster-id>.clusters.rdpa.co/v1/traces",
    headers={"Authorization": "Bearer YOUR_TOKEN"}
)

# Set up tracing with batch processing
provider = TracerProvider(resource=resource)
processor = BatchSpanProcessor(exporter)
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)

# Use the tracer with GenAI semantic conventions
tracer = trace.get_tracer(__name__)
with tracer.start_as_current_span(
    "invoke_agent my-assistant",
    kind=trace.SpanKind.INTERNAL
) as span:
    # Set GenAI semantic convention attributes
    span.set_attribute("gen_ai.operation.name", "invoke_agent")
    span.set_attribute("gen_ai.agent.name", "my-assistant")
    span.set_attribute("gen_ai.provider.name", "openai")
    span.set_attribute("gen_ai.request.model", "gpt-4")

    # Your agent logic here
    result = process_request()

    # Set token usage if available
    span.set_attribute("gen_ai.usage.input_tokens", 150)
    span.set_attribute("gen_ai.usage.output_tokens", 75)
----

.Node.js example for OTLP HTTP exporter
[,javascript]
----
const { NodeTracerProvider } = require('@opentelemetry/sdk-trace-node');
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http');
const { BatchSpanProcessor } = require('@opentelemetry/sdk-trace-base');
const { Resource } = require('@opentelemetry/resources');
const { trace, SpanKind } = require('@opentelemetry/api');

// Configure resource
const resource = new Resource({
  'service.name': 'my-custom-agent',
  'service.version': '1.0.0'
});

// Configure OTLP HTTP exporter
const exporter = new OTLPTraceExporter({
  url: 'https://<pipeline-id>.pipelines.<cluster-id>.clusters.rdpa.co/v1/traces',
  headers: {
    'Authorization': 'Bearer YOUR_TOKEN'
  }
});

// Set up provider
const provider = new NodeTracerProvider({ resource });
provider.addSpanProcessor(new BatchSpanProcessor(exporter));
provider.register();

// Use the tracer with GenAI semantic conventions
const tracer = trace.getTracer('my-agent');
const span = tracer.startSpan('invoke_agent my-assistant', {
  kind: SpanKind.INTERNAL
});

// Set GenAI semantic convention attributes
span.setAttribute('gen_ai.operation.name', 'invoke_agent');
span.setAttribute('gen_ai.agent.name', 'my-assistant');
span.setAttribute('gen_ai.provider.name', 'openai');
span.setAttribute('gen_ai.request.model', 'gpt-4');

// Your agent logic
processRequest().then(result => {
  // Set token usage if available
  span.setAttribute('gen_ai.usage.input_tokens', 150);
  span.setAttribute('gen_ai.usage.output_tokens', 75);
  span.end();
});
----

.Go example for OTLP HTTP exporter
[,go]
----
package main

import (
    "context"
    "log"

    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/attribute"
    "go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp"
    "go.opentelemetry.io/otel/sdk/resource"
    sdktrace "go.opentelemetry.io/otel/sdk/trace"
    semconv "go.opentelemetry.io/otel/semconv/v1.26.0"
    "go.opentelemetry.io/otel/trace"
)

func main() {
    ctx := context.Background()

    // Configure OTLP HTTP exporter
    exporter, err := otlptracehttp.New(ctx,
        otlptracehttp.WithEndpoint("<pipeline-id>.pipelines.<cluster-id>.clusters.rdpa.co"),
        otlptracehttp.WithHeaders(map[string]string{
            "Authorization": "Bearer YOUR_TOKEN",
        }),
    )
    if err != nil {
        log.Fatalf("Failed to create exporter: %v", err)
    }

    // Configure resource
    res, _ := resource.New(ctx,
        resource.WithAttributes(
            semconv.ServiceName("my-custom-agent"),
            semconv.ServiceVersion("1.0.0"),
        ),
    )

    // Set up tracer provider
    tp := sdktrace.NewTracerProvider(
        sdktrace.WithBatcher(exporter),
        sdktrace.WithResource(res),
    )
    defer tp.Shutdown(ctx)
    otel.SetTracerProvider(tp)

    tracer := tp.Tracer("my-agent")

    // Create span with GenAI semantic conventions
    _, span := tracer.Start(ctx, "invoke_agent my-assistant",
        trace.WithSpanKind(trace.SpanKindInternal),
    )
    span.SetAttributes(
        attribute.String("gen_ai.operation.name", "invoke_agent"),
        attribute.String("gen_ai.agent.name", "my-assistant"),
        attribute.String("gen_ai.provider.name", "openai"),
        attribute.String("gen_ai.request.model", "gpt-4"),
        attribute.Int("gen_ai.usage.input_tokens", 150),
        attribute.Int("gen_ai.usage.output_tokens", 75),
    )
    span.End()

    tp.ForceFlush(ctx)
}
----

TIP: Use environment variables for the endpoint URL and authentication token to keep credentials out of your code.

[[grpc-exporter-examples]]
==== gRPC transport examples

If you're using the `otlp_grpc` input, configure your exporter to use gRPC transport.

.Python example for OTLP gRPC exporter
[,python]
----
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.sdk.resources import Resource

resource = Resource(attributes={
    "service.name": "my-custom-agent",
    "service.version": "1.0.0"
})

# gRPC endpoint without https:// prefix
exporter = OTLPSpanExporter(
    endpoint="<pipeline-id>.pipelines.<cluster-id>.clusters.rdpa.co:443",
    headers={"authorization": "Bearer YOUR_TOKEN"}
)

provider = TracerProvider(resource=resource)
provider.add_span_processor(BatchSpanProcessor(exporter))
trace.set_tracer_provider(provider)
----

.Node.js example for OTLP gRPC exporter
[,javascript]
----
const { NodeTracerProvider } = require('@opentelemetry/sdk-trace-node');
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-grpc');
const { BatchSpanProcessor } = require('@opentelemetry/sdk-trace-base');
const { Resource } = require('@opentelemetry/resources');

const resource = new Resource({
  'service.name': 'my-custom-agent',
  'service.version': '1.0.0'
});

// gRPC exporter with TLS
const exporter = new OTLPTraceExporter({
  url: 'https://<pipeline-id>.pipelines.<cluster-id>.clusters.rdpa.co:443',
  headers: {
    'authorization': 'Bearer YOUR_TOKEN'
  }
});

const provider = new NodeTracerProvider({ resource });
provider.addSpanProcessor(new BatchSpanProcessor(exporter));
provider.register();
----

.Go example for OTLP gRPC exporter
[,go]
----
package main

import (
    "context"

    "go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
    "google.golang.org/grpc"
    "google.golang.org/grpc/credentials"
)

func createGRPCExporter(ctx context.Context) (*otlptracegrpc.Exporter, error) {
    return otlptracegrpc.New(ctx,
        otlptracegrpc.WithEndpoint("<pipeline-id>.pipelines.<cluster-id>.clusters.rdpa.co:443"),
        otlptracegrpc.WithDialOption(grpc.WithTransportCredentials(credentials.NewTLS(nil))),
        otlptracegrpc.WithHeaders(map[string]string{
            "authorization": "Bearer YOUR_TOKEN",
        }),
    )
}
----

=== Use recommended semantic conventions

The Transcripts view recognizes https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-agent-spans/[OpenTelemetry semantic conventions for GenAI operations^]. Following these conventions ensures your traces display correctly with proper attribution, token usage, and operation identification.

==== Required attributes for agent operations

Following the OpenTelemetry semantic conventions, agent spans should include these attributes:

* Operation identification:
** `gen_ai.operation.name` - Set to `"invoke_agent"` for agent execution spans
** `gen_ai.agent.name` - Human-readable name of your agent (displayed in Transcripts view)
* LLM provider details:
** `gen_ai.provider.name` - LLM provider identifier (e.g., `"openai"`, `"anthropic"`, `"gcp.vertex_ai"`)
** `gen_ai.request.model` - Model name (e.g., `"gpt-4"`, `"claude-sonnet-4"`)
* Token usage (for cost tracking):
** `gen_ai.usage.input_tokens` - Number of input tokens consumed
** `gen_ai.usage.output_tokens` - Number of output tokens generated
* Session correlation:
** `gen_ai.conversation.id` - Identifier linking related agent invocations in the same conversation

==== Example with semantic conventions

.Python example with GenAI semantic conventions
[,python]
----
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

# Create an agent invocation span
with tracer.start_as_current_span(
    "invoke_agent my-assistant",
    kind=trace.SpanKind.INTERNAL
) as span:
    # Set required attributes
    span.set_attribute("gen_ai.operation.name", "invoke_agent")
    span.set_attribute("gen_ai.agent.name", "my-assistant")
    span.set_attribute("gen_ai.provider.name", "openai")
    span.set_attribute("gen_ai.request.model", "gpt-4")
    span.set_attribute("gen_ai.conversation.id", "session-abc-123")

    # Your agent logic here
    response = process_agent_request(user_input)

    # Set token usage after completion
    span.set_attribute("gen_ai.usage.input_tokens", response.usage.input_tokens)
    span.set_attribute("gen_ai.usage.output_tokens", response.usage.output_tokens)
----

.Node.js example with GenAI semantic conventions
[,javascript]
----
const { trace, SpanKind } = require('@opentelemetry/api');

const tracer = trace.getTracer('my-agent');

const span = tracer.startSpan('invoke_agent my-assistant', {
  kind: SpanKind.INTERNAL
});

// Set required attributes
span.setAttribute('gen_ai.operation.name', 'invoke_agent');
span.setAttribute('gen_ai.agent.name', 'my-assistant');
span.setAttribute('gen_ai.provider.name', 'openai');
span.setAttribute('gen_ai.request.model', 'gpt-4');
span.setAttribute('gen_ai.conversation.id', 'session-abc-123');

// Your agent logic
const response = await processAgentRequest(userInput);

// Set token usage
span.setAttribute('gen_ai.usage.input_tokens', response.usage.inputTokens);
span.setAttribute('gen_ai.usage.output_tokens', response.usage.outputTokens);

span.end();
----

== Verify trace ingestion

After deploying your pipeline and configuring your custom agent, verify traces are flowing correctly.

=== Consume traces from the topic

Check that traces are being published to the `redpanda.otel_traces` topic:

[,bash]
----
rpk topic consume redpanda.otel_traces --offset end -n 10
----

You can also view the `redpanda.otel_traces` topic in the *Topics* page of Redpanda Cloud UI.

Look for spans with your custom `instrumentationScope.name` to identify traces from your agent.

=== View traces in Transcripts 

After your custom agent sends traces through the pipeline, they appear in your cluster's *Agentic AI > Transcripts* view alongside traces from Remote MCP servers and declarative agents.

==== Identify custom agent transcripts

Custom agent transcripts are identified by the `service.name` resource attribute, which differs from Redpanda's built-in services (`ai-agent` for declarative agents, `mcp-{server-id}` for MCP servers). See xref:ai-agents:observability/concepts.adoc#cross-service-transcripts[Cross-service transcripts] to understand how the `service.name` attribute identifies transcript sources.

Your custom agent transcripts display with:

* **Service name** in the service filter dropdown (from your `service.name` resource attribute)
* **Agent name** in span details (from the `gen_ai.agent.name` attribute)
* **Operation names** like `"invoke_agent my-assistant"` indicating agent executions

For detailed instructions on filtering, searching, and navigating transcripts in the UI, see xref:ai-agents:observability/view-transcripts.adoc[View Transcripts].

==== Token usage tracking

If your spans include the recommended token usage attributes (`gen_ai.usage.input_tokens` and `gen_ai.usage.output_tokens`), they display in the summary panel's token usage section. This enables cost tracking alongside Remote MCP server and declarative agent transcripts.

== Troubleshooting

=== Pipeline not receiving requests

If your custom agent cannot reach the ingestion endpoint:

. Verify the endpoint URL format:
   * HTTP: `https://<pipeline-id>.pipelines.<cluster-id>.clusters.rdpa.co/v1/traces`
   * gRPC: `<pipeline-id>.pipelines.<cluster-id>.clusters.rdpa.co:443` (no `https://` prefix for gRPC clients)
. Check network connectivity and firewall rules.
. Ensure authentication tokens are valid and properly formatted in the `Authorization: Bearer <token>` header (HTTP) or `authorization` metadata field (gRPC).
. Verify the Content-Type header matches your data format (`application/x-protobuf` or `application/json`).
. Review pipeline logs for connection errors or authentication failures.

=== Traces not appearing in topic

If requests succeed but traces do not appear in `redpanda.otel_traces`:

. Check pipeline output configuration.
. Verify topic permissions.
. Validate trace format matches OTLP specification.

== Limitations

* The `otlp_http` and `otlp_grpc` inputs accept only traces, logs, and metrics, not profiles.
* Only traces are published to the `redpanda.otel_traces` topic.
* Exceeded rate limits return HTTP 429 (HTTP) or ResourceExhausted status (gRPC).

== Next steps

* xref:ai-agents:observability/view-transcripts.adoc[]
* https://docs.redpanda.com/redpanda-connect/components/inputs/otlp_http/[OTLP HTTP input reference^] - Complete configuration options for the `otlp_http` component
* https://docs.redpanda.com/redpanda-connect/components/inputs/otlp_grpc/[OTLP gRPC input reference^] - Alternative gRPC-based trace ingestion