= Redpanda Connect Quickstart
:description: Learn how to quickly start building data pipelines with Redpanda Connect in Redpanda Cloud.

The *Connect* page provides a wizard that creates pipelines to stream data into and out of Redpanda. The wizard populates the required YAML configuration automatically, so you can get started quickly.

TIP: Advanced users can skip directly to the *Edit pipeline* step in the wizard to configure the YAML file themselves. 

Follow this quickstart to create a sample Redpanda Connect pipeline. This pipeline generates names, transforms them to capitalized format, and writes them to a topic in your cluster. It uses the following Redpanda Connect components:

[cols="1,2,3"]
|===
|Component type |Component |Purpose

|Input
|xref:develop:connect/components/inputs/generate.adoc[`generate`]
|Creates fake names for testing

|Output
|xref:components:outputs/redpanda.adoc[`redpanda`]
|Writes messages to your cluster topic

|Processor
|xref:components:processors/mapping.adoc[`mapping`]
|Transforms the names to uppercase
|===

== Prerequisites

You must have a Redpanda Cloud account with a Serverless, Dedicated, or standard BYOC cluster. If you don't already have an account, https://redpanda.com/try-redpanda/cloud-trial[sign up for a free trial^].

NOTE: Serverless supports up to 10 pipelines.

== Build your data pipeline

Follow these steps to create your pipeline:

. Go to the **Connect** page on your cluster, and click **Create pipeline**.

. **Add an input**: Search for and select `generate` from the list of connectors. Click **Next**.

. **Add an output**: Search for and select `redpanda` from the list of connectors. Click **Next**.

. **Add a topic**: Select to create a new topic called `sample-topic`. This is where Redpanda will store the generated messages. Click **Next**.

. **Add permissions**: 
.. Select to configure a new user called `connect`.
.. Enter this password for the user: `connect-password`.
.. Select to enable topic-specific permissions for this user. This is required, because the user needs to have read and write permissions to interact with the topic.
.. Click **Next**.

. **Edit pipeline**:
.. Enter this name for the pipeline: `sample-pipeline`.
.. The **Configuration** section automatically populates the YAML with your selected components. To enhance the pipeline, replace the entire configuration with the following YAML, which includes a `mapping` processor to convert the generated names to uppercase:
+
[source,yaml]
----
input:
  generate:
    mapping: |
      let first_name = fake("first_name")
      let last_name = fake("last_name")
      root.user_id = counter()
      root.name = $first_name + " " + $last_name
      root.timestamp = now()
    interval: 1s # Optional (default: "1s")
    count: 0 # Optional (default: 0)
    batch_size: 1 # Optional (default: 1)
    auto_replay_nacks: true # Optional (default: true)

output:
  redpanda:
    seed_brokers:
      # Optional
      - ${REDPANDA_BROKERS}
    tls:
      enabled: true # Optional (default: false)
    topic: "" # Optional

pipeline:
  processors:
    - mapping: |
        root.name = this.name.uppercase()


----

.. Click **Create** to start your pipeline.


. Your pipeline details are displayed, and after a few seconds, the status changes from **Starting** to **Running**. If you don't see this change, refresh the page.
+
Once running, your pipeline will generate new names every second and write the processed messages to your topic.

+
After a minute, select the pipeline, and click **Stop** so you can examine the results.

[NOTE]
====
* Notice the `$\{REDPANDA_BROKERS}` xref:develop:connect/configuration/contextual-variables.adoc[contextual variable] in the configuration. This automatically references your cluster's bootstrap server address, so you can use it in any pipeline without hardcoding connection details. 
* The Brave browser does not fully support code snippets.

====

== View the processed messages

. Go to the **Topics** page and select the `sample-topic` topic.
. Click any message to see the structure. Notice that the original fields from the `generate` input now include the capitalized names added by the `mapping` processor:
+
<NEED TO REPLACE WHEN I CAN SEE MESSAGES>
+
[source,json]
----
{
    "content": "Aliquam quidem tempore expedita debitis ab. Officiis optio eveniet ab magni commodi...",
    "id": "35522c66-6fcd-47da-b97b-857b983477d1",
    "title": "PRIVATE AND CONFIDENTIAL",
    "user": {
        "email": "oCcXPTh@RrKHZRQ.info",
        "name": "King Francis Torphy"
    }
}
----

== Review the pipeline logs

. Return to the **Connect** page and select your `sample-pipeline`.
. Click the **Logs** tab to see the pipeline's activity log. 
. Click through the log messages to see the startup sequence. For example, you'll see when the output becomes active:

+
[source,json]
----
{
    "instance_id": "cr3j2rab2tks83v3gbh0",
    "label": "",
    "level": "INFO",
    "message": "Output type redpanda is now active",
    "path": "root.output",
    "pipeline_id": "cr3j2r6hqokqcph9p4b0",
    "time": "2024-08-22T12:39:09.729899336Z"
}
----

== Update your pipeline

Now try adding custom logging to your configuration. You can make the updates while your data pipeline is running.

. On the **Configuration** tab of your pipeline, click **Start** and wait for it to reach **Running** status.
. Click **Edit** and replace the entire `processors` section with this enhanced version: 

+
[source,yaml]

  processors:
    - mapping: |
        root.name = this.name.uppercase()
    - log:
       level: INFO
       message: 'Processed email for ${!this.user.name}'
       fields_mapping: |
         root.reason = "SUCCESS"
         root.id = this.id
----

+
This enhanced configuration:

* Adds custom logging with the xref:components:processors/log.adoc[`log` processor]
* Records processing success for each message

. Click **Update** to apply the changes.
. Switch to the **Logs** tab and check the most recent log message. You'll see the custom logging fields and the uppercase sender name: 

+
<NEED TO REPLACE WHEN I CAN SEE MESSAGES>
+
[source,json]
----
{
    "id": "f64d1f1a-2d76-47ad-a215-52410ab4e22f",
    "instance_id": "cr3ncrvom8ofl3bn3rk0",
    "label": "",
    "level": "INFO",
    "message": "Processed email for MISS IMELDA REICHERT",
    "path": "root.pipeline.processors.1",
    "pipeline_id": "cr3me2uhqokqcph9p4bg",
    "reason": "SUCCESS",
    "time": "2024-08-22T17:33:46.676903284Z"
}
----
. Click **Stop**.

== Clean up

When you've finished experimenting with your data pipeline, you can delete the pipeline and topic you created for this quickstart.

. On the **Connect** page, select the delete icon next to the `sample-pipeline`.
. Confirm your deletion to remove the data pipeline and associated logs.
. On the **Topics** page, delete the `sample-topic` topic.

== Suggested reading

* Try one of our xref:cookbooks:index.adoc[Redpanda Connect cookbooks]. 
* Choose xref:develop:connect/components/about.adoc[connectors for your use case].
* Learn how to xref:develop:connect/configuration/secret-management.adoc[add secrets to your pipeline].
* Learn how to xref:develop:connect/configuration/monitor-connect.adoc[monitor a data pipeline on a BYOC or Dedicated cluster].
* Learn how to xref:develop:connect/configuration/scale-pipelines.adoc[manually scale resources for a pipeline].
* Learn how to xref:redpanda-connect:guides:getting_started.adoc[configure, test, and run a data pipeline locally].
