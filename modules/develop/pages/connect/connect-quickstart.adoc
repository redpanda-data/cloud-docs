= Redpanda Connect Quickstart
:description: Learn how to quickly start building data pipelines with Redpanda Connect in Redpanda Cloud.

Follow this quickstart to create your first Redpanda Connect pipeline. Pipelines help you stream data into and out of Redpanda without writing producer/consumer code. 

This quickstart builds a simple email processing pipeline that:

. Generates sample email messages
. Adds a "PRIVATE AND CONFIDENTIAL" title to each message  
. Writes the processed messages to a topic in your cluster

It uses the following Redpanda Connect components:

[cols="1,2,3"]
|===
|Component type |Component |Purpose

|Input
|xref:develop:connect/components/inputs/generate.adoc[`generate`]
|Creates sample email messages for testing

|Processor
|xref:components:processors/mutation.adoc[`mutation`]
|Adds a title field to each message

|Output
|xref:components:outputs/kafka_franz.adoc[`kafka_franz`]
|Writes messages to your cluster topic
|===

== Prerequisites

A Redpanda Cloud account with a Serverless, Dedicated, or standard BYOC cluster. If you don't already have an account, https://redpanda.com/try-redpanda/cloud-trial[sign up for a free trial^].

NOTE: Serverless supports up to 10 pipelines.

== Build your data pipeline

Follow these steps to create your email processing pipeline:

. Go to the **Connect** page on your cluster, and click **Create pipeline**.

. **Add an input**: Search for and select **generate** from the list of connectors. Click **Next**.

. **Add an output**: Search for and select **kafka_franz** from the list of connectors. Click **Next**.

. **Add a topic**: Select to create a new topic called **processed-emails**. This is where Redpanda will store the generated messages. Click **Next**.

. **Add permissions**: 
.. Select to configure a new user called **connect**.
.. Enter a password for this user. This quickstart configuration uses **connect-password**.
.. Enable permissions for this user on your pipeline's topic. The user must have **Read** and **Write** permissions to interact with the topic.
.. Click **Next**.

. **Edit pipeline**:
.. Set the pipeline name to **emailprocessor-pipeline**.
.. In the **Configuration** section, paste the following YAML:
+
[source,yaml]
----
input:
 generate:
   interval: 1s
   mapping: |
     root.id = uuid_v4()
     root.user.name = fake("name")
     root.user.email = fake("email")
     root.content = fake("paragraph")

pipeline:
 processors:
   - mutation: |
       root.title = "PRIVATE AND CONFIDENTIAL"

output:
 kafka_franz:
   seed_brokers:
     - ${REDPANDA_BROKERS}
   sasl:
     - mechanism: SCRAM-SHA-256
       password: connect-password
       username: connect
   topic: processed-emails
   tls:
     enabled: true
----

.. Click **Create** to start your pipeline.


. Your pipeline details are displayed, and the status changes from **Starting** to **Running** (this may take a few minutes). If you don't see this change, refresh the page.
+
Once running, your pipeline will:
+
* Generate a new email message every second.
* Add "PRIVATE AND CONFIDENTIAL" to each message.
* Write the processed messages to the **processed-emails** topic.

+
After a minute, select the pipeline, and click **Stop** so you can examine the results.

TIP: Notice the `$\{REDPANDA_BROKERS}` xref:develop:connect/configuration/contextual-variables.adoc[contextual variable] in the configuration. This automatically references your cluster's bootstrap server address, so you can use it in any pipeline without hardcoding connection details. 

NOTE: The Brave browser does not fully support code snippets. 

== View the processed messages

. Go to the **Topics** page and select the **processed-emails** topic.
. Click on any message to see the structure. Notice that the original fields from the `generate` input now include the `title` field added by the `mutation` processor:

+
[source,json]
----
{
    "content": "Aliquam quidem tempore expedita debitis ab. Officiis optio eveniet ab magni commodi...",
    "id": "35522c66-6fcd-47da-b97b-857b983477d1",
    "title": "PRIVATE AND CONFIDENTIAL",
    "user": {
        "email": "oCcXPTh@RrKHZRQ.info",
        "name": "King Francis Torphy"
    }
}
----

== Review the pipeline logs

. Return to the **Connect** page and select your **emailprocessor-pipeline**.
. Click the **Logs** tab to see the pipeline's activity log. 
. Click through the log messages to see the startup sequence. For example, you'll see when the output becomes active:

+
[source,json]
----
{
    "instance_id": "cr3j2rab2tks83v3gbh0",
    "label": "",
    "level": "INFO",
    "message": "Output type kafka_franz is now active",
    "path": "root.output",
    "pipeline_id": "cr3j2r6hqokqcph9p4b0",
    "time": "2024-08-22T12:39:09.729899336Z"
}
----

== Update your pipeline

Now try adding custom logging and an extra data transformation to your configuration. You can make the updates while your data pipeline is running.

. On the **Configuration** tab of your pipeline, click **Start** and wait for it to reach **Running** status.
. Click **Edit** and replace the entire `processors` section with this enhanced version: 

+
[source,yaml]
----
  processors:
    - mutation: |
        root.title = "PRIVATE AND CONFIDENTIAL"
        root.user.name = root.user.name.uppercase()
    - log:
       level: INFO
       message: 'Processed email for ${!this.user.name}'
       fields_mapping: |
         root.reason = "SUCCESS"
         root.id = this.id
----

+
This enhanced configuration:

* Converts each sender's name to uppercase
* Adds custom logging with the xref:components:processors/log.adoc[`log` processor]
* Records processing success for each message

. Click **Update** to apply the changes.
. Switch to the **Logs** tab and check the most recent log message. You'll see the custom logging fields and the uppercase sender name: 

+
[source,json]
----
{
    "id": "f64d1f1a-2d76-47ad-a215-52410ab4e22f",
    "instance_id": "cr3ncrvom8ofl3bn3rk0",
    "label": "",
    "level": "INFO",
    "message": "Processed email for MISS IMELDA REICHERT",
    "path": "root.pipeline.processors.1",
    "pipeline_id": "cr3me2uhqokqcph9p4bg",
    "reason": "SUCCESS",
    "time": "2024-08-22T17:33:46.676903284Z"
}
----
. Click **Stop**.

== Clean up

When you've finished experimenting with your data pipeline, you can delete the pipeline, topic, and cluster you created for this quickstart.

. On the **Connect** page, select the delete icon next to the **emailprocessor-pipeline**.
. Confirm your deletion to remove the data pipeline and associated logs.
. On the **Topics** page, delete the **processed-emails** topic.

== Suggested reading

* Try one of our xref:cookbooks:index.adoc[Redpanda Connect cookbooks]. 
* Choose xref:develop:connect/components/about.adoc[connectors for your use case].
* Learn how to xref:develop:connect/configuration/secret-management.adoc[add secrets to your pipeline].
* Learn how to xref:develop:connect/configuration/monitor-connect.adoc[monitor a data pipeline on a BYOC or Dedicated cluster].
* Learn how to xref:develop:connect/configuration/scale-pipelines.adoc[manually scale resources for a pipeline].
* Learn how to xref:redpanda-connect:guides:getting_started.adoc[configure, test, and run a data pipeline locally].
